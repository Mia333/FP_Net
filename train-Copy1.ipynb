{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\" The main script for module RNN\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib.framework import nest\n",
    "\n",
    "from utils import DataLoader, draw_strokes\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "  data_loader = DataLoader(args.batch_size, args.data_scale, args.bptt_length) # args.bptt_length?????????\n",
    "  data_loader.reset_batch_pointer()\n",
    "\n",
    "  # model needs to know the dim of one-hot vectors\n",
    "  args.char_vec_dim = data_loader.char_vec_dim\n",
    "  # also the max length of char vector\n",
    "  args.max_char_len = data_loader.max_char_len\n",
    "\n",
    "  if args.model_dir != '' and not os.path.exists(args.model_dir):\n",
    "    os.makedirs(args.model_dir)\n",
    "  \n",
    "  print(args.model_dir) ######!!!!!!!!!!!!!!!!\n",
    "  with open(os.path.join(args.model_dir, 'config.pkl'), 'wb') as f:\n",
    "    pickle.dump(args, f)\n",
    "  print(\"hyperparam. saved.\")\n",
    "\n",
    "  model = Model(args)\n",
    "\n",
    "  # training\n",
    "  with tf.Session(config = config) as sess:\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # args.load_model: Reload a model checkpoint and restore training\n",
    "    if args.load_model is not None:\n",
    "        saver.restore(sess, args.load_model)\n",
    "        print('args.load_model: ',args.load_model) #################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        _, ep_start = args.load_model.rsplit(\"-\", 1)\n",
    "        ep_start = int(ep_start) # the last element of load_model\n",
    "        model_steps = int(ep_start * data_loader.num_batches)\n",
    "    else:\n",
    "        ep_start = 0\n",
    "        model_steps = last_model_steps = 0  # model_steps?????????????\n",
    "\n",
    "    last_time = time.time()\n",
    "\n",
    "    for ep in range(ep_start, args.num_epochs):\n",
    "      ep_loss = []\n",
    "      sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** ep))) #????????\n",
    "      print('data_loader.num_sequences: %d, args.batch_size: %d' %(data_loader.num_sequences, args.batch_size)) ###!!!\n",
    "      for i in range(int(data_loader.num_sequences / args.batch_size)):\n",
    "        idx = ep * data_loader.num_sequences + i * args.batch_size\n",
    "        start = time.time()\n",
    "        x, y, w, c = data_loader.next_batch()\n",
    "\n",
    "        loss_list, model_steps = model.train(\n",
    "          sess=sess, \n",
    "          sequence=x, \n",
    "          targets=y, \n",
    "          weights=w, \n",
    "          texts=c, \n",
    "          subseq_length=args.bptt_length, \n",
    "          step_count=model_steps\n",
    "          ) # def train(self, sess, sequence, targets, weights, texts, subseq_length, step_count)\n",
    "\n",
    "        ep_loss += loss_list\n",
    "\n",
    "        if model_steps - last_model_steps >= 100:\n",
    "          last_model_steps = model_steps\n",
    "          new_time = time.time()\n",
    "          print(\n",
    "            \"Sequence %d/%d (epoch %d), batch %d, train_loss = %.3f, time/(100*batch) = %.3f\" \n",
    "            % (\n",
    "                idx,\n",
    "                args.num_epochs * data_loader.num_sequences,\n",
    "                ep,\n",
    "                model_steps,\n",
    "                np.mean(loss_list),\n",
    "                new_time - last_time\n",
    "              ),\n",
    "            flush=True\n",
    "            )\n",
    "          last_time = new_time\n",
    "      print('x: ' ,type(x) , '; c', type(c) ) ####!!!\n",
    "      print(\"Epoch %d completed, average train loss %.6f\" % (ep, np.mean(ep_loss)))\n",
    "      if not os.path.isdir(args.model_dir):\n",
    "        os.makedirs(args.model_dir)\n",
    "      if (ep+1) % args.save_every == 0:\n",
    "        checkpoint_path = os.path.join(args.model_dir, 'model.ckpt')\n",
    "        saver.save(sess, save_path=checkpoint_path, global_step = (ep+1))\n",
    "        print(\"model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints\n",
      "['a-b c', ' d']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(args.model_dir )\n",
    "str = 'a-b c- d'\n",
    "a = str.rsplit(\"-\", 1)\n",
    "print(a)\n",
    "print(os.path.isdir(args.model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Main func to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--dim_rec', type=int, default=400,\n",
    "                     help='size of RNN hidden state')\n",
    "  parser.add_argument('--num_layers', type=int, default=3,\n",
    "                     help='number of layers in the RNN. ' \\\n",
    "                     'Needs to be larger than 3 for synthesis.')\n",
    "  parser.add_argument('--batch_size', type=int, default=50,\n",
    "                     help='minibatch size')\n",
    "  parser.add_argument('--num_epochs', type=int, default=200,\n",
    "                     help='number of epochs')\n",
    "  parser.add_argument('--save_every', type=int, default=5,\n",
    "                     help='save frequency by epoches')\n",
    "  parser.add_argument('--model_dir', type=str, default='checkpoints',\n",
    "                     help='directory to save model to')\n",
    "  parser.add_argument('--summary_dir', type=str, default='summary',\n",
    "                     help='directory to save tensorboard info')\n",
    "  parser.add_argument('--max_grad_norm', type=float, default=10.,\n",
    "                     help='clip gradients at this value')\n",
    "  parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                     help='learning rate')\n",
    "  parser.add_argument('--decay_rate', type=float, default=1.0,\n",
    "                     help='decay rate for rmsprop')\n",
    "  parser.add_argument('--num_mixture', type=int, default=20,\n",
    "                     help='number of gaussian mixtures')\n",
    "  parser.add_argument('--data_scale', type=float, default=20,\n",
    "                     help='factor to scale raw data down by')\n",
    "  parser.add_argument('--mode', type=str, default='synthesis',\n",
    "                     help='prediction / synthesis' )\n",
    "  parser.add_argument('--load_model', type=str, default=None,\n",
    "                     help='Reload a model checkpoint and restore training.' )\n",
    "  parser.add_argument('--bptt_length', type=int, default=300,\n",
    "                     help='How many steps should the gradients pass back.' )\n",
    "  \n",
    "  args = parser.parse_args(['--num_epochs','4'])\n",
    "\n",
    "  train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 12187, valid data: 0\n",
      "hyperparam. saved.\n",
      "Sequence 1250/48748 (epoch 0), batch 102, train_loss = 1.692, time/(100*batch) = 38.107\n",
      "Sequence 2450/48748 (epoch 0), batch 202, train_loss = 0.651, time/(100*batch) = 38.098\n",
      "Sequence 3700/48748 (epoch 0), batch 304, train_loss = 0.505, time/(100*batch) = 37.955\n",
      "Sequence 4900/48748 (epoch 0), batch 408, train_loss = 0.479, time/(100*batch) = 39.223\n",
      "Sequence 6250/48748 (epoch 0), batch 511, train_loss = 0.798, time/(100*batch) = 37.924\n",
      "Sequence 7500/48748 (epoch 0), batch 612, train_loss = 0.309, time/(100*batch) = 37.890\n",
      "Sequence 8750/48748 (epoch 0), batch 715, train_loss = 0.409, time/(100*batch) = 37.845\n",
      "Sequence 10000/48748 (epoch 0), batch 817, train_loss = 0.303, time/(100*batch) = 37.650\n",
      "Sequence 11300/48748 (epoch 0), batch 919, train_loss = 0.519, time/(100*batch) = 37.166\n",
      "Epoch 0 completed, average train loss 0.649594\n",
      "Sequence 12587/48748 (epoch 1), batch 1022, train_loss = 0.187, time/(100*batch) = 36.691\n",
      "Sequence 13837/48748 (epoch 1), batch 1124, train_loss = -0.153, time/(100*batch) = 38.793\n",
      "Sequence 15137/48748 (epoch 1), batch 1226, train_loss = 0.343, time/(100*batch) = 37.624\n",
      "Sequence 16387/48748 (epoch 1), batch 1328, train_loss = 0.332, time/(100*batch) = 37.617\n",
      "Sequence 17537/48748 (epoch 1), batch 1430, train_loss = 0.169, time/(100*batch) = 37.277\n",
      "Sequence 18887/48748 (epoch 1), batch 1535, train_loss = -0.195, time/(100*batch) = 38.964\n",
      "Sequence 20187/48748 (epoch 1), batch 1637, train_loss = 0.049, time/(100*batch) = 37.793\n",
      "Sequence 21437/48748 (epoch 1), batch 1738, train_loss = 0.857, time/(100*batch) = 36.210\n",
      "Sequence 22687/48748 (epoch 1), batch 1838, train_loss = 0.150, time/(100*batch) = 36.964\n",
      "Sequence 23937/48748 (epoch 1), batch 1938, train_loss = 0.115, time/(100*batch) = 35.651\n",
      "Epoch 1 completed, average train loss 0.209235\n",
      "Sequence 25274/48748 (epoch 2), batch 2039, train_loss = 0.468, time/(100*batch) = 37.036\n",
      "Sequence 26474/48748 (epoch 2), batch 2139, train_loss = 0.001, time/(100*batch) = 36.742\n",
      "Sequence 27774/48748 (epoch 2), batch 2241, train_loss = 0.297, time/(100*batch) = 38.106\n",
      "Sequence 29024/48748 (epoch 2), batch 2345, train_loss = -0.225, time/(100*batch) = 37.899\n",
      "Sequence 30274/48748 (epoch 2), batch 2448, train_loss = 0.147, time/(100*batch) = 38.144\n",
      "Sequence 31574/48748 (epoch 2), batch 2550, train_loss = 0.206, time/(100*batch) = 37.130\n",
      "Sequence 32824/48748 (epoch 2), batch 2651, train_loss = 0.208, time/(100*batch) = 36.316\n",
      "Sequence 34074/48748 (epoch 2), batch 2753, train_loss = -0.135, time/(100*batch) = 36.590\n",
      "Sequence 35374/48748 (epoch 2), batch 2853, train_loss = 0.136, time/(100*batch) = 35.859\n",
      "Epoch 2 completed, average train loss 0.098501\n",
      "Sequence 36711/48748 (epoch 3), batch 2955, train_loss = -0.054, time/(100*batch) = 36.701\n",
      "Sequence 38011/48748 (epoch 3), batch 3058, train_loss = -0.059, time/(100*batch) = 37.257\n",
      "Sequence 39261/48748 (epoch 3), batch 3159, train_loss = 0.060, time/(100*batch) = 36.873\n",
      "Sequence 40511/48748 (epoch 3), batch 3262, train_loss = 0.155, time/(100*batch) = 38.211\n",
      "Sequence 41711/48748 (epoch 3), batch 3365, train_loss = 0.452, time/(100*batch) = 37.424\n",
      "Sequence 43011/48748 (epoch 3), batch 3465, train_loss = 0.162, time/(100*batch) = 35.662\n",
      "Sequence 44261/48748 (epoch 3), batch 3565, train_loss = 0.340, time/(100*batch) = 36.235\n",
      "Sequence 45511/48748 (epoch 3), batch 3665, train_loss = 0.429, time/(100*batch) = 36.289\n",
      "Sequence 46761/48748 (epoch 3), batch 3767, train_loss = 0.254, time/(100*batch) = 36.413\n",
      "Sequence 48061/48748 (epoch 3), batch 3870, train_loss = 0.446, time/(100*batch) = 36.857\n",
      "Epoch 3 completed, average train loss 0.046841\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Alternative training in Ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define arguments \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dim_rec', type=int, default=400,\n",
    "                     help='size of RNN hidden state')\n",
    "parser.add_argument('--num_layers', type=int, default=3,\n",
    "                     help='number of layers in the RNN. ' \\\n",
    "                     'Needs to be larger than 3 for synthesis.')\n",
    "parser.add_argument('--batch_size', type=int, default=50,\n",
    "                     help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200,\n",
    "                     help='number of epochs')\n",
    "parser.add_argument('--save_every', type=int, default=5,\n",
    "                     help='save frequency by epoches')\n",
    "parser.add_argument('--model_dir', type=str, default='checkpoints',\n",
    "                     help='directory to save model to')\n",
    "parser.add_argument('--summary_dir', type=str, default='summary',\n",
    "                     help='directory to save tensorboard info')\n",
    "parser.add_argument('--max_grad_norm', type=float, default=10.,\n",
    "                     help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                     help='learning rate')\n",
    "parser.add_argument('--decay_rate', type=float, default=1.0,\n",
    "                     help='decay rate for rmsprop')\n",
    "parser.add_argument('--num_mixture', type=int, default=20,\n",
    "                     help='number of gaussian mixtures')\n",
    "parser.add_argument('--data_scale', type=float, default=20,\n",
    "                     help='factor to scale raw data down by')\n",
    "parser.add_argument('--mode', type=str, default='synthesis',\n",
    "                     help='prediction / synthesis' )\n",
    "parser.add_argument('--load_model', type=str, default=None,\n",
    "                     help='Reload a model checkpoint and restore training.' )\n",
    "parser.add_argument('--bptt_length', type=int, default=300,\n",
    "                     help='How many steps should the gradients pass back.' )\n",
    "  \n",
    "args = parser.parse_args(['--num_epochs','4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 12187, valid data: 0\n",
      "checkpoints\n",
      "hyperparam. saved.\n",
      "data_loader.num_sequences: 12187, args.batch_size: 50\n",
      "Sequence 1250/48748 (epoch 0), batch 102, train_loss = 1.940, time/(100*batch) = 39.823\n",
      "Sequence 2450/48748 (epoch 0), batch 202, train_loss = 0.683, time/(100*batch) = 40.481\n",
      "Sequence 3700/48748 (epoch 0), batch 304, train_loss = 0.605, time/(100*batch) = 39.665\n",
      "Sequence 4900/48748 (epoch 0), batch 408, train_loss = 0.429, time/(100*batch) = 41.094\n",
      "Sequence 6250/48748 (epoch 0), batch 511, train_loss = 0.787, time/(100*batch) = 39.597\n",
      "Sequence 7500/48748 (epoch 0), batch 612, train_loss = 0.352, time/(100*batch) = 40.004\n",
      "Sequence 8750/48748 (epoch 0), batch 715, train_loss = 0.446, time/(100*batch) = 39.391\n",
      "Sequence 10000/48748 (epoch 0), batch 817, train_loss = 0.297, time/(100*batch) = 39.327\n",
      "Sequence 11300/48748 (epoch 0), batch 919, train_loss = 0.475, time/(100*batch) = 38.534\n",
      "x:  <class 'numpy.ndarray'> ; c <class 'numpy.ndarray'>\n",
      "Epoch 0 completed, average train loss 0.688186\n",
      "data_loader.num_sequences: 12187, args.batch_size: 50\n",
      "Sequence 12587/48748 (epoch 1), batch 1022, train_loss = 0.174, time/(100*batch) = 38.684\n",
      "Sequence 13837/48748 (epoch 1), batch 1124, train_loss = -0.194, time/(100*batch) = 40.528\n",
      "Sequence 15137/48748 (epoch 1), batch 1226, train_loss = 0.341, time/(100*batch) = 39.386\n",
      "Sequence 16387/48748 (epoch 1), batch 1328, train_loss = 0.310, time/(100*batch) = 39.998\n",
      "Sequence 17537/48748 (epoch 1), batch 1430, train_loss = 0.144, time/(100*batch) = 39.000\n",
      "Sequence 18887/48748 (epoch 1), batch 1535, train_loss = -0.230, time/(100*batch) = 40.458\n",
      "Sequence 20187/48748 (epoch 1), batch 1637, train_loss = 0.045, time/(100*batch) = 39.198\n",
      "Sequence 21437/48748 (epoch 1), batch 1738, train_loss = 0.807, time/(100*batch) = 37.559\n",
      "Sequence 22687/48748 (epoch 1), batch 1838, train_loss = 0.111, time/(100*batch) = 38.195\n",
      "Sequence 23937/48748 (epoch 1), batch 1938, train_loss = 0.128, time/(100*batch) = 37.302\n",
      "x:  <class 'numpy.ndarray'> ; c <class 'numpy.ndarray'>\n",
      "Epoch 1 completed, average train loss 0.202451\n",
      "model saved.\n",
      "data_loader.num_sequences: 12187, args.batch_size: 50\n",
      "Sequence 25274/48748 (epoch 2), batch 2039, train_loss = 0.454, time/(100*batch) = 39.341\n",
      "Sequence 26474/48748 (epoch 2), batch 2139, train_loss = -0.024, time/(100*batch) = 38.219\n",
      "Sequence 27774/48748 (epoch 2), batch 2241, train_loss = 0.282, time/(100*batch) = 39.859\n",
      "Sequence 29024/48748 (epoch 2), batch 2345, train_loss = -0.232, time/(100*batch) = 40.122\n",
      "Sequence 30274/48748 (epoch 2), batch 2448, train_loss = 0.159, time/(100*batch) = 40.400\n",
      "Sequence 31574/48748 (epoch 2), batch 2550, train_loss = 0.220, time/(100*batch) = 39.731\n",
      "Sequence 32824/48748 (epoch 2), batch 2651, train_loss = 0.189, time/(100*batch) = 38.616\n",
      "Sequence 34074/48748 (epoch 2), batch 2753, train_loss = -0.127, time/(100*batch) = 38.956\n",
      "Sequence 35374/48748 (epoch 2), batch 2853, train_loss = 0.162, time/(100*batch) = 38.053\n",
      "x:  <class 'numpy.ndarray'> ; c <class 'numpy.ndarray'>\n",
      "Epoch 2 completed, average train loss 0.088459\n",
      "data_loader.num_sequences: 12187, args.batch_size: 50\n",
      "Sequence 36711/48748 (epoch 3), batch 2955, train_loss = -0.072, time/(100*batch) = 38.359\n",
      "Sequence 38011/48748 (epoch 3), batch 3058, train_loss = -0.066, time/(100*batch) = 39.828\n",
      "Sequence 39261/48748 (epoch 3), batch 3159, train_loss = 0.051, time/(100*batch) = 38.570\n",
      "Sequence 40511/48748 (epoch 3), batch 3262, train_loss = 0.153, time/(100*batch) = 39.366\n",
      "Sequence 41711/48748 (epoch 3), batch 3365, train_loss = 0.506, time/(100*batch) = 38.980\n",
      "Sequence 43011/48748 (epoch 3), batch 3465, train_loss = 0.151, time/(100*batch) = 36.738\n",
      "Sequence 44261/48748 (epoch 3), batch 3565, train_loss = 0.322, time/(100*batch) = 37.438\n",
      "Sequence 45511/48748 (epoch 3), batch 3665, train_loss = 0.403, time/(100*batch) = 36.921\n",
      "Sequence 46761/48748 (epoch 3), batch 3767, train_loss = 0.244, time/(100*batch) = 37.509\n",
      "Sequence 48061/48748 (epoch 3), batch 3870, train_loss = 0.457, time/(100*batch) = 37.946\n",
      "x:  <class 'numpy.ndarray'> ; c <class 'numpy.ndarray'>\n",
      "Epoch 3 completed, average train loss 0.039291\n",
      "model saved.\n"
     ]
    }
   ],
   "source": [
    "args.num_epochs = 4\n",
    "args.save_every = 2\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_index1: [0. 1.]\n",
      "char_index2: [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "char_index3: [[[0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[0. 1.]\n",
      "  [0. 1.]\n",
      "  [0. 1.]]]\n",
      "(5, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "def expand_and_repeat(data, axis, num_repeat):\n",
    "    data = np.expand_dims(data, axis=axis)\n",
    "    data = np.repeat(data, repeats=num_repeat, axis=axis)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "max_char_len = 2\n",
    "num_mixture = 3\n",
    "batch_size = 5\n",
    "char_index = np.arange(max_char_len, dtype=np.float32)\n",
    "print('char_index1:',char_index)\n",
    "char_index = expand_and_repeat(char_index, 0, num_mixture)\n",
    "print('char_index2:',char_index)\n",
    "char_index = expand_and_repeat(char_index, 0, batch_size)\n",
    "print('char_index3:',char_index)\n",
    "print(char_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"foo\"):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(\"foo\", reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "assert v1 == v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable foo/v already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-11-ef6e0cc39327>\", line 2, in <module>\n    v = tf.get_variable(\"v\", [1])\n  File \"/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ef6e0cc39327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mv1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    740\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 742\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable foo/v already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-11-ef6e0cc39327>\", line 2, in <module>\n    v = tf.get_variable(\"v\", [1])\n  File \"/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"foo\"):\n",
    "    v = tf.get_variable(\"v\", [1])\n",
    "with tf.variable_scope(\"foo\", reuse=True):\n",
    "    v1 = tf.get_variable(\"v\", [1])\n",
    "assert v1 == v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
