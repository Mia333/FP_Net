{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\" The main script for training\n",
    "\n",
    "requirement: python 2, tensorflow r1.4 or r1.2 \n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib.framework import nest\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "  data_loader = DataLoader(args.batch_size, args.data_scale, args.bptt_length) # batch_size=10, bptt_length=120\n",
    "  data_loader.reset_batch_pointer()\n",
    "\n",
    "  if args.model_dir != '' and not os.path.exists(args.model_dir):\n",
    "    os.makedirs(args.model_dir)\n",
    "\n",
    "  with open(os.path.join(args.model_dir, 'config.pkl'), 'wb') as f:\n",
    "    pickle.dump(args, f)\n",
    "  print(\"hyperparam. saved.\")\n",
    "\n",
    "  model = Model(args)\n",
    "\n",
    "  # training\n",
    "  with tf.Session(config=config) as sess:\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    if args.load_model is not None:\n",
    "        saver.restore(sess, args.load_model)\n",
    "        _, ep_start = args.load_model.rsplit(\"-\", 1)\n",
    "        ep_start = int(ep_start)\n",
    "        model_steps = int(ep_start * data_loader.num_batches)\n",
    "    else:\n",
    "        ep_start = 0\n",
    "        model_steps = last_model_steps = 0\n",
    "\n",
    "    last_time = time.time()\n",
    "\n",
    "    for ep in range(ep_start, args.num_epochs):\n",
    "      ep_loss = []\n",
    "      sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** ep)))\n",
    "\n",
    "      for i in range(int(data_loader.num_sequences / args.batch_size)):\n",
    "        idx = ep * data_loader.num_sequences + i * args.batch_size\n",
    "        start = time.time()\n",
    "        x, y, w, c, lens = data_loader.next_batch() #x :[batch_size,time_step????????,6]\n",
    "\n",
    "        loss_list, model_steps = model.train(\n",
    "          sess=sess, \n",
    "          sequence=x, \n",
    "          targets=y, \n",
    "          weights=w, \n",
    "          conditions=c, \n",
    "          subseq_length=args.bptt_length, \n",
    "          step_count=model_steps\n",
    "          )\n",
    "\n",
    "        ep_loss += loss_list\n",
    "\n",
    "        if model_steps - last_model_steps >= 100:\n",
    "          new_time = time.time()\n",
    "          print(\n",
    "            \"Sequence %d/%d (epoch %d), batch %d, train_loss = %.3f, time/batch = %.3f\" \n",
    "            % (\n",
    "                idx,\n",
    "                args.num_epochs * data_loader.num_sequences, #.num_sequences = len(self.data[\"inputs\"]) #[time, 3*DoF*joints]\n",
    "                ep,\n",
    "                model_steps,\n",
    "                np.mean(loss_list),\n",
    "                (new_time - last_time) / (model_steps - last_model_steps)\n",
    "              )\n",
    "            ) #data_loader.num_sequences = 32632\n",
    "          sys.stdout.flush()\n",
    "          last_model_steps = model_steps\n",
    "          last_time = new_time\n",
    "      print(\"Epoch %d completed, average train loss %.6f, learning rate %.4f\" % (ep, np.mean(ep_loss), args.learning_rate * (args.decay_rate ** ep)))\n",
    "      sys.stdout.flush()\n",
    "      if not os.path.isdir(args.model_dir):\n",
    "        os.makedirs(args.model_dir)\n",
    "      if (ep+1) % args.save_every == 0:\n",
    "        checkpoint_path = os.path.join(args.model_dir, 'model.ckpt')\n",
    "        saver.save(sess, save_path=checkpoint_path, global_step = (ep+1))\n",
    "        print(\"model saved.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. train method1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--dim_rec', type=int, default=128,\n",
    "                     help='size of RNN hidden state')\n",
    "  parser.add_argument('--num_layers', type=int, default=2,\n",
    "                     help='number of layers in the RNN. ')\n",
    "  parser.add_argument('--batch_size', type=int, default=10,\n",
    "                     help='minibatch size')\n",
    "  parser.add_argument('--num_epochs', type=int, default=200,\n",
    "                     help='number of epochs')\n",
    "  parser.add_argument('--save_every', type=int, default=10,\n",
    "                     help='save frequency by epoches')\n",
    "  parser.add_argument('--model_dir', type=str, default='checkpoints',\n",
    "                     help='directory to save model to')\n",
    "  parser.add_argument('--summary_dir', type=str, default='summary',\n",
    "                     help='directory to save tensorboard info')\n",
    "  parser.add_argument('--max_grad_norm', type=float, default=1.,\n",
    "                     help='clip gradients at this value')\n",
    "  parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                     help='learning rate')\n",
    "  parser.add_argument('--decay_rate', type=float, default=1.0,\n",
    "                     help='decay rate for the optimizer')\n",
    "  parser.add_argument('--num_mixture', type=int, default=2,\n",
    "                     help='number of gaussian mixtures')\n",
    "  parser.add_argument('--data_scale', type=float, default=1000,\n",
    "                     help='factor to scale raw data down by')\n",
    "  parser.add_argument('--load_model', type=str, default=None,\n",
    "                     help='Reload a model checkpoint and restore training.' )\n",
    "  parser.add_argument('--bptt_length', type=int, default=120,\n",
    "                     help='How many steps should the gradients pass back.' )\n",
    "  parser.add_argument('--loss_form', type=str, default='mse',\n",
    "                     help='mse / gmm' )\n",
    "  parser.add_argument('--constraint_factor', type=float, default=0.,\n",
    "                     help='the weight for constraint term in the cost function.' )\n",
    "  \n",
    "  args = parser.parse_args(['--num_epochs','200'])\n",
    "\n",
    "  args.num_epochs = 100\n",
    "  args.save_every = 5\n",
    "\n",
    "\n",
    "  train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal length of the training data is 169\n",
      "Training data case distribution: [5348, 5108, 4904, 1208, 1260, 1224, 916, 1180, 1216, 1044, 876, 456, 1264, 992, 1080, 932, 1008, 420, 1160, 1036]\n",
      "Validation data case distribution: [70, 67, 65, 15, 17, 16, 12, 15, 16, 14, 12, 6, 17, 13, 14, 13, 14, 5, 15, 13]\n",
      "Shuffling training data...\n",
      "Shuffling training data...\n",
      "hyperparam. saved.\n",
      "Number of trainable variables 6\n",
      "[<tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(134, 512) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'out_W:0' shape=(128, 3) dtype=float32_ref>, <tf.Variable 'out_b:0' shape=(3,) dtype=float32_ref>]\n",
      "Sequence 980/3263200 (epoch 0), batch 100, train_loss = 6.122, time/batch = 0.095\n",
      "Sequence 1950/3263200 (epoch 0), batch 200, train_loss = 9.091, time/batch = 0.084\n",
      "Sequence 2940/3263200 (epoch 0), batch 300, train_loss = 6.049, time/batch = 0.086\n",
      "Sequence 3940/3263200 (epoch 0), batch 400, train_loss = 4.646, time/batch = 0.087\n",
      "Sequence 4920/3263200 (epoch 0), batch 500, train_loss = 3.681, time/batch = 0.085\n",
      "Sequence 5880/3263200 (epoch 0), batch 600, train_loss = 3.973, time/batch = 0.085\n",
      "Sequence 6870/3263200 (epoch 0), batch 700, train_loss = 2.323, time/batch = 0.087\n",
      "Sequence 7850/3263200 (epoch 0), batch 800, train_loss = 2.012, time/batch = 0.084\n",
      "Sequence 8820/3263200 (epoch 0), batch 900, train_loss = 3.019, time/batch = 0.085\n",
      "Sequence 9800/3263200 (epoch 0), batch 1000, train_loss = 2.444, time/batch = 0.086\n",
      "Sequence 10800/3263200 (epoch 0), batch 1100, train_loss = 2.677, time/batch = 0.083\n",
      "Sequence 11790/3263200 (epoch 0), batch 1200, train_loss = 3.406, time/batch = 0.085\n",
      "Sequence 12780/3263200 (epoch 0), batch 1300, train_loss = 2.530, time/batch = 0.085\n",
      "Sequence 13780/3263200 (epoch 0), batch 1400, train_loss = 2.337, time/batch = 0.086\n",
      "Sequence 14770/3263200 (epoch 0), batch 1500, train_loss = 3.646, time/batch = 0.086\n",
      "Sequence 15760/3263200 (epoch 0), batch 1600, train_loss = 2.816, time/batch = 0.085\n",
      "Sequence 16730/3263200 (epoch 0), batch 1700, train_loss = 2.224, time/batch = 0.084\n",
      "Sequence 17720/3263200 (epoch 0), batch 1800, train_loss = 2.745, time/batch = 0.084\n",
      "Sequence 18710/3263200 (epoch 0), batch 1900, train_loss = 1.603, time/batch = 0.083\n",
      "Sequence 19700/3263200 (epoch 0), batch 2000, train_loss = 3.192, time/batch = 0.083\n",
      "Sequence 20660/3263200 (epoch 0), batch 2100, train_loss = 2.894, time/batch = 0.082\n",
      "Sequence 21630/3263200 (epoch 0), batch 2200, train_loss = 2.249, time/batch = 0.084\n",
      "Sequence 22600/3263200 (epoch 0), batch 2300, train_loss = 2.985, time/batch = 0.085\n",
      "Sequence 23520/3263200 (epoch 0), batch 2400, train_loss = 1.753, time/batch = 0.085\n",
      "Sequence 24490/3263200 (epoch 0), batch 2500, train_loss = 2.024, time/batch = 0.085\n",
      "Sequence 25490/3263200 (epoch 0), batch 2600, train_loss = 3.133, time/batch = 0.090\n",
      "Sequence 26490/3263200 (epoch 0), batch 2700, train_loss = 3.411, time/batch = 0.089\n",
      "Sequence 27490/3263200 (epoch 0), batch 2800, train_loss = 4.153, time/batch = 0.087\n",
      "Sequence 28470/3263200 (epoch 0), batch 2900, train_loss = 3.511, time/batch = 0.086\n",
      "Sequence 29440/3263200 (epoch 0), batch 3000, train_loss = 2.288, time/batch = 0.084\n",
      "Sequence 30400/3263200 (epoch 0), batch 3100, train_loss = 2.555, time/batch = 0.089\n",
      "Sequence 31370/3263200 (epoch 0), batch 3200, train_loss = 1.111, time/batch = 0.089\n",
      "Sequence 32360/3263200 (epoch 0), batch 3300, train_loss = 3.075, time/batch = 0.087\n",
      "Epoch 0 completed, average train loss 3.260217, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 33352/3263200 (epoch 1), batch 3400, train_loss = 2.141, time/batch = 0.089\n",
      "Sequence 34332/3263200 (epoch 1), batch 3501, train_loss = 0.972, time/batch = 0.085\n",
      "Sequence 35322/3263200 (epoch 1), batch 3601, train_loss = 2.060, time/batch = 0.087\n",
      "Sequence 36302/3263200 (epoch 1), batch 3701, train_loss = 3.368, time/batch = 0.088\n",
      "Sequence 37302/3263200 (epoch 1), batch 3801, train_loss = 1.711, time/batch = 0.088\n",
      "Sequence 38292/3263200 (epoch 1), batch 3901, train_loss = 1.916, time/batch = 0.088\n",
      "Sequence 39262/3263200 (epoch 1), batch 4001, train_loss = 1.558, time/batch = 0.085\n",
      "Sequence 40222/3263200 (epoch 1), batch 4101, train_loss = 3.218, time/batch = 0.086\n",
      "Sequence 41222/3263200 (epoch 1), batch 4201, train_loss = 2.871, time/batch = 0.087\n",
      "Sequence 42192/3263200 (epoch 1), batch 4301, train_loss = 2.448, time/batch = 0.091\n",
      "Sequence 43192/3263200 (epoch 1), batch 4401, train_loss = 1.932, time/batch = 0.091\n",
      "Sequence 44182/3263200 (epoch 1), batch 4501, train_loss = 3.909, time/batch = 0.088\n",
      "Sequence 45172/3263200 (epoch 1), batch 4601, train_loss = 1.342, time/batch = 0.087\n",
      "Sequence 46142/3263200 (epoch 1), batch 4701, train_loss = 1.509, time/batch = 0.088\n",
      "Sequence 47112/3263200 (epoch 1), batch 4801, train_loss = 3.243, time/batch = 0.091\n",
      "Sequence 48072/3263200 (epoch 1), batch 4902, train_loss = 3.712, time/batch = 0.088\n",
      "Sequence 49032/3263200 (epoch 1), batch 5002, train_loss = 2.912, time/batch = 0.092\n",
      "Sequence 50032/3263200 (epoch 1), batch 5102, train_loss = 2.034, time/batch = 0.092\n",
      "Sequence 51032/3263200 (epoch 1), batch 5202, train_loss = 1.572, time/batch = 0.091\n",
      "Sequence 52032/3263200 (epoch 1), batch 5302, train_loss = 1.928, time/batch = 0.093\n",
      "Sequence 53012/3263200 (epoch 1), batch 5402, train_loss = 3.958, time/batch = 0.093\n",
      "Sequence 53982/3263200 (epoch 1), batch 5502, train_loss = 2.453, time/batch = 0.092\n",
      "Sequence 54972/3263200 (epoch 1), batch 5602, train_loss = 2.615, time/batch = 0.089\n",
      "Sequence 55952/3263200 (epoch 1), batch 5702, train_loss = 1.631, time/batch = 0.091\n",
      "Sequence 56952/3263200 (epoch 1), batch 5802, train_loss = 1.964, time/batch = 0.089\n",
      "Sequence 57912/3263200 (epoch 1), batch 5902, train_loss = 2.563, time/batch = 0.089\n",
      "Sequence 58892/3263200 (epoch 1), batch 6002, train_loss = 2.691, time/batch = 0.091\n",
      "Sequence 59872/3263200 (epoch 1), batch 6103, train_loss = 0.693, time/batch = 0.092\n",
      "Sequence 60862/3263200 (epoch 1), batch 6203, train_loss = 3.665, time/batch = 0.091\n",
      "Sequence 61842/3263200 (epoch 1), batch 6303, train_loss = 2.170, time/batch = 0.089\n",
      "Sequence 62822/3263200 (epoch 1), batch 6403, train_loss = 3.060, time/batch = 0.090\n",
      "Sequence 63812/3263200 (epoch 1), batch 6503, train_loss = 3.447, time/batch = 0.090\n",
      "Sequence 64782/3263200 (epoch 1), batch 6603, train_loss = 1.319, time/batch = 0.092\n",
      "Epoch 1 completed, average train loss 2.385857, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 65764/3263200 (epoch 2), batch 6703, train_loss = 2.286, time/batch = 0.095\n",
      "Sequence 66744/3263200 (epoch 2), batch 6803, train_loss = 1.578, time/batch = 0.091\n",
      "Sequence 67744/3263200 (epoch 2), batch 6903, train_loss = 1.405, time/batch = 0.095\n",
      "Sequence 68734/3263200 (epoch 2), batch 7003, train_loss = 1.549, time/batch = 0.102\n",
      "Sequence 69714/3263200 (epoch 2), batch 7103, train_loss = 5.306, time/batch = 0.101\n",
      "Sequence 70704/3263200 (epoch 2), batch 7203, train_loss = 1.811, time/batch = 0.101\n",
      "Sequence 71684/3263200 (epoch 2), batch 7303, train_loss = 1.883, time/batch = 0.101\n",
      "Sequence 72654/3263200 (epoch 2), batch 7403, train_loss = 1.382, time/batch = 0.099\n",
      "Sequence 73644/3263200 (epoch 2), batch 7503, train_loss = 2.482, time/batch = 0.102\n",
      "Sequence 74634/3263200 (epoch 2), batch 7603, train_loss = 1.798, time/batch = 0.101\n",
      "Sequence 75604/3263200 (epoch 2), batch 7703, train_loss = 2.716, time/batch = 0.101\n",
      "Sequence 76584/3263200 (epoch 2), batch 7803, train_loss = 1.547, time/batch = 0.102\n",
      "Sequence 77574/3263200 (epoch 2), batch 7903, train_loss = 4.017, time/batch = 0.102\n",
      "Sequence 78564/3263200 (epoch 2), batch 8003, train_loss = 3.048, time/batch = 0.101\n",
      "Sequence 79524/3263200 (epoch 2), batch 8103, train_loss = 1.467, time/batch = 0.101\n",
      "Sequence 80474/3263200 (epoch 2), batch 8203, train_loss = 2.182, time/batch = 0.102\n",
      "Sequence 81474/3263200 (epoch 2), batch 8303, train_loss = 1.712, time/batch = 0.098\n",
      "Sequence 82474/3263200 (epoch 2), batch 8403, train_loss = 1.906, time/batch = 0.101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 83444/3263200 (epoch 2), batch 8503, train_loss = 2.764, time/batch = 0.100\n",
      "Sequence 84404/3263200 (epoch 2), batch 8603, train_loss = 1.936, time/batch = 0.100\n",
      "Sequence 85384/3263200 (epoch 2), batch 8703, train_loss = 1.923, time/batch = 0.102\n",
      "Sequence 86374/3263200 (epoch 2), batch 8803, train_loss = 2.569, time/batch = 0.099\n",
      "Sequence 87354/3263200 (epoch 2), batch 8903, train_loss = 2.102, time/batch = 0.099\n",
      "Sequence 88344/3263200 (epoch 2), batch 9003, train_loss = 1.870, time/batch = 0.099\n",
      "Sequence 89324/3263200 (epoch 2), batch 9103, train_loss = 3.633, time/batch = 0.100\n",
      "Sequence 90274/3263200 (epoch 2), batch 9203, train_loss = 2.269, time/batch = 0.103\n",
      "Sequence 91264/3263200 (epoch 2), batch 9303, train_loss = 1.620, time/batch = 0.100\n",
      "Sequence 92264/3263200 (epoch 2), batch 9403, train_loss = 3.496, time/batch = 0.098\n",
      "Sequence 93264/3263200 (epoch 2), batch 9503, train_loss = 2.104, time/batch = 0.099\n",
      "Sequence 94224/3263200 (epoch 2), batch 9603, train_loss = 1.845, time/batch = 0.100\n",
      "Sequence 95194/3263200 (epoch 2), batch 9703, train_loss = 1.326, time/batch = 0.101\n",
      "Sequence 96174/3263200 (epoch 2), batch 9803, train_loss = 3.043, time/batch = 0.104\n",
      "Sequence 97144/3263200 (epoch 2), batch 9903, train_loss = 2.422, time/batch = 0.101\n",
      "Epoch 2 completed, average train loss 2.160677, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 98116/3263200 (epoch 3), batch 10003, train_loss = 2.351, time/batch = 0.101\n",
      "Sequence 99086/3263200 (epoch 3), batch 10103, train_loss = 1.528, time/batch = 0.100\n",
      "Sequence 100056/3263200 (epoch 3), batch 10203, train_loss = 1.574, time/batch = 0.098\n",
      "Sequence 101026/3263200 (epoch 3), batch 10303, train_loss = 1.737, time/batch = 0.102\n",
      "Sequence 102026/3263200 (epoch 3), batch 10403, train_loss = 2.659, time/batch = 0.098\n",
      "Sequence 103016/3263200 (epoch 3), batch 10503, train_loss = 1.438, time/batch = 0.100\n",
      "Sequence 104006/3263200 (epoch 3), batch 10603, train_loss = 1.612, time/batch = 0.100\n",
      "Sequence 104986/3263200 (epoch 3), batch 10703, train_loss = 1.952, time/batch = 0.099\n",
      "Sequence 105976/3263200 (epoch 3), batch 10803, train_loss = 1.605, time/batch = 0.099\n",
      "Sequence 106976/3263200 (epoch 3), batch 10903, train_loss = 2.857, time/batch = 0.101\n",
      "Sequence 107936/3263200 (epoch 3), batch 11003, train_loss = 2.472, time/batch = 0.101\n",
      "Sequence 108906/3263200 (epoch 3), batch 11103, train_loss = 1.331, time/batch = 0.102\n",
      "Sequence 109886/3263200 (epoch 3), batch 11203, train_loss = 1.987, time/batch = 0.099\n",
      "Sequence 110866/3263200 (epoch 3), batch 11303, train_loss = 2.826, time/batch = 0.101\n",
      "Sequence 111836/3263200 (epoch 3), batch 11403, train_loss = 1.769, time/batch = 0.102\n",
      "Sequence 112786/3263200 (epoch 3), batch 11503, train_loss = 1.058, time/batch = 0.098\n",
      "Sequence 113756/3263200 (epoch 3), batch 11603, train_loss = 1.997, time/batch = 0.099\n",
      "Sequence 114736/3263200 (epoch 3), batch 11703, train_loss = 1.278, time/batch = 0.102\n",
      "Sequence 115706/3263200 (epoch 3), batch 11803, train_loss = 1.757, time/batch = 0.100\n",
      "Sequence 116706/3263200 (epoch 3), batch 11903, train_loss = 2.242, time/batch = 0.101\n",
      "Sequence 117656/3263200 (epoch 3), batch 12003, train_loss = 2.146, time/batch = 0.102\n",
      "Sequence 118646/3263200 (epoch 3), batch 12103, train_loss = 1.388, time/batch = 0.100\n",
      "Sequence 119626/3263200 (epoch 3), batch 12203, train_loss = 1.094, time/batch = 0.102\n",
      "Sequence 120616/3263200 (epoch 3), batch 12303, train_loss = 2.855, time/batch = 0.102\n",
      "Sequence 121606/3263200 (epoch 3), batch 12403, train_loss = 2.073, time/batch = 0.101\n",
      "Sequence 122586/3263200 (epoch 3), batch 12503, train_loss = 2.954, time/batch = 0.101\n",
      "Sequence 123576/3263200 (epoch 3), batch 12603, train_loss = 1.841, time/batch = 0.098\n",
      "Sequence 124566/3263200 (epoch 3), batch 12703, train_loss = 1.405, time/batch = 0.099\n",
      "Sequence 125556/3263200 (epoch 3), batch 12804, train_loss = 1.076, time/batch = 0.098\n",
      "Sequence 126546/3263200 (epoch 3), batch 12904, train_loss = 1.395, time/batch = 0.101\n",
      "Sequence 127516/3263200 (epoch 3), batch 13004, train_loss = 1.393, time/batch = 0.100\n",
      "Sequence 128516/3263200 (epoch 3), batch 13104, train_loss = 2.399, time/batch = 0.099\n",
      "Sequence 129516/3263200 (epoch 3), batch 13204, train_loss = 1.992, time/batch = 0.103\n",
      "Sequence 130496/3263200 (epoch 3), batch 13304, train_loss = 1.971, time/batch = 0.102\n",
      "Epoch 3 completed, average train loss 2.020145, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 131478/3263200 (epoch 4), batch 13404, train_loss = 1.302, time/batch = 0.100\n",
      "Sequence 132478/3263200 (epoch 4), batch 13504, train_loss = 2.103, time/batch = 0.102\n",
      "Sequence 133478/3263200 (epoch 4), batch 13604, train_loss = 1.244, time/batch = 0.101\n",
      "Sequence 134478/3263200 (epoch 4), batch 13704, train_loss = 2.795, time/batch = 0.099\n",
      "Sequence 135448/3263200 (epoch 4), batch 13804, train_loss = 2.579, time/batch = 0.100\n",
      "Sequence 136438/3263200 (epoch 4), batch 13904, train_loss = 1.359, time/batch = 0.104\n",
      "Sequence 137418/3263200 (epoch 4), batch 14004, train_loss = 1.946, time/batch = 0.100\n",
      "Sequence 138378/3263200 (epoch 4), batch 14104, train_loss = 1.827, time/batch = 0.102\n",
      "Sequence 139358/3263200 (epoch 4), batch 14204, train_loss = 1.425, time/batch = 0.099\n",
      "Sequence 140358/3263200 (epoch 4), batch 14304, train_loss = 2.179, time/batch = 0.101\n",
      "Sequence 141328/3263200 (epoch 4), batch 14404, train_loss = 2.438, time/batch = 0.102\n",
      "Sequence 142308/3263200 (epoch 4), batch 14504, train_loss = 1.952, time/batch = 0.100\n",
      "Sequence 143308/3263200 (epoch 4), batch 14604, train_loss = 1.996, time/batch = 0.099\n",
      "Sequence 144288/3263200 (epoch 4), batch 14704, train_loss = 1.598, time/batch = 0.101\n",
      "Sequence 145248/3263200 (epoch 4), batch 14804, train_loss = 1.221, time/batch = 0.099\n",
      "Sequence 146228/3263200 (epoch 4), batch 14904, train_loss = 1.951, time/batch = 0.099\n",
      "Sequence 147198/3263200 (epoch 4), batch 15004, train_loss = 1.009, time/batch = 0.100\n",
      "Sequence 148158/3263200 (epoch 4), batch 15104, train_loss = 2.976, time/batch = 0.101\n",
      "Sequence 149158/3263200 (epoch 4), batch 15204, train_loss = 2.035, time/batch = 0.099\n",
      "Sequence 150138/3263200 (epoch 4), batch 15304, train_loss = 1.355, time/batch = 0.101\n",
      "Sequence 151118/3263200 (epoch 4), batch 15404, train_loss = 2.049, time/batch = 0.101\n",
      "Sequence 152108/3263200 (epoch 4), batch 15504, train_loss = 0.996, time/batch = 0.103\n",
      "Sequence 153098/3263200 (epoch 4), batch 15604, train_loss = 1.788, time/batch = 0.102\n",
      "Sequence 154048/3263200 (epoch 4), batch 15704, train_loss = 0.929, time/batch = 0.099\n",
      "Sequence 155018/3263200 (epoch 4), batch 15804, train_loss = 0.909, time/batch = 0.101\n",
      "Sequence 155998/3263200 (epoch 4), batch 15904, train_loss = 1.269, time/batch = 0.099\n",
      "Sequence 156978/3263200 (epoch 4), batch 16004, train_loss = 2.143, time/batch = 0.100\n",
      "Sequence 157978/3263200 (epoch 4), batch 16104, train_loss = 1.978, time/batch = 0.100\n",
      "Sequence 158968/3263200 (epoch 4), batch 16204, train_loss = 1.627, time/batch = 0.100\n",
      "Sequence 159968/3263200 (epoch 4), batch 16304, train_loss = 1.039, time/batch = 0.100\n",
      "Sequence 160948/3263200 (epoch 4), batch 16404, train_loss = 2.948, time/batch = 0.100\n",
      "Sequence 161928/3263200 (epoch 4), batch 16504, train_loss = 1.562, time/batch = 0.098\n",
      "Sequence 162878/3263200 (epoch 4), batch 16604, train_loss = 1.454, time/batch = 0.099\n",
      "Epoch 4 completed, average train loss 1.899271, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 163870/3263200 (epoch 5), batch 16704, train_loss = 1.411, time/batch = 0.102\n",
      "Sequence 164860/3263200 (epoch 5), batch 16804, train_loss = 2.159, time/batch = 0.102\n",
      "Sequence 165810/3263200 (epoch 5), batch 16904, train_loss = 2.050, time/batch = 0.099\n",
      "Sequence 166790/3263200 (epoch 5), batch 17004, train_loss = 1.836, time/batch = 0.101\n",
      "Sequence 167770/3263200 (epoch 5), batch 17104, train_loss = 1.591, time/batch = 0.100\n",
      "Sequence 168730/3263200 (epoch 5), batch 17204, train_loss = 2.831, time/batch = 0.102\n",
      "Sequence 169710/3263200 (epoch 5), batch 17304, train_loss = 1.789, time/batch = 0.100\n",
      "Sequence 170700/3263200 (epoch 5), batch 17404, train_loss = 2.246, time/batch = 0.102\n",
      "Sequence 171690/3263200 (epoch 5), batch 17504, train_loss = 1.298, time/batch = 0.100\n",
      "Sequence 172660/3263200 (epoch 5), batch 17604, train_loss = 1.870, time/batch = 0.103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 173650/3263200 (epoch 5), batch 17704, train_loss = 1.514, time/batch = 0.100\n",
      "Sequence 174620/3263200 (epoch 5), batch 17804, train_loss = 2.613, time/batch = 0.102\n",
      "Sequence 175610/3263200 (epoch 5), batch 17904, train_loss = 1.627, time/batch = 0.101\n",
      "Sequence 176610/3263200 (epoch 5), batch 18004, train_loss = 1.280, time/batch = 0.100\n",
      "Sequence 177580/3263200 (epoch 5), batch 18104, train_loss = 1.817, time/batch = 0.100\n",
      "Sequence 178560/3263200 (epoch 5), batch 18204, train_loss = 2.135, time/batch = 0.098\n",
      "Sequence 179540/3263200 (epoch 5), batch 18304, train_loss = 2.746, time/batch = 0.099\n",
      "Sequence 180530/3263200 (epoch 5), batch 18404, train_loss = 2.050, time/batch = 0.101\n",
      "Sequence 181520/3263200 (epoch 5), batch 18504, train_loss = 4.436, time/batch = 0.100\n",
      "Sequence 182520/3263200 (epoch 5), batch 18604, train_loss = 1.284, time/batch = 0.101\n",
      "Sequence 183470/3263200 (epoch 5), batch 18704, train_loss = 1.550, time/batch = 0.101\n",
      "Sequence 184460/3263200 (epoch 5), batch 18804, train_loss = 2.221, time/batch = 0.101\n",
      "Sequence 185460/3263200 (epoch 5), batch 18904, train_loss = 4.868, time/batch = 0.100\n",
      "Sequence 186450/3263200 (epoch 5), batch 19004, train_loss = 1.454, time/batch = 0.104\n",
      "Sequence 187430/3263200 (epoch 5), batch 19104, train_loss = 1.142, time/batch = 0.100\n",
      "Sequence 188400/3263200 (epoch 5), batch 19204, train_loss = 1.892, time/batch = 0.098\n",
      "Sequence 189370/3263200 (epoch 5), batch 19304, train_loss = 2.255, time/batch = 0.099\n",
      "Sequence 190360/3263200 (epoch 5), batch 19404, train_loss = 2.159, time/batch = 0.101\n",
      "Sequence 191340/3263200 (epoch 5), batch 19504, train_loss = 0.970, time/batch = 0.100\n",
      "Sequence 192330/3263200 (epoch 5), batch 19604, train_loss = 1.602, time/batch = 0.099\n",
      "Sequence 193290/3263200 (epoch 5), batch 19704, train_loss = 1.392, time/batch = 0.100\n",
      "Sequence 194280/3263200 (epoch 5), batch 19804, train_loss = 1.932, time/batch = 0.099\n",
      "Sequence 195260/3263200 (epoch 5), batch 19904, train_loss = 2.421, time/batch = 0.099\n",
      "Epoch 5 completed, average train loss 1.796115, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 196232/3263200 (epoch 6), batch 20004, train_loss = 1.146, time/batch = 0.100\n",
      "Sequence 197202/3263200 (epoch 6), batch 20104, train_loss = 1.699, time/batch = 0.099\n",
      "Sequence 198202/3263200 (epoch 6), batch 20204, train_loss = 1.451, time/batch = 0.100\n",
      "Sequence 199202/3263200 (epoch 6), batch 20304, train_loss = 1.662, time/batch = 0.101\n",
      "Sequence 200192/3263200 (epoch 6), batch 20404, train_loss = 0.956, time/batch = 0.099\n",
      "Sequence 201162/3263200 (epoch 6), batch 20504, train_loss = 0.908, time/batch = 0.102\n",
      "Sequence 202132/3263200 (epoch 6), batch 20604, train_loss = 1.968, time/batch = 0.102\n",
      "Sequence 203112/3263200 (epoch 6), batch 20704, train_loss = 1.100, time/batch = 0.100\n",
      "Sequence 204092/3263200 (epoch 6), batch 20804, train_loss = 1.516, time/batch = 0.100\n",
      "Sequence 205082/3263200 (epoch 6), batch 20904, train_loss = 1.406, time/batch = 0.098\n",
      "Sequence 206042/3263200 (epoch 6), batch 21004, train_loss = 2.759, time/batch = 0.100\n",
      "Sequence 207022/3263200 (epoch 6), batch 21104, train_loss = 1.519, time/batch = 0.103\n",
      "Sequence 208012/3263200 (epoch 6), batch 21204, train_loss = 2.457, time/batch = 0.101\n",
      "Sequence 208962/3263200 (epoch 6), batch 21304, train_loss = 1.494, time/batch = 0.102\n",
      "Sequence 209962/3263200 (epoch 6), batch 21404, train_loss = 1.554, time/batch = 0.104\n",
      "Sequence 210962/3263200 (epoch 6), batch 21504, train_loss = 2.380, time/batch = 0.100\n",
      "Sequence 211942/3263200 (epoch 6), batch 21604, train_loss = 1.927, time/batch = 0.102\n",
      "Sequence 212942/3263200 (epoch 6), batch 21704, train_loss = 1.471, time/batch = 0.103\n",
      "Sequence 213942/3263200 (epoch 6), batch 21804, train_loss = 2.039, time/batch = 0.102\n",
      "Sequence 214882/3263200 (epoch 6), batch 21904, train_loss = 1.212, time/batch = 0.104\n",
      "Sequence 215862/3263200 (epoch 6), batch 22004, train_loss = 2.154, time/batch = 0.101\n",
      "Sequence 216842/3263200 (epoch 6), batch 22104, train_loss = 1.521, time/batch = 0.100\n",
      "Sequence 217812/3263200 (epoch 6), batch 22204, train_loss = 1.909, time/batch = 0.101\n",
      "Sequence 218802/3263200 (epoch 6), batch 22304, train_loss = 2.979, time/batch = 0.101\n",
      "Sequence 219802/3263200 (epoch 6), batch 22405, train_loss = 1.572, time/batch = 0.102\n",
      "Sequence 220782/3263200 (epoch 6), batch 22505, train_loss = 1.173, time/batch = 0.099\n",
      "Sequence 221752/3263200 (epoch 6), batch 22605, train_loss = 1.441, time/batch = 0.101\n",
      "Sequence 222722/3263200 (epoch 6), batch 22705, train_loss = 2.084, time/batch = 0.103\n",
      "Sequence 223722/3263200 (epoch 6), batch 22805, train_loss = 1.879, time/batch = 0.102\n",
      "Sequence 224702/3263200 (epoch 6), batch 22905, train_loss = 1.224, time/batch = 0.100\n",
      "Sequence 225682/3263200 (epoch 6), batch 23005, train_loss = 2.148, time/batch = 0.101\n",
      "Sequence 226672/3263200 (epoch 6), batch 23106, train_loss = 0.857, time/batch = 0.102\n",
      "Sequence 227652/3263200 (epoch 6), batch 23206, train_loss = 1.851, time/batch = 0.102\n",
      "Epoch 6 completed, average train loss 1.725673, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 228624/3263200 (epoch 7), batch 23306, train_loss = 1.637, time/batch = 0.101\n",
      "Sequence 229594/3263200 (epoch 7), batch 23406, train_loss = 1.424, time/batch = 0.103\n",
      "Sequence 230574/3263200 (epoch 7), batch 23506, train_loss = 1.697, time/batch = 0.102\n",
      "Sequence 231574/3263200 (epoch 7), batch 23606, train_loss = 1.275, time/batch = 0.101\n",
      "Sequence 232564/3263200 (epoch 7), batch 23706, train_loss = 0.970, time/batch = 0.099\n",
      "Sequence 233534/3263200 (epoch 7), batch 23806, train_loss = 0.977, time/batch = 0.101\n",
      "Sequence 234514/3263200 (epoch 7), batch 23906, train_loss = 2.064, time/batch = 0.102\n",
      "Sequence 235504/3263200 (epoch 7), batch 24006, train_loss = 1.646, time/batch = 0.100\n",
      "Sequence 236474/3263200 (epoch 7), batch 24106, train_loss = 1.205, time/batch = 0.101\n",
      "Sequence 237474/3263200 (epoch 7), batch 24207, train_loss = 1.028, time/batch = 0.102\n",
      "Sequence 238434/3263200 (epoch 7), batch 24307, train_loss = 1.682, time/batch = 0.102\n",
      "Sequence 239424/3263200 (epoch 7), batch 24407, train_loss = 1.849, time/batch = 0.101\n",
      "Sequence 240404/3263200 (epoch 7), batch 24507, train_loss = 2.329, time/batch = 0.103\n",
      "Sequence 241404/3263200 (epoch 7), batch 24607, train_loss = 1.334, time/batch = 0.103\n",
      "Sequence 242404/3263200 (epoch 7), batch 24707, train_loss = 1.931, time/batch = 0.101\n",
      "Sequence 243374/3263200 (epoch 7), batch 24807, train_loss = 1.934, time/batch = 0.101\n",
      "Sequence 244354/3263200 (epoch 7), batch 24907, train_loss = 1.105, time/batch = 0.099\n",
      "Sequence 245334/3263200 (epoch 7), batch 25007, train_loss = 1.712, time/batch = 0.099\n",
      "Sequence 246294/3263200 (epoch 7), batch 25107, train_loss = 1.144, time/batch = 0.102\n",
      "Sequence 247274/3263200 (epoch 7), batch 25207, train_loss = 3.130, time/batch = 0.101\n",
      "Sequence 248254/3263200 (epoch 7), batch 25307, train_loss = 1.758, time/batch = 0.100\n",
      "Sequence 249244/3263200 (epoch 7), batch 25407, train_loss = 2.332, time/batch = 0.103\n",
      "Sequence 250234/3263200 (epoch 7), batch 25507, train_loss = 1.662, time/batch = 0.104\n",
      "Sequence 251214/3263200 (epoch 7), batch 25607, train_loss = 1.340, time/batch = 0.102\n",
      "Sequence 252204/3263200 (epoch 7), batch 25707, train_loss = 1.619, time/batch = 0.099\n",
      "Sequence 253174/3263200 (epoch 7), batch 25808, train_loss = 0.968, time/batch = 0.101\n",
      "Sequence 254144/3263200 (epoch 7), batch 25908, train_loss = 1.751, time/batch = 0.099\n",
      "Sequence 255124/3263200 (epoch 7), batch 26008, train_loss = 3.158, time/batch = 0.101\n",
      "Sequence 256124/3263200 (epoch 7), batch 26108, train_loss = 1.629, time/batch = 0.102\n",
      "Sequence 257114/3263200 (epoch 7), batch 26208, train_loss = 1.654, time/batch = 0.101\n",
      "Sequence 258094/3263200 (epoch 7), batch 26308, train_loss = 1.352, time/batch = 0.099\n",
      "Sequence 259054/3263200 (epoch 7), batch 26408, train_loss = 2.319, time/batch = 0.101\n",
      "Sequence 260044/3263200 (epoch 7), batch 26508, train_loss = 2.437, time/batch = 0.101\n",
      "Sequence 261014/3263200 (epoch 7), batch 26608, train_loss = 2.250, time/batch = 0.100\n",
      "Epoch 7 completed, average train loss 1.677405, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 262006/3263200 (epoch 8), batch 26708, train_loss = 1.702, time/batch = 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 262986/3263200 (epoch 8), batch 26808, train_loss = 1.144, time/batch = 0.099\n",
      "Sequence 263966/3263200 (epoch 8), batch 26908, train_loss = 1.685, time/batch = 0.100\n",
      "Sequence 264926/3263200 (epoch 8), batch 27008, train_loss = 1.323, time/batch = 0.101\n",
      "Sequence 265906/3263200 (epoch 8), batch 27108, train_loss = 1.935, time/batch = 0.101\n",
      "Sequence 266896/3263200 (epoch 8), batch 27208, train_loss = 1.053, time/batch = 0.100\n",
      "Sequence 267876/3263200 (epoch 8), batch 27308, train_loss = 1.466, time/batch = 0.100\n",
      "Sequence 268866/3263200 (epoch 8), batch 27408, train_loss = 0.934, time/batch = 0.102\n",
      "Sequence 269826/3263200 (epoch 8), batch 27508, train_loss = 1.837, time/batch = 0.102\n",
      "Sequence 270826/3263200 (epoch 8), batch 27608, train_loss = 3.521, time/batch = 0.100\n",
      "Sequence 271816/3263200 (epoch 8), batch 27708, train_loss = 1.414, time/batch = 0.100\n",
      "Sequence 272796/3263200 (epoch 8), batch 27808, train_loss = 1.958, time/batch = 0.099\n",
      "Sequence 273786/3263200 (epoch 8), batch 27908, train_loss = 2.100, time/batch = 0.101\n",
      "Sequence 274746/3263200 (epoch 8), batch 28008, train_loss = 0.507, time/batch = 0.100\n",
      "Sequence 275736/3263200 (epoch 8), batch 28108, train_loss = 1.375, time/batch = 0.102\n",
      "Sequence 276726/3263200 (epoch 8), batch 28208, train_loss = 2.810, time/batch = 0.102\n",
      "Sequence 277706/3263200 (epoch 8), batch 28308, train_loss = 1.568, time/batch = 0.103\n",
      "Sequence 278696/3263200 (epoch 8), batch 28408, train_loss = 1.432, time/batch = 0.100\n",
      "Sequence 279696/3263200 (epoch 8), batch 28508, train_loss = 1.967, time/batch = 0.102\n",
      "Sequence 280696/3263200 (epoch 8), batch 28608, train_loss = 1.662, time/batch = 0.100\n",
      "Sequence 281656/3263200 (epoch 8), batch 28708, train_loss = 1.172, time/batch = 0.100\n",
      "Sequence 282636/3263200 (epoch 8), batch 28808, train_loss = 1.230, time/batch = 0.099\n",
      "Sequence 283606/3263200 (epoch 8), batch 28908, train_loss = 1.529, time/batch = 0.101\n",
      "Sequence 284596/3263200 (epoch 8), batch 29008, train_loss = 1.442, time/batch = 0.102\n",
      "Sequence 285586/3263200 (epoch 8), batch 29108, train_loss = 1.263, time/batch = 0.101\n",
      "Sequence 286566/3263200 (epoch 8), batch 29208, train_loss = 1.329, time/batch = 0.101\n",
      "Sequence 287546/3263200 (epoch 8), batch 29308, train_loss = 1.003, time/batch = 0.103\n",
      "Sequence 288516/3263200 (epoch 8), batch 29408, train_loss = 1.283, time/batch = 0.101\n",
      "Sequence 289496/3263200 (epoch 8), batch 29508, train_loss = 1.027, time/batch = 0.101\n",
      "Sequence 290486/3263200 (epoch 8), batch 29608, train_loss = 1.496, time/batch = 0.099\n",
      "Sequence 291466/3263200 (epoch 8), batch 29708, train_loss = 1.253, time/batch = 0.101\n",
      "Sequence 292426/3263200 (epoch 8), batch 29808, train_loss = 1.730, time/batch = 0.099\n",
      "Sequence 293376/3263200 (epoch 8), batch 29908, train_loss = 1.515, time/batch = 0.101\n",
      "Epoch 8 completed, average train loss 1.639072, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 294368/3263200 (epoch 9), batch 30008, train_loss = 1.723, time/batch = 0.103\n",
      "Sequence 295318/3263200 (epoch 9), batch 30108, train_loss = 1.664, time/batch = 0.100\n",
      "Sequence 296308/3263200 (epoch 9), batch 30208, train_loss = 1.000, time/batch = 0.099\n",
      "Sequence 297268/3263200 (epoch 9), batch 30308, train_loss = 1.109, time/batch = 0.101\n",
      "Sequence 298268/3263200 (epoch 9), batch 30408, train_loss = 1.681, time/batch = 0.101\n",
      "Sequence 299258/3263200 (epoch 9), batch 30508, train_loss = 1.266, time/batch = 0.105\n",
      "Sequence 300228/3263200 (epoch 9), batch 30608, train_loss = 1.633, time/batch = 0.104\n",
      "Sequence 301218/3263200 (epoch 9), batch 30708, train_loss = 2.005, time/batch = 0.101\n",
      "Sequence 302218/3263200 (epoch 9), batch 30808, train_loss = 1.273, time/batch = 0.106\n",
      "Sequence 303188/3263200 (epoch 9), batch 30908, train_loss = 2.537, time/batch = 0.105\n",
      "Sequence 304158/3263200 (epoch 9), batch 31008, train_loss = 1.646, time/batch = 0.105\n",
      "Sequence 305138/3263200 (epoch 9), batch 31108, train_loss = 1.288, time/batch = 0.110\n",
      "Sequence 306128/3263200 (epoch 9), batch 31208, train_loss = 0.977, time/batch = 0.110\n",
      "Sequence 307118/3263200 (epoch 9), batch 31308, train_loss = 1.589, time/batch = 0.107\n",
      "Sequence 308088/3263200 (epoch 9), batch 31408, train_loss = 1.233, time/batch = 0.109\n",
      "Sequence 309078/3263200 (epoch 9), batch 31508, train_loss = 1.168, time/batch = 0.103\n",
      "Sequence 310068/3263200 (epoch 9), batch 31608, train_loss = 2.028, time/batch = 0.099\n",
      "Sequence 311038/3263200 (epoch 9), batch 31708, train_loss = 1.725, time/batch = 0.098\n",
      "Sequence 312038/3263200 (epoch 9), batch 31808, train_loss = 1.343, time/batch = 0.102\n",
      "Sequence 313008/3263200 (epoch 9), batch 31908, train_loss = 0.976, time/batch = 0.101\n",
      "Sequence 313978/3263200 (epoch 9), batch 32008, train_loss = 1.983, time/batch = 0.101\n",
      "Sequence 314958/3263200 (epoch 9), batch 32108, train_loss = 1.911, time/batch = 0.102\n",
      "Sequence 315948/3263200 (epoch 9), batch 32208, train_loss = 1.984, time/batch = 0.104\n",
      "Sequence 316918/3263200 (epoch 9), batch 32308, train_loss = 2.914, time/batch = 0.100\n",
      "Sequence 317888/3263200 (epoch 9), batch 32408, train_loss = 1.344, time/batch = 0.099\n",
      "Sequence 318868/3263200 (epoch 9), batch 32508, train_loss = 0.994, time/batch = 0.101\n",
      "Sequence 319858/3263200 (epoch 9), batch 32608, train_loss = 1.499, time/batch = 0.102\n",
      "Sequence 320848/3263200 (epoch 9), batch 32708, train_loss = 1.152, time/batch = 0.102\n",
      "Sequence 321818/3263200 (epoch 9), batch 32808, train_loss = 2.250, time/batch = 0.100\n",
      "Sequence 322818/3263200 (epoch 9), batch 32908, train_loss = 1.489, time/batch = 0.101\n",
      "Sequence 323818/3263200 (epoch 9), batch 33008, train_loss = 2.155, time/batch = 0.102\n",
      "Sequence 324798/3263200 (epoch 9), batch 33108, train_loss = 2.617, time/batch = 0.099\n",
      "Sequence 325778/3263200 (epoch 9), batch 33208, train_loss = 1.613, time/batch = 0.100\n",
      "Epoch 9 completed, average train loss 1.608922, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 326720/3263200 (epoch 10), batch 33308, train_loss = 1.271, time/batch = 0.100\n",
      "Sequence 327700/3263200 (epoch 10), batch 33408, train_loss = 1.175, time/batch = 0.099\n",
      "Sequence 328670/3263200 (epoch 10), batch 33508, train_loss = 1.556, time/batch = 0.101\n",
      "Sequence 329660/3263200 (epoch 10), batch 33608, train_loss = 1.644, time/batch = 0.101\n",
      "Sequence 330640/3263200 (epoch 10), batch 33708, train_loss = 1.221, time/batch = 0.100\n",
      "Sequence 331600/3263200 (epoch 10), batch 33808, train_loss = 2.060, time/batch = 0.099\n",
      "Sequence 332580/3263200 (epoch 10), batch 33909, train_loss = 3.574, time/batch = 0.099\n",
      "Sequence 333570/3263200 (epoch 10), batch 34009, train_loss = 1.471, time/batch = 0.100\n",
      "Sequence 334550/3263200 (epoch 10), batch 34109, train_loss = 1.951, time/batch = 0.100\n",
      "Sequence 335530/3263200 (epoch 10), batch 34209, train_loss = 1.793, time/batch = 0.099\n",
      "Sequence 336500/3263200 (epoch 10), batch 34309, train_loss = 0.940, time/batch = 0.100\n",
      "Sequence 337500/3263200 (epoch 10), batch 34409, train_loss = 1.894, time/batch = 0.101\n",
      "Sequence 338500/3263200 (epoch 10), batch 34509, train_loss = 1.280, time/batch = 0.100\n",
      "Sequence 339500/3263200 (epoch 10), batch 34609, train_loss = 2.555, time/batch = 0.098\n",
      "Sequence 340460/3263200 (epoch 10), batch 34709, train_loss = 1.617, time/batch = 0.099\n",
      "Sequence 341450/3263200 (epoch 10), batch 34809, train_loss = 1.742, time/batch = 0.100\n",
      "Sequence 342440/3263200 (epoch 10), batch 34909, train_loss = 1.209, time/batch = 0.100\n",
      "Sequence 343410/3263200 (epoch 10), batch 35009, train_loss = 1.131, time/batch = 0.100\n",
      "Sequence 344400/3263200 (epoch 10), batch 35109, train_loss = 1.735, time/batch = 0.098\n",
      "Sequence 345390/3263200 (epoch 10), batch 35209, train_loss = 1.016, time/batch = 0.099\n",
      "Sequence 346380/3263200 (epoch 10), batch 35309, train_loss = 1.775, time/batch = 0.100\n",
      "Sequence 347370/3263200 (epoch 10), batch 35409, train_loss = 2.098, time/batch = 0.099\n",
      "Sequence 348360/3263200 (epoch 10), batch 35509, train_loss = 1.326, time/batch = 0.100\n",
      "Sequence 349350/3263200 (epoch 10), batch 35609, train_loss = 1.963, time/batch = 0.102\n",
      "Sequence 350330/3263200 (epoch 10), batch 35709, train_loss = 1.556, time/batch = 0.100\n",
      "Sequence 351290/3263200 (epoch 10), batch 35809, train_loss = 1.764, time/batch = 0.099\n",
      "Sequence 352260/3263200 (epoch 10), batch 35909, train_loss = 1.685, time/batch = 0.101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 353230/3263200 (epoch 10), batch 36009, train_loss = 1.838, time/batch = 0.101\n",
      "Sequence 354220/3263200 (epoch 10), batch 36109, train_loss = 0.953, time/batch = 0.098\n",
      "Sequence 355180/3263200 (epoch 10), batch 36209, train_loss = 1.720, time/batch = 0.099\n",
      "Sequence 356160/3263200 (epoch 10), batch 36309, train_loss = 1.334, time/batch = 0.099\n",
      "Sequence 357140/3263200 (epoch 10), batch 36409, train_loss = 1.390, time/batch = 0.099\n",
      "Sequence 358140/3263200 (epoch 10), batch 36509, train_loss = 1.820, time/batch = 0.099\n",
      "Epoch 10 completed, average train loss 1.583449, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 359132/3263200 (epoch 11), batch 36609, train_loss = 1.982, time/batch = 0.097\n",
      "Sequence 360122/3263200 (epoch 11), batch 36709, train_loss = 1.377, time/batch = 0.097\n",
      "Sequence 361082/3263200 (epoch 11), batch 36809, train_loss = 1.333, time/batch = 0.101\n",
      "Sequence 362072/3263200 (epoch 11), batch 36909, train_loss = 1.897, time/batch = 0.099\n",
      "Sequence 363062/3263200 (epoch 11), batch 37009, train_loss = 1.704, time/batch = 0.099\n",
      "Sequence 364052/3263200 (epoch 11), batch 37109, train_loss = 1.121, time/batch = 0.100\n",
      "Sequence 365042/3263200 (epoch 11), batch 37209, train_loss = 1.536, time/batch = 0.099\n",
      "Sequence 366012/3263200 (epoch 11), batch 37309, train_loss = 1.787, time/batch = 0.097\n",
      "Sequence 367012/3263200 (epoch 11), batch 37409, train_loss = 1.290, time/batch = 0.098\n",
      "Sequence 368002/3263200 (epoch 11), batch 37509, train_loss = 0.985, time/batch = 0.098\n",
      "Sequence 368982/3263200 (epoch 11), batch 37609, train_loss = 2.052, time/batch = 0.097\n",
      "Sequence 369962/3263200 (epoch 11), batch 37709, train_loss = 1.836, time/batch = 0.098\n",
      "Sequence 370902/3263200 (epoch 11), batch 37809, train_loss = 1.438, time/batch = 0.099\n",
      "Sequence 371892/3263200 (epoch 11), batch 37909, train_loss = 0.542, time/batch = 0.097\n",
      "Sequence 372872/3263200 (epoch 11), batch 38009, train_loss = 1.404, time/batch = 0.099\n",
      "Sequence 373862/3263200 (epoch 11), batch 38109, train_loss = 2.136, time/batch = 0.100\n",
      "Sequence 374852/3263200 (epoch 11), batch 38209, train_loss = 1.782, time/batch = 0.100\n",
      "Sequence 375822/3263200 (epoch 11), batch 38309, train_loss = 1.396, time/batch = 0.097\n",
      "Sequence 376792/3263200 (epoch 11), batch 38409, train_loss = 2.003, time/batch = 0.099\n",
      "Sequence 377772/3263200 (epoch 11), batch 38509, train_loss = 1.716, time/batch = 0.097\n",
      "Sequence 378752/3263200 (epoch 11), batch 38609, train_loss = 1.222, time/batch = 0.097\n",
      "Sequence 379732/3263200 (epoch 11), batch 38709, train_loss = 1.335, time/batch = 0.098\n",
      "Sequence 380722/3263200 (epoch 11), batch 38809, train_loss = 1.273, time/batch = 0.097\n",
      "Sequence 381712/3263200 (epoch 11), batch 38909, train_loss = 1.229, time/batch = 0.100\n",
      "Sequence 382692/3263200 (epoch 11), batch 39009, train_loss = 1.217, time/batch = 0.100\n",
      "Sequence 383632/3263200 (epoch 11), batch 39109, train_loss = 1.616, time/batch = 0.098\n",
      "Sequence 384622/3263200 (epoch 11), batch 39209, train_loss = 1.848, time/batch = 0.097\n",
      "Sequence 385622/3263200 (epoch 11), batch 39309, train_loss = 2.103, time/batch = 0.099\n",
      "Sequence 386602/3263200 (epoch 11), batch 39409, train_loss = 2.764, time/batch = 0.098\n",
      "Sequence 387582/3263200 (epoch 11), batch 39509, train_loss = 2.813, time/batch = 0.098\n",
      "Sequence 388572/3263200 (epoch 11), batch 39609, train_loss = 2.374, time/batch = 0.097\n",
      "Sequence 389562/3263200 (epoch 11), batch 39709, train_loss = 1.271, time/batch = 0.097\n",
      "Sequence 390542/3263200 (epoch 11), batch 39809, train_loss = 1.460, time/batch = 0.101\n",
      "Sequence 391502/3263200 (epoch 11), batch 39909, train_loss = 1.944, time/batch = 0.098\n",
      "Epoch 11 completed, average train loss 1.558963, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 392484/3263200 (epoch 12), batch 40009, train_loss = 1.208, time/batch = 0.103\n",
      "Sequence 393464/3263200 (epoch 12), batch 40109, train_loss = 1.887, time/batch = 0.100\n",
      "Sequence 394434/3263200 (epoch 12), batch 40209, train_loss = 1.089, time/batch = 0.099\n",
      "Sequence 395434/3263200 (epoch 12), batch 40309, train_loss = 2.703, time/batch = 0.100\n",
      "Sequence 396414/3263200 (epoch 12), batch 40409, train_loss = 1.348, time/batch = 0.100\n",
      "Sequence 397404/3263200 (epoch 12), batch 40509, train_loss = 1.362, time/batch = 0.099\n",
      "Sequence 398384/3263200 (epoch 12), batch 40609, train_loss = 1.019, time/batch = 0.098\n",
      "Sequence 399374/3263200 (epoch 12), batch 40709, train_loss = 2.610, time/batch = 0.100\n",
      "Sequence 400334/3263200 (epoch 12), batch 40809, train_loss = 1.649, time/batch = 0.099\n",
      "Sequence 401284/3263200 (epoch 12), batch 40909, train_loss = 1.396, time/batch = 0.101\n",
      "Sequence 402274/3263200 (epoch 12), batch 41009, train_loss = 1.389, time/batch = 0.101\n",
      "Sequence 403274/3263200 (epoch 12), batch 41109, train_loss = 0.786, time/batch = 0.099\n",
      "Sequence 404254/3263200 (epoch 12), batch 41209, train_loss = 1.654, time/batch = 0.098\n",
      "Sequence 405234/3263200 (epoch 12), batch 41309, train_loss = 1.100, time/batch = 0.097\n",
      "Sequence 406214/3263200 (epoch 12), batch 41409, train_loss = 2.632, time/batch = 0.099\n",
      "Sequence 407184/3263200 (epoch 12), batch 41509, train_loss = 1.229, time/batch = 0.101\n",
      "Sequence 408154/3263200 (epoch 12), batch 41609, train_loss = 2.403, time/batch = 0.097\n",
      "Sequence 409144/3263200 (epoch 12), batch 41709, train_loss = 1.313, time/batch = 0.100\n",
      "Sequence 410134/3263200 (epoch 12), batch 41809, train_loss = 1.020, time/batch = 0.102\n",
      "Sequence 411094/3263200 (epoch 12), batch 41909, train_loss = 0.908, time/batch = 0.098\n",
      "Sequence 412084/3263200 (epoch 12), batch 42009, train_loss = 1.957, time/batch = 0.101\n",
      "Sequence 413064/3263200 (epoch 12), batch 42109, train_loss = 1.434, time/batch = 0.098\n",
      "Sequence 414044/3263200 (epoch 12), batch 42209, train_loss = 1.425, time/batch = 0.098\n",
      "Sequence 415024/3263200 (epoch 12), batch 42309, train_loss = 0.897, time/batch = 0.098\n",
      "Sequence 416004/3263200 (epoch 12), batch 42409, train_loss = 0.943, time/batch = 0.098\n",
      "Sequence 416994/3263200 (epoch 12), batch 42509, train_loss = 1.674, time/batch = 0.095\n",
      "Sequence 417974/3263200 (epoch 12), batch 42609, train_loss = 1.172, time/batch = 0.095\n",
      "Sequence 418954/3263200 (epoch 12), batch 42709, train_loss = 0.402, time/batch = 0.097\n",
      "Sequence 419934/3263200 (epoch 12), batch 42809, train_loss = 1.471, time/batch = 0.102\n",
      "Sequence 420924/3263200 (epoch 12), batch 42909, train_loss = 1.168, time/batch = 0.098\n",
      "Sequence 421904/3263200 (epoch 12), batch 43009, train_loss = 1.176, time/batch = 0.097\n",
      "Sequence 422894/3263200 (epoch 12), batch 43109, train_loss = 1.182, time/batch = 0.098\n",
      "Sequence 423874/3263200 (epoch 12), batch 43209, train_loss = 1.538, time/batch = 0.097\n",
      "Epoch 12 completed, average train loss 1.540761, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 424856/3263200 (epoch 13), batch 43309, train_loss = 1.288, time/batch = 0.099\n",
      "Sequence 425826/3263200 (epoch 13), batch 43409, train_loss = 0.882, time/batch = 0.096\n",
      "Sequence 426826/3263200 (epoch 13), batch 43509, train_loss = 1.535, time/batch = 0.096\n",
      "Sequence 427796/3263200 (epoch 13), batch 43609, train_loss = 1.335, time/batch = 0.097\n",
      "Sequence 428776/3263200 (epoch 13), batch 43709, train_loss = 1.506, time/batch = 0.096\n",
      "Sequence 429756/3263200 (epoch 13), batch 43809, train_loss = 1.236, time/batch = 0.096\n",
      "Sequence 430736/3263200 (epoch 13), batch 43909, train_loss = 1.187, time/batch = 0.097\n",
      "Sequence 431716/3263200 (epoch 13), batch 44009, train_loss = 1.845, time/batch = 0.099\n",
      "Sequence 432676/3263200 (epoch 13), batch 44109, train_loss = 1.940, time/batch = 0.099\n",
      "Sequence 433656/3263200 (epoch 13), batch 44209, train_loss = 1.078, time/batch = 0.101\n",
      "Sequence 434656/3263200 (epoch 13), batch 44309, train_loss = 2.833, time/batch = 0.101\n",
      "Sequence 435656/3263200 (epoch 13), batch 44409, train_loss = 1.511, time/batch = 0.097\n",
      "Sequence 436626/3263200 (epoch 13), batch 44509, train_loss = 1.162, time/batch = 0.096\n",
      "Sequence 437616/3263200 (epoch 13), batch 44609, train_loss = 1.047, time/batch = 0.097\n",
      "Sequence 438616/3263200 (epoch 13), batch 44709, train_loss = 1.329, time/batch = 0.096\n",
      "Sequence 439596/3263200 (epoch 13), batch 44809, train_loss = 1.956, time/batch = 0.098\n",
      "Sequence 440566/3263200 (epoch 13), batch 44909, train_loss = 1.321, time/batch = 0.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 441566/3263200 (epoch 13), batch 45009, train_loss = 0.831, time/batch = 0.096\n",
      "Sequence 442566/3263200 (epoch 13), batch 45109, train_loss = 2.185, time/batch = 0.095\n",
      "Sequence 443546/3263200 (epoch 13), batch 45209, train_loss = 1.407, time/batch = 0.097\n",
      "Sequence 444536/3263200 (epoch 13), batch 45309, train_loss = 0.738, time/batch = 0.096\n",
      "Sequence 445516/3263200 (epoch 13), batch 45409, train_loss = 1.603, time/batch = 0.095\n",
      "Sequence 446496/3263200 (epoch 13), batch 45509, train_loss = 1.654, time/batch = 0.098\n",
      "Sequence 447456/3263200 (epoch 13), batch 45609, train_loss = 0.831, time/batch = 0.099\n",
      "Sequence 448426/3263200 (epoch 13), batch 45709, train_loss = 1.782, time/batch = 0.100\n",
      "Sequence 449406/3263200 (epoch 13), batch 45809, train_loss = 1.648, time/batch = 0.098\n",
      "Sequence 450406/3263200 (epoch 13), batch 45909, train_loss = 1.018, time/batch = 0.100\n",
      "Sequence 451396/3263200 (epoch 13), batch 46009, train_loss = 1.679, time/batch = 0.100\n",
      "Sequence 452366/3263200 (epoch 13), batch 46109, train_loss = 1.919, time/batch = 0.102\n",
      "Sequence 453336/3263200 (epoch 13), batch 46209, train_loss = 1.157, time/batch = 0.101\n",
      "Sequence 454306/3263200 (epoch 13), batch 46309, train_loss = 1.999, time/batch = 0.099\n",
      "Sequence 455266/3263200 (epoch 13), batch 46409, train_loss = 1.101, time/batch = 0.101\n",
      "Sequence 456266/3263200 (epoch 13), batch 46509, train_loss = 1.572, time/batch = 0.099\n",
      "Epoch 13 completed, average train loss 1.522180, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 457218/3263200 (epoch 14), batch 46609, train_loss = 1.384, time/batch = 0.100\n",
      "Sequence 458218/3263200 (epoch 14), batch 46709, train_loss = 1.452, time/batch = 0.097\n",
      "Sequence 459188/3263200 (epoch 14), batch 46809, train_loss = 1.197, time/batch = 0.097\n",
      "Sequence 460168/3263200 (epoch 14), batch 46909, train_loss = 1.296, time/batch = 0.099\n",
      "Sequence 461168/3263200 (epoch 14), batch 47009, train_loss = 1.998, time/batch = 0.098\n",
      "Sequence 462138/3263200 (epoch 14), batch 47110, train_loss = 0.685, time/batch = 0.100\n",
      "Sequence 463118/3263200 (epoch 14), batch 47210, train_loss = 1.274, time/batch = 0.101\n",
      "Sequence 464088/3263200 (epoch 14), batch 47310, train_loss = 1.213, time/batch = 0.099\n",
      "Sequence 465058/3263200 (epoch 14), batch 47410, train_loss = 1.190, time/batch = 0.101\n",
      "Sequence 466048/3263200 (epoch 14), batch 47510, train_loss = 1.866, time/batch = 0.102\n",
      "Sequence 466998/3263200 (epoch 14), batch 47610, train_loss = 2.221, time/batch = 0.100\n",
      "Sequence 467998/3263200 (epoch 14), batch 47710, train_loss = 1.809, time/batch = 0.097\n",
      "Sequence 468978/3263200 (epoch 14), batch 47810, train_loss = 1.100, time/batch = 0.096\n",
      "Sequence 469978/3263200 (epoch 14), batch 47910, train_loss = 2.083, time/batch = 0.097\n",
      "Sequence 470958/3263200 (epoch 14), batch 48010, train_loss = 1.245, time/batch = 0.098\n",
      "Sequence 471908/3263200 (epoch 14), batch 48110, train_loss = 1.203, time/batch = 0.097\n",
      "Sequence 472878/3263200 (epoch 14), batch 48210, train_loss = 1.233, time/batch = 0.097\n",
      "Sequence 473858/3263200 (epoch 14), batch 48310, train_loss = 1.511, time/batch = 0.099\n",
      "Sequence 474858/3263200 (epoch 14), batch 48410, train_loss = 0.755, time/batch = 0.097\n",
      "Sequence 475818/3263200 (epoch 14), batch 48510, train_loss = 1.136, time/batch = 0.098\n",
      "Sequence 476808/3263200 (epoch 14), batch 48610, train_loss = 1.332, time/batch = 0.099\n",
      "Sequence 477788/3263200 (epoch 14), batch 48710, train_loss = 1.256, time/batch = 0.097\n",
      "Sequence 478758/3263200 (epoch 14), batch 48810, train_loss = 0.856, time/batch = 0.098\n",
      "Sequence 479758/3263200 (epoch 14), batch 48910, train_loss = 1.340, time/batch = 0.098\n",
      "Sequence 480758/3263200 (epoch 14), batch 49010, train_loss = 1.352, time/batch = 0.098\n",
      "Sequence 481748/3263200 (epoch 14), batch 49110, train_loss = 1.660, time/batch = 0.099\n",
      "Sequence 482738/3263200 (epoch 14), batch 49210, train_loss = 1.144, time/batch = 0.099\n",
      "Sequence 483728/3263200 (epoch 14), batch 49310, train_loss = 2.948, time/batch = 0.095\n",
      "Sequence 484708/3263200 (epoch 14), batch 49410, train_loss = 2.657, time/batch = 0.097\n",
      "Sequence 485698/3263200 (epoch 14), batch 49510, train_loss = 0.948, time/batch = 0.097\n",
      "Sequence 486668/3263200 (epoch 14), batch 49610, train_loss = 1.461, time/batch = 0.100\n",
      "Sequence 487638/3263200 (epoch 14), batch 49710, train_loss = 1.768, time/batch = 0.095\n",
      "Sequence 488618/3263200 (epoch 14), batch 49810, train_loss = 1.684, time/batch = 0.095\n",
      "Epoch 14 completed, average train loss 1.505495, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 489620/3263200 (epoch 15), batch 49910, train_loss = 1.177, time/batch = 0.097\n",
      "Sequence 490590/3263200 (epoch 15), batch 50010, train_loss = 1.088, time/batch = 0.096\n",
      "Sequence 491580/3263200 (epoch 15), batch 50110, train_loss = 1.490, time/batch = 0.097\n",
      "Sequence 492560/3263200 (epoch 15), batch 50210, train_loss = 0.885, time/batch = 0.097\n",
      "Sequence 493520/3263200 (epoch 15), batch 50310, train_loss = 1.601, time/batch = 0.101\n",
      "Sequence 494510/3263200 (epoch 15), batch 50410, train_loss = 1.412, time/batch = 0.098\n",
      "Sequence 495460/3263200 (epoch 15), batch 50510, train_loss = 1.966, time/batch = 0.100\n",
      "Sequence 496440/3263200 (epoch 15), batch 50610, train_loss = 1.526, time/batch = 0.099\n",
      "Sequence 497430/3263200 (epoch 15), batch 50710, train_loss = 1.814, time/batch = 0.100\n",
      "Sequence 498410/3263200 (epoch 15), batch 50810, train_loss = 1.556, time/batch = 0.098\n",
      "Sequence 499400/3263200 (epoch 15), batch 50910, train_loss = 1.734, time/batch = 0.100\n",
      "Sequence 500360/3263200 (epoch 15), batch 51010, train_loss = 1.930, time/batch = 0.100\n",
      "Sequence 501340/3263200 (epoch 15), batch 51110, train_loss = 1.080, time/batch = 0.099\n",
      "Sequence 502330/3263200 (epoch 15), batch 51210, train_loss = 2.167, time/batch = 0.099\n",
      "Sequence 503310/3263200 (epoch 15), batch 51310, train_loss = 1.058, time/batch = 0.098\n",
      "Sequence 504290/3263200 (epoch 15), batch 51410, train_loss = 1.420, time/batch = 0.098\n",
      "Sequence 505290/3263200 (epoch 15), batch 51510, train_loss = 1.297, time/batch = 0.098\n",
      "Sequence 506260/3263200 (epoch 15), batch 51610, train_loss = 2.232, time/batch = 0.100\n",
      "Sequence 507260/3263200 (epoch 15), batch 51710, train_loss = 1.287, time/batch = 0.099\n",
      "Sequence 508250/3263200 (epoch 15), batch 51810, train_loss = 1.709, time/batch = 0.104\n",
      "Sequence 509240/3263200 (epoch 15), batch 51910, train_loss = 1.244, time/batch = 0.100\n",
      "Sequence 510220/3263200 (epoch 15), batch 52010, train_loss = 1.170, time/batch = 0.103\n",
      "Sequence 511220/3263200 (epoch 15), batch 52110, train_loss = 1.747, time/batch = 0.099\n",
      "Sequence 512210/3263200 (epoch 15), batch 52210, train_loss = 1.498, time/batch = 0.099\n",
      "Sequence 513190/3263200 (epoch 15), batch 52310, train_loss = 0.912, time/batch = 0.101\n",
      "Sequence 514160/3263200 (epoch 15), batch 52410, train_loss = 1.678, time/batch = 0.097\n",
      "Sequence 515120/3263200 (epoch 15), batch 52510, train_loss = 0.854, time/batch = 0.096\n",
      "Sequence 516100/3263200 (epoch 15), batch 52610, train_loss = 1.086, time/batch = 0.099\n",
      "Sequence 517090/3263200 (epoch 15), batch 52710, train_loss = 1.707, time/batch = 0.097\n",
      "Sequence 518030/3263200 (epoch 15), batch 52810, train_loss = 1.095, time/batch = 0.101\n",
      "Sequence 519020/3263200 (epoch 15), batch 52910, train_loss = 1.376, time/batch = 0.100\n",
      "Sequence 520010/3263200 (epoch 15), batch 53010, train_loss = 1.106, time/batch = 0.104\n",
      "Sequence 520990/3263200 (epoch 15), batch 53110, train_loss = 1.333, time/batch = 0.100\n",
      "Sequence 521980/3263200 (epoch 15), batch 53210, train_loss = 1.494, time/batch = 0.099\n",
      "Epoch 15 completed, average train loss 1.488474, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 522962/3263200 (epoch 16), batch 53310, train_loss = 1.365, time/batch = 0.099\n",
      "Sequence 523932/3263200 (epoch 16), batch 53411, train_loss = 0.867, time/batch = 0.100\n",
      "Sequence 524902/3263200 (epoch 16), batch 53511, train_loss = 0.991, time/batch = 0.098\n",
      "Sequence 525892/3263200 (epoch 16), batch 53611, train_loss = 1.518, time/batch = 0.098\n",
      "Sequence 526882/3263200 (epoch 16), batch 53711, train_loss = 0.891, time/batch = 0.098\n",
      "Sequence 527862/3263200 (epoch 16), batch 53811, train_loss = 1.689, time/batch = 0.100\n",
      "Sequence 528842/3263200 (epoch 16), batch 53911, train_loss = 1.694, time/batch = 0.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 529802/3263200 (epoch 16), batch 54011, train_loss = 1.089, time/batch = 0.096\n",
      "Sequence 530772/3263200 (epoch 16), batch 54111, train_loss = 1.802, time/batch = 0.098\n",
      "Sequence 531762/3263200 (epoch 16), batch 54211, train_loss = 1.693, time/batch = 0.098\n",
      "Sequence 532752/3263200 (epoch 16), batch 54311, train_loss = 0.908, time/batch = 0.097\n",
      "Sequence 533742/3263200 (epoch 16), batch 54411, train_loss = 1.606, time/batch = 0.097\n",
      "Sequence 534722/3263200 (epoch 16), batch 54511, train_loss = 1.105, time/batch = 0.099\n",
      "Sequence 535712/3263200 (epoch 16), batch 54611, train_loss = 1.484, time/batch = 0.095\n",
      "Sequence 536702/3263200 (epoch 16), batch 54711, train_loss = 2.294, time/batch = 0.096\n",
      "Sequence 537702/3263200 (epoch 16), batch 54811, train_loss = 1.624, time/batch = 0.095\n",
      "Sequence 538702/3263200 (epoch 16), batch 54911, train_loss = 1.376, time/batch = 0.098\n",
      "Sequence 539702/3263200 (epoch 16), batch 55011, train_loss = 1.345, time/batch = 0.099\n",
      "Sequence 540672/3263200 (epoch 16), batch 55111, train_loss = 1.708, time/batch = 0.100\n",
      "Sequence 541652/3263200 (epoch 16), batch 55211, train_loss = 0.890, time/batch = 0.098\n",
      "Sequence 542612/3263200 (epoch 16), batch 55311, train_loss = 1.794, time/batch = 0.098\n",
      "Sequence 543612/3263200 (epoch 16), batch 55411, train_loss = 1.097, time/batch = 0.099\n",
      "Sequence 544612/3263200 (epoch 16), batch 55511, train_loss = 2.214, time/batch = 0.100\n",
      "Sequence 545592/3263200 (epoch 16), batch 55611, train_loss = 1.046, time/batch = 0.099\n",
      "Sequence 546562/3263200 (epoch 16), batch 55711, train_loss = 0.498, time/batch = 0.101\n",
      "Sequence 547512/3263200 (epoch 16), batch 55811, train_loss = 1.033, time/batch = 0.101\n",
      "Sequence 548492/3263200 (epoch 16), batch 55911, train_loss = 1.352, time/batch = 0.099\n",
      "Sequence 549472/3263200 (epoch 16), batch 56011, train_loss = 2.818, time/batch = 0.099\n",
      "Sequence 550472/3263200 (epoch 16), batch 56111, train_loss = 1.255, time/batch = 0.095\n",
      "Sequence 551442/3263200 (epoch 16), batch 56211, train_loss = 1.134, time/batch = 0.097\n",
      "Sequence 552422/3263200 (epoch 16), batch 56311, train_loss = 1.297, time/batch = 0.099\n",
      "Sequence 553412/3263200 (epoch 16), batch 56411, train_loss = 1.267, time/batch = 0.097\n",
      "Sequence 554392/3263200 (epoch 16), batch 56511, train_loss = 1.895, time/batch = 0.098\n",
      "Epoch 16 completed, average train loss 1.474872, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 555344/3263200 (epoch 17), batch 56611, train_loss = 1.784, time/batch = 0.100\n",
      "Sequence 556304/3263200 (epoch 17), batch 56711, train_loss = 1.208, time/batch = 0.099\n",
      "Sequence 557284/3263200 (epoch 17), batch 56812, train_loss = 0.666, time/batch = 0.100\n",
      "Sequence 558264/3263200 (epoch 17), batch 56912, train_loss = 1.377, time/batch = 0.098\n",
      "Sequence 559254/3263200 (epoch 17), batch 57012, train_loss = 1.986, time/batch = 0.098\n",
      "Sequence 560224/3263200 (epoch 17), batch 57112, train_loss = 2.209, time/batch = 0.101\n",
      "Sequence 561194/3263200 (epoch 17), batch 57212, train_loss = 1.524, time/batch = 0.101\n",
      "Sequence 562194/3263200 (epoch 17), batch 57312, train_loss = 1.080, time/batch = 0.099\n",
      "Sequence 563144/3263200 (epoch 17), batch 57412, train_loss = 0.785, time/batch = 0.098\n",
      "Sequence 564124/3263200 (epoch 17), batch 57512, train_loss = 0.908, time/batch = 0.101\n",
      "Sequence 565114/3263200 (epoch 17), batch 57612, train_loss = 1.644, time/batch = 0.102\n",
      "Sequence 566094/3263200 (epoch 17), batch 57712, train_loss = 1.192, time/batch = 0.098\n",
      "Sequence 567094/3263200 (epoch 17), batch 57812, train_loss = 1.385, time/batch = 0.098\n",
      "Sequence 568084/3263200 (epoch 17), batch 57912, train_loss = 1.579, time/batch = 0.099\n",
      "Sequence 569064/3263200 (epoch 17), batch 58012, train_loss = 1.827, time/batch = 0.098\n",
      "Sequence 570044/3263200 (epoch 17), batch 58112, train_loss = 1.090, time/batch = 0.096\n",
      "Sequence 571034/3263200 (epoch 17), batch 58212, train_loss = 1.640, time/batch = 0.096\n",
      "Sequence 572024/3263200 (epoch 17), batch 58312, train_loss = 1.155, time/batch = 0.097\n",
      "Sequence 572994/3263200 (epoch 17), batch 58412, train_loss = 2.016, time/batch = 0.099\n",
      "Sequence 573984/3263200 (epoch 17), batch 58512, train_loss = 1.401, time/batch = 0.101\n",
      "Sequence 574974/3263200 (epoch 17), batch 58612, train_loss = 1.410, time/batch = 0.096\n",
      "Sequence 575964/3263200 (epoch 17), batch 58712, train_loss = 0.956, time/batch = 0.096\n",
      "Sequence 576954/3263200 (epoch 17), batch 58812, train_loss = 1.336, time/batch = 0.099\n",
      "Sequence 577934/3263200 (epoch 17), batch 58912, train_loss = 1.249, time/batch = 0.099\n",
      "Sequence 578934/3263200 (epoch 17), batch 59012, train_loss = 1.168, time/batch = 0.094\n",
      "Sequence 579904/3263200 (epoch 17), batch 59112, train_loss = 1.699, time/batch = 0.099\n",
      "Sequence 580894/3263200 (epoch 17), batch 59212, train_loss = 1.225, time/batch = 0.099\n",
      "Sequence 581894/3263200 (epoch 17), batch 59312, train_loss = 1.625, time/batch = 0.100\n",
      "Sequence 582874/3263200 (epoch 17), batch 59412, train_loss = 0.993, time/batch = 0.100\n",
      "Sequence 583874/3263200 (epoch 17), batch 59512, train_loss = 0.977, time/batch = 0.101\n",
      "Sequence 584824/3263200 (epoch 17), batch 59612, train_loss = 1.764, time/batch = 0.100\n",
      "Sequence 585784/3263200 (epoch 17), batch 59712, train_loss = 1.276, time/batch = 0.098\n",
      "Sequence 586764/3263200 (epoch 17), batch 59812, train_loss = 1.169, time/batch = 0.102\n",
      "Epoch 17 completed, average train loss 1.464824, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 587756/3263200 (epoch 18), batch 59912, train_loss = 1.508, time/batch = 0.099\n",
      "Sequence 588736/3263200 (epoch 18), batch 60012, train_loss = 0.909, time/batch = 0.102\n",
      "Sequence 589696/3263200 (epoch 18), batch 60112, train_loss = 2.439, time/batch = 0.099\n",
      "Sequence 590686/3263200 (epoch 18), batch 60212, train_loss = 1.257, time/batch = 0.098\n",
      "Sequence 591676/3263200 (epoch 18), batch 60312, train_loss = 1.388, time/batch = 0.095\n",
      "Sequence 592656/3263200 (epoch 18), batch 60412, train_loss = 1.514, time/batch = 0.093\n",
      "Sequence 593626/3263200 (epoch 18), batch 60512, train_loss = 1.557, time/batch = 0.095\n",
      "Sequence 594596/3263200 (epoch 18), batch 60612, train_loss = 1.251, time/batch = 0.098\n",
      "Sequence 595586/3263200 (epoch 18), batch 60712, train_loss = 0.929, time/batch = 0.096\n",
      "Sequence 596576/3263200 (epoch 18), batch 60812, train_loss = 1.170, time/batch = 0.095\n",
      "Sequence 597556/3263200 (epoch 18), batch 60912, train_loss = 1.392, time/batch = 0.099\n",
      "Sequence 598536/3263200 (epoch 18), batch 61012, train_loss = 1.531, time/batch = 0.098\n",
      "Sequence 599516/3263200 (epoch 18), batch 61112, train_loss = 1.113, time/batch = 0.099\n",
      "Sequence 600476/3263200 (epoch 18), batch 61212, train_loss = 0.998, time/batch = 0.101\n",
      "Sequence 601456/3263200 (epoch 18), batch 61312, train_loss = 1.635, time/batch = 0.102\n",
      "Sequence 602446/3263200 (epoch 18), batch 61412, train_loss = 1.820, time/batch = 0.101\n",
      "Sequence 603436/3263200 (epoch 18), batch 61512, train_loss = 2.078, time/batch = 0.099\n",
      "Sequence 604416/3263200 (epoch 18), batch 61612, train_loss = 1.424, time/batch = 0.096\n",
      "Sequence 605406/3263200 (epoch 18), batch 61712, train_loss = 2.222, time/batch = 0.095\n",
      "Sequence 606366/3263200 (epoch 18), batch 61812, train_loss = 1.630, time/batch = 0.097\n",
      "Sequence 607336/3263200 (epoch 18), batch 61912, train_loss = 2.130, time/batch = 0.099\n",
      "Sequence 608306/3263200 (epoch 18), batch 62012, train_loss = 1.654, time/batch = 0.098\n",
      "Sequence 609306/3263200 (epoch 18), batch 62112, train_loss = 1.108, time/batch = 0.098\n",
      "Sequence 610276/3263200 (epoch 18), batch 62212, train_loss = 1.267, time/batch = 0.098\n",
      "Sequence 611266/3263200 (epoch 18), batch 62312, train_loss = 0.918, time/batch = 0.100\n",
      "Sequence 612246/3263200 (epoch 18), batch 62412, train_loss = 1.247, time/batch = 0.098\n",
      "Sequence 613226/3263200 (epoch 18), batch 62512, train_loss = 1.660, time/batch = 0.098\n",
      "Sequence 614226/3263200 (epoch 18), batch 62612, train_loss = 1.805, time/batch = 0.098\n",
      "Sequence 615216/3263200 (epoch 18), batch 62712, train_loss = 1.324, time/batch = 0.098\n",
      "Sequence 616196/3263200 (epoch 18), batch 62812, train_loss = 1.323, time/batch = 0.096\n",
      "Sequence 617186/3263200 (epoch 18), batch 62912, train_loss = 0.971, time/batch = 0.095\n",
      "Sequence 618176/3263200 (epoch 18), batch 63012, train_loss = 1.548, time/batch = 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 619136/3263200 (epoch 18), batch 63112, train_loss = 1.165, time/batch = 0.097\n",
      "Epoch 18 completed, average train loss 1.452224, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 620138/3263200 (epoch 19), batch 63212, train_loss = 1.640, time/batch = 0.096\n",
      "Sequence 621128/3263200 (epoch 19), batch 63312, train_loss = 1.082, time/batch = 0.097\n",
      "Sequence 622118/3263200 (epoch 19), batch 63413, train_loss = 1.204, time/batch = 0.098\n",
      "Sequence 623108/3263200 (epoch 19), batch 63514, train_loss = 0.900, time/batch = 0.100\n",
      "Sequence 624078/3263200 (epoch 19), batch 63614, train_loss = 1.185, time/batch = 0.097\n",
      "Sequence 625048/3263200 (epoch 19), batch 63714, train_loss = 1.258, time/batch = 0.100\n",
      "Sequence 626048/3263200 (epoch 19), batch 63814, train_loss = 1.440, time/batch = 0.097\n",
      "Sequence 627038/3263200 (epoch 19), batch 63914, train_loss = 0.816, time/batch = 0.096\n",
      "Sequence 628008/3263200 (epoch 19), batch 64015, train_loss = 0.728, time/batch = 0.099\n",
      "Sequence 628998/3263200 (epoch 19), batch 64115, train_loss = 2.146, time/batch = 0.098\n",
      "Sequence 629958/3263200 (epoch 19), batch 64215, train_loss = 2.001, time/batch = 0.099\n",
      "Sequence 630958/3263200 (epoch 19), batch 64315, train_loss = 2.356, time/batch = 0.098\n",
      "Sequence 631938/3263200 (epoch 19), batch 64415, train_loss = 1.002, time/batch = 0.099\n",
      "Sequence 632938/3263200 (epoch 19), batch 64516, train_loss = 0.651, time/batch = 0.100\n",
      "Sequence 633918/3263200 (epoch 19), batch 64616, train_loss = 0.747, time/batch = 0.102\n",
      "Sequence 634888/3263200 (epoch 19), batch 64716, train_loss = 1.410, time/batch = 0.099\n",
      "Sequence 635878/3263200 (epoch 19), batch 64816, train_loss = 0.754, time/batch = 0.101\n",
      "Sequence 636848/3263200 (epoch 19), batch 64916, train_loss = 1.663, time/batch = 0.100\n",
      "Sequence 637848/3263200 (epoch 19), batch 65016, train_loss = 1.223, time/batch = 0.098\n",
      "Sequence 638818/3263200 (epoch 19), batch 65116, train_loss = 1.614, time/batch = 0.102\n",
      "Sequence 639768/3263200 (epoch 19), batch 65216, train_loss = 1.294, time/batch = 0.100\n",
      "Sequence 640748/3263200 (epoch 19), batch 65316, train_loss = 1.353, time/batch = 0.100\n",
      "Sequence 641728/3263200 (epoch 19), batch 65416, train_loss = 1.825, time/batch = 0.099\n",
      "Sequence 642708/3263200 (epoch 19), batch 65516, train_loss = 1.682, time/batch = 0.099\n",
      "Sequence 643698/3263200 (epoch 19), batch 65616, train_loss = 1.425, time/batch = 0.100\n",
      "Sequence 644678/3263200 (epoch 19), batch 65716, train_loss = 1.440, time/batch = 0.099\n",
      "Sequence 645658/3263200 (epoch 19), batch 65816, train_loss = 2.342, time/batch = 0.101\n",
      "Sequence 646638/3263200 (epoch 19), batch 65916, train_loss = 1.879, time/batch = 0.102\n",
      "Sequence 647618/3263200 (epoch 19), batch 66016, train_loss = 1.103, time/batch = 0.099\n",
      "Sequence 648608/3263200 (epoch 19), batch 66116, train_loss = 2.627, time/batch = 0.104\n",
      "Sequence 649588/3263200 (epoch 19), batch 66216, train_loss = 1.438, time/batch = 0.098\n",
      "Sequence 650568/3263200 (epoch 19), batch 66316, train_loss = 1.016, time/batch = 0.101\n",
      "Sequence 651548/3263200 (epoch 19), batch 66416, train_loss = 1.083, time/batch = 0.101\n",
      "Sequence 652538/3263200 (epoch 19), batch 66516, train_loss = 2.407, time/batch = 0.100\n",
      "Epoch 19 completed, average train loss 1.440069, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 653520/3263200 (epoch 20), batch 66616, train_loss = 1.199, time/batch = 0.100\n",
      "Sequence 654500/3263200 (epoch 20), batch 66716, train_loss = 1.013, time/batch = 0.100\n",
      "Sequence 655480/3263200 (epoch 20), batch 66816, train_loss = 1.848, time/batch = 0.100\n",
      "Sequence 656440/3263200 (epoch 20), batch 66916, train_loss = 1.377, time/batch = 0.100\n",
      "Sequence 657420/3263200 (epoch 20), batch 67016, train_loss = 2.271, time/batch = 0.100\n",
      "Sequence 658410/3263200 (epoch 20), batch 67116, train_loss = 1.160, time/batch = 0.100\n",
      "Sequence 659400/3263200 (epoch 20), batch 67216, train_loss = 1.182, time/batch = 0.099\n",
      "Sequence 660380/3263200 (epoch 20), batch 67316, train_loss = 1.087, time/batch = 0.100\n",
      "Sequence 661380/3263200 (epoch 20), batch 67416, train_loss = 0.736, time/batch = 0.099\n",
      "Sequence 662350/3263200 (epoch 20), batch 67516, train_loss = 1.305, time/batch = 0.099\n",
      "Sequence 663340/3263200 (epoch 20), batch 67616, train_loss = 1.070, time/batch = 0.102\n",
      "Sequence 664330/3263200 (epoch 20), batch 67716, train_loss = 2.290, time/batch = 0.099\n",
      "Sequence 665310/3263200 (epoch 20), batch 67816, train_loss = 0.705, time/batch = 0.102\n",
      "Sequence 666290/3263200 (epoch 20), batch 67916, train_loss = 2.394, time/batch = 0.100\n",
      "Sequence 667290/3263200 (epoch 20), batch 68016, train_loss = 1.582, time/batch = 0.100\n",
      "Sequence 668250/3263200 (epoch 20), batch 68116, train_loss = 1.349, time/batch = 0.100\n",
      "Sequence 669240/3263200 (epoch 20), batch 68216, train_loss = 2.084, time/batch = 0.100\n",
      "Sequence 670230/3263200 (epoch 20), batch 68316, train_loss = 1.373, time/batch = 0.099\n",
      "Sequence 671220/3263200 (epoch 20), batch 68416, train_loss = 1.475, time/batch = 0.100\n",
      "Sequence 672190/3263200 (epoch 20), batch 68516, train_loss = 0.870, time/batch = 0.104\n",
      "Sequence 673170/3263200 (epoch 20), batch 68616, train_loss = 1.160, time/batch = 0.101\n",
      "Sequence 674140/3263200 (epoch 20), batch 68716, train_loss = 1.567, time/batch = 0.099\n",
      "Sequence 675140/3263200 (epoch 20), batch 68816, train_loss = 1.484, time/batch = 0.098\n",
      "Sequence 676130/3263200 (epoch 20), batch 68916, train_loss = 1.904, time/batch = 0.102\n",
      "Sequence 677120/3263200 (epoch 20), batch 69016, train_loss = 1.862, time/batch = 0.100\n",
      "Sequence 678080/3263200 (epoch 20), batch 69116, train_loss = 2.131, time/batch = 0.101\n",
      "Sequence 679060/3263200 (epoch 20), batch 69216, train_loss = 2.455, time/batch = 0.100\n",
      "Sequence 680040/3263200 (epoch 20), batch 69316, train_loss = 1.378, time/batch = 0.100\n",
      "Sequence 681020/3263200 (epoch 20), batch 69416, train_loss = 1.458, time/batch = 0.100\n",
      "Sequence 681980/3263200 (epoch 20), batch 69516, train_loss = 1.194, time/batch = 0.099\n",
      "Sequence 682950/3263200 (epoch 20), batch 69616, train_loss = 1.674, time/batch = 0.098\n",
      "Sequence 683940/3263200 (epoch 20), batch 69716, train_loss = 1.149, time/batch = 0.100\n",
      "Sequence 684910/3263200 (epoch 20), batch 69816, train_loss = 1.086, time/batch = 0.101\n",
      "Epoch 20 completed, average train loss 1.428447, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 685912/3263200 (epoch 21), batch 69916, train_loss = 1.325, time/batch = 0.100\n",
      "Sequence 686872/3263200 (epoch 21), batch 70016, train_loss = 1.315, time/batch = 0.102\n",
      "Sequence 687852/3263200 (epoch 21), batch 70116, train_loss = 0.772, time/batch = 0.100\n",
      "Sequence 688822/3263200 (epoch 21), batch 70216, train_loss = 1.117, time/batch = 0.098\n",
      "Sequence 689812/3263200 (epoch 21), batch 70316, train_loss = 1.740, time/batch = 0.098\n",
      "Sequence 690782/3263200 (epoch 21), batch 70416, train_loss = 1.347, time/batch = 0.098\n",
      "Sequence 691762/3263200 (epoch 21), batch 70516, train_loss = 0.847, time/batch = 0.096\n",
      "Sequence 692752/3263200 (epoch 21), batch 70616, train_loss = 1.432, time/batch = 0.097\n",
      "Sequence 693742/3263200 (epoch 21), batch 70716, train_loss = 2.007, time/batch = 0.097\n",
      "Sequence 694702/3263200 (epoch 21), batch 70816, train_loss = 0.810, time/batch = 0.097\n",
      "Sequence 695692/3263200 (epoch 21), batch 70916, train_loss = 1.010, time/batch = 0.098\n",
      "Sequence 696682/3263200 (epoch 21), batch 71016, train_loss = 1.199, time/batch = 0.097\n",
      "Sequence 697672/3263200 (epoch 21), batch 71116, train_loss = 1.658, time/batch = 0.098\n",
      "Sequence 698672/3263200 (epoch 21), batch 71216, train_loss = 1.587, time/batch = 0.096\n",
      "Sequence 699662/3263200 (epoch 21), batch 71316, train_loss = 2.260, time/batch = 0.099\n",
      "Sequence 700652/3263200 (epoch 21), batch 71416, train_loss = 1.707, time/batch = 0.098\n",
      "Sequence 701622/3263200 (epoch 21), batch 71516, train_loss = 1.193, time/batch = 0.099\n",
      "Sequence 702572/3263200 (epoch 21), batch 71616, train_loss = 0.641, time/batch = 0.100\n",
      "Sequence 703522/3263200 (epoch 21), batch 71716, train_loss = 1.519, time/batch = 0.102\n",
      "Sequence 704522/3263200 (epoch 21), batch 71816, train_loss = 1.619, time/batch = 0.100\n",
      "Sequence 705472/3263200 (epoch 21), batch 71916, train_loss = 1.044, time/batch = 0.098\n",
      "Sequence 706462/3263200 (epoch 21), batch 72016, train_loss = 1.669, time/batch = 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 707452/3263200 (epoch 21), batch 72116, train_loss = 1.637, time/batch = 0.100\n",
      "Sequence 708442/3263200 (epoch 21), batch 72216, train_loss = 1.661, time/batch = 0.100\n",
      "Sequence 709422/3263200 (epoch 21), batch 72316, train_loss = 1.197, time/batch = 0.102\n",
      "Sequence 710402/3263200 (epoch 21), batch 72416, train_loss = 1.941, time/batch = 0.104\n",
      "Sequence 711382/3263200 (epoch 21), batch 72516, train_loss = 1.636, time/batch = 0.101\n",
      "Sequence 712362/3263200 (epoch 21), batch 72616, train_loss = 1.535, time/batch = 0.098\n",
      "Sequence 713352/3263200 (epoch 21), batch 72716, train_loss = 1.634, time/batch = 0.098\n",
      "Sequence 714332/3263200 (epoch 21), batch 72816, train_loss = 1.300, time/batch = 0.100\n",
      "Sequence 715322/3263200 (epoch 21), batch 72916, train_loss = 1.157, time/batch = 0.100\n",
      "Sequence 716322/3263200 (epoch 21), batch 73016, train_loss = 1.973, time/batch = 0.102\n",
      "Sequence 717302/3263200 (epoch 21), batch 73116, train_loss = 2.173, time/batch = 0.098\n",
      "Epoch 21 completed, average train loss 1.418856, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 718274/3263200 (epoch 22), batch 73216, train_loss = 1.634, time/batch = 0.102\n",
      "Sequence 719264/3263200 (epoch 22), batch 73316, train_loss = 1.983, time/batch = 0.100\n",
      "Sequence 720254/3263200 (epoch 22), batch 73416, train_loss = 2.990, time/batch = 0.099\n",
      "Sequence 721254/3263200 (epoch 22), batch 73516, train_loss = 1.402, time/batch = 0.101\n",
      "Sequence 722214/3263200 (epoch 22), batch 73616, train_loss = 1.729, time/batch = 0.103\n",
      "Sequence 723204/3263200 (epoch 22), batch 73716, train_loss = 1.020, time/batch = 0.099\n",
      "Sequence 724194/3263200 (epoch 22), batch 73816, train_loss = 1.590, time/batch = 0.100\n",
      "Sequence 725164/3263200 (epoch 22), batch 73916, train_loss = 1.859, time/batch = 0.099\n",
      "Sequence 726134/3263200 (epoch 22), batch 74016, train_loss = 1.328, time/batch = 0.100\n",
      "Sequence 727084/3263200 (epoch 22), batch 74116, train_loss = 0.935, time/batch = 0.096\n",
      "Sequence 728074/3263200 (epoch 22), batch 74216, train_loss = 1.699, time/batch = 0.098\n",
      "Sequence 729074/3263200 (epoch 22), batch 74316, train_loss = 1.726, time/batch = 0.098\n",
      "Sequence 730064/3263200 (epoch 22), batch 74416, train_loss = 1.651, time/batch = 0.097\n",
      "Sequence 731054/3263200 (epoch 22), batch 74516, train_loss = 1.318, time/batch = 0.098\n",
      "Sequence 731984/3263200 (epoch 22), batch 74616, train_loss = 0.834, time/batch = 0.099\n",
      "Sequence 732984/3263200 (epoch 22), batch 74716, train_loss = 1.490, time/batch = 0.096\n",
      "Sequence 733954/3263200 (epoch 22), batch 74816, train_loss = 1.410, time/batch = 0.096\n",
      "Sequence 734944/3263200 (epoch 22), batch 74916, train_loss = 1.306, time/batch = 0.097\n",
      "Sequence 735924/3263200 (epoch 22), batch 75016, train_loss = 1.157, time/batch = 0.098\n",
      "Sequence 736914/3263200 (epoch 22), batch 75116, train_loss = 1.722, time/batch = 0.097\n",
      "Sequence 737894/3263200 (epoch 22), batch 75216, train_loss = 1.292, time/batch = 0.097\n",
      "Sequence 738874/3263200 (epoch 22), batch 75316, train_loss = 1.844, time/batch = 0.097\n",
      "Sequence 739854/3263200 (epoch 22), batch 75416, train_loss = 1.895, time/batch = 0.099\n",
      "Sequence 740824/3263200 (epoch 22), batch 75516, train_loss = 1.077, time/batch = 0.096\n",
      "Sequence 741814/3263200 (epoch 22), batch 75616, train_loss = 2.183, time/batch = 0.096\n",
      "Sequence 742784/3263200 (epoch 22), batch 75716, train_loss = 1.327, time/batch = 0.096\n",
      "Sequence 743764/3263200 (epoch 22), batch 75816, train_loss = 1.770, time/batch = 0.097\n",
      "Sequence 744754/3263200 (epoch 22), batch 75916, train_loss = 1.383, time/batch = 0.097\n",
      "Sequence 745724/3263200 (epoch 22), batch 76016, train_loss = 1.099, time/batch = 0.098\n",
      "Sequence 746714/3263200 (epoch 22), batch 76116, train_loss = 1.392, time/batch = 0.097\n",
      "Sequence 747714/3263200 (epoch 22), batch 76216, train_loss = 1.007, time/batch = 0.099\n",
      "Sequence 748694/3263200 (epoch 22), batch 76316, train_loss = 1.076, time/batch = 0.098\n",
      "Sequence 749684/3263200 (epoch 22), batch 76416, train_loss = 0.868, time/batch = 0.098\n",
      "Epoch 22 completed, average train loss 1.408591, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 750656/3263200 (epoch 23), batch 76516, train_loss = 1.194, time/batch = 0.104\n",
      "Sequence 751636/3263200 (epoch 23), batch 76616, train_loss = 1.863, time/batch = 0.099\n",
      "Sequence 752616/3263200 (epoch 23), batch 76716, train_loss = 1.818, time/batch = 0.101\n",
      "Sequence 753596/3263200 (epoch 23), batch 76816, train_loss = 0.902, time/batch = 0.100\n",
      "Sequence 754576/3263200 (epoch 23), batch 76916, train_loss = 1.629, time/batch = 0.100\n",
      "Sequence 755546/3263200 (epoch 23), batch 77016, train_loss = 1.526, time/batch = 0.097\n",
      "Sequence 756526/3263200 (epoch 23), batch 77116, train_loss = 0.887, time/batch = 0.100\n",
      "Sequence 757476/3263200 (epoch 23), batch 77216, train_loss = 1.093, time/batch = 0.100\n",
      "Sequence 758476/3263200 (epoch 23), batch 77316, train_loss = 1.178, time/batch = 0.099\n",
      "Sequence 759456/3263200 (epoch 23), batch 77416, train_loss = 1.431, time/batch = 0.098\n",
      "Sequence 760436/3263200 (epoch 23), batch 77516, train_loss = 1.172, time/batch = 0.098\n",
      "Sequence 761416/3263200 (epoch 23), batch 77616, train_loss = 1.641, time/batch = 0.097\n",
      "Sequence 762386/3263200 (epoch 23), batch 77716, train_loss = 1.499, time/batch = 0.097\n",
      "Sequence 763376/3263200 (epoch 23), batch 77816, train_loss = 1.422, time/batch = 0.096\n",
      "Sequence 764376/3263200 (epoch 23), batch 77916, train_loss = 1.522, time/batch = 0.099\n",
      "Sequence 765346/3263200 (epoch 23), batch 78017, train_loss = 0.667, time/batch = 0.098\n",
      "Sequence 766336/3263200 (epoch 23), batch 78117, train_loss = 1.359, time/batch = 0.099\n",
      "Sequence 767316/3263200 (epoch 23), batch 78217, train_loss = 0.928, time/batch = 0.099\n",
      "Sequence 768306/3263200 (epoch 23), batch 78317, train_loss = 1.406, time/batch = 0.098\n",
      "Sequence 769286/3263200 (epoch 23), batch 78417, train_loss = 1.072, time/batch = 0.098\n",
      "Sequence 770276/3263200 (epoch 23), batch 78517, train_loss = 0.969, time/batch = 0.101\n",
      "Sequence 771256/3263200 (epoch 23), batch 78618, train_loss = 2.388, time/batch = 0.100\n",
      "Sequence 772256/3263200 (epoch 23), batch 78718, train_loss = 1.079, time/batch = 0.100\n",
      "Sequence 773236/3263200 (epoch 23), batch 78818, train_loss = 1.487, time/batch = 0.099\n",
      "Sequence 774216/3263200 (epoch 23), batch 78918, train_loss = 1.374, time/batch = 0.101\n",
      "Sequence 775176/3263200 (epoch 23), batch 79018, train_loss = 0.878, time/batch = 0.100\n",
      "Sequence 776146/3263200 (epoch 23), batch 79118, train_loss = 1.967, time/batch = 0.101\n",
      "Sequence 777136/3263200 (epoch 23), batch 79218, train_loss = 1.026, time/batch = 0.100\n",
      "Sequence 778116/3263200 (epoch 23), batch 79318, train_loss = 0.932, time/batch = 0.100\n",
      "Sequence 779096/3263200 (epoch 23), batch 79418, train_loss = 2.151, time/batch = 0.101\n",
      "Sequence 780076/3263200 (epoch 23), batch 79518, train_loss = 1.979, time/batch = 0.099\n",
      "Sequence 781056/3263200 (epoch 23), batch 79618, train_loss = 1.035, time/batch = 0.098\n",
      "Sequence 782046/3263200 (epoch 23), batch 79718, train_loss = 0.997, time/batch = 0.099\n",
      "Sequence 783036/3263200 (epoch 23), batch 79818, train_loss = 2.050, time/batch = 0.103\n",
      "Epoch 23 completed, average train loss 1.400293, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 784038/3263200 (epoch 24), batch 79918, train_loss = 1.816, time/batch = 0.100\n",
      "Sequence 785028/3263200 (epoch 24), batch 80018, train_loss = 1.846, time/batch = 0.101\n",
      "Sequence 785988/3263200 (epoch 24), batch 80118, train_loss = 1.445, time/batch = 0.102\n",
      "Sequence 786968/3263200 (epoch 24), batch 80218, train_loss = 1.213, time/batch = 0.101\n",
      "Sequence 787938/3263200 (epoch 24), batch 80318, train_loss = 1.578, time/batch = 0.103\n",
      "Sequence 788918/3263200 (epoch 24), batch 80418, train_loss = 1.360, time/batch = 0.099\n",
      "Sequence 789908/3263200 (epoch 24), batch 80518, train_loss = 1.536, time/batch = 0.100\n",
      "Sequence 790888/3263200 (epoch 24), batch 80618, train_loss = 1.153, time/batch = 0.100\n",
      "Sequence 791878/3263200 (epoch 24), batch 80718, train_loss = 1.669, time/batch = 0.099\n",
      "Sequence 792838/3263200 (epoch 24), batch 80818, train_loss = 0.925, time/batch = 0.100\n",
      "Sequence 793828/3263200 (epoch 24), batch 80918, train_loss = 1.286, time/batch = 0.100\n",
      "Sequence 794788/3263200 (epoch 24), batch 81018, train_loss = 1.781, time/batch = 0.099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 795758/3263200 (epoch 24), batch 81118, train_loss = 1.942, time/batch = 0.098\n",
      "Sequence 796728/3263200 (epoch 24), batch 81218, train_loss = 1.532, time/batch = 0.098\n",
      "Sequence 797708/3263200 (epoch 24), batch 81318, train_loss = 1.604, time/batch = 0.097\n",
      "Sequence 798668/3263200 (epoch 24), batch 81418, train_loss = 1.671, time/batch = 0.098\n",
      "Sequence 799668/3263200 (epoch 24), batch 81518, train_loss = 1.712, time/batch = 0.099\n",
      "Sequence 800658/3263200 (epoch 24), batch 81618, train_loss = 1.623, time/batch = 0.100\n",
      "Sequence 801618/3263200 (epoch 24), batch 81718, train_loss = 1.118, time/batch = 0.098\n",
      "Sequence 802608/3263200 (epoch 24), batch 81818, train_loss = 0.963, time/batch = 0.098\n",
      "Sequence 803588/3263200 (epoch 24), batch 81918, train_loss = 1.144, time/batch = 0.098\n",
      "Sequence 804588/3263200 (epoch 24), batch 82018, train_loss = 1.479, time/batch = 0.095\n",
      "Sequence 805588/3263200 (epoch 24), batch 82118, train_loss = 1.265, time/batch = 0.093\n",
      "Sequence 806568/3263200 (epoch 24), batch 82218, train_loss = 1.490, time/batch = 0.096\n",
      "Sequence 807558/3263200 (epoch 24), batch 82318, train_loss = 1.443, time/batch = 0.095\n",
      "Sequence 808528/3263200 (epoch 24), batch 82418, train_loss = 1.465, time/batch = 0.098\n",
      "Sequence 809508/3263200 (epoch 24), batch 82518, train_loss = 1.286, time/batch = 0.096\n",
      "Sequence 810478/3263200 (epoch 24), batch 82618, train_loss = 0.762, time/batch = 0.095\n",
      "Sequence 811458/3263200 (epoch 24), batch 82718, train_loss = 1.165, time/batch = 0.097\n",
      "Sequence 812428/3263200 (epoch 24), batch 82818, train_loss = 1.749, time/batch = 0.096\n",
      "Sequence 813428/3263200 (epoch 24), batch 82918, train_loss = 1.257, time/batch = 0.098\n",
      "Sequence 814408/3263200 (epoch 24), batch 83018, train_loss = 1.155, time/batch = 0.099\n",
      "Sequence 815398/3263200 (epoch 24), batch 83118, train_loss = 0.770, time/batch = 0.098\n",
      "Epoch 24 completed, average train loss 1.393986, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 816400/3263200 (epoch 25), batch 83218, train_loss = 1.194, time/batch = 0.099\n",
      "Sequence 817380/3263200 (epoch 25), batch 83318, train_loss = 1.207, time/batch = 0.099\n",
      "Sequence 818360/3263200 (epoch 25), batch 83418, train_loss = 2.924, time/batch = 0.098\n",
      "Sequence 819340/3263200 (epoch 25), batch 83519, train_loss = 0.652, time/batch = 0.098\n",
      "Sequence 820320/3263200 (epoch 25), batch 83619, train_loss = 1.613, time/batch = 0.101\n",
      "Sequence 821290/3263200 (epoch 25), batch 83719, train_loss = 1.070, time/batch = 0.099\n",
      "Sequence 822280/3263200 (epoch 25), batch 83819, train_loss = 1.297, time/batch = 0.099\n",
      "Sequence 823260/3263200 (epoch 25), batch 83919, train_loss = 1.083, time/batch = 0.101\n",
      "Sequence 824230/3263200 (epoch 25), batch 84019, train_loss = 1.620, time/batch = 0.101\n",
      "Sequence 825210/3263200 (epoch 25), batch 84119, train_loss = 1.061, time/batch = 0.098\n",
      "Sequence 826170/3263200 (epoch 25), batch 84219, train_loss = 2.003, time/batch = 0.100\n",
      "Sequence 827140/3263200 (epoch 25), batch 84319, train_loss = 1.385, time/batch = 0.099\n",
      "Sequence 828130/3263200 (epoch 25), batch 84419, train_loss = 2.143, time/batch = 0.101\n",
      "Sequence 829110/3263200 (epoch 25), batch 84519, train_loss = 1.596, time/batch = 0.098\n",
      "Sequence 830080/3263200 (epoch 25), batch 84619, train_loss = 2.322, time/batch = 0.096\n",
      "Sequence 831060/3263200 (epoch 25), batch 84719, train_loss = 1.491, time/batch = 0.097\n",
      "Sequence 832030/3263200 (epoch 25), batch 84819, train_loss = 1.655, time/batch = 0.100\n",
      "Sequence 833030/3263200 (epoch 25), batch 84919, train_loss = 0.938, time/batch = 0.098\n",
      "Sequence 833980/3263200 (epoch 25), batch 85019, train_loss = 1.680, time/batch = 0.098\n",
      "Sequence 834970/3263200 (epoch 25), batch 85119, train_loss = 1.191, time/batch = 0.098\n",
      "Sequence 835960/3263200 (epoch 25), batch 85219, train_loss = 1.126, time/batch = 0.096\n",
      "Sequence 836960/3263200 (epoch 25), batch 85319, train_loss = 1.550, time/batch = 0.096\n",
      "Sequence 837950/3263200 (epoch 25), batch 85419, train_loss = 1.593, time/batch = 0.097\n",
      "Sequence 838940/3263200 (epoch 25), batch 85519, train_loss = 1.373, time/batch = 0.097\n",
      "Sequence 839910/3263200 (epoch 25), batch 85619, train_loss = 0.726, time/batch = 0.098\n",
      "Sequence 840870/3263200 (epoch 25), batch 85719, train_loss = 1.041, time/batch = 0.100\n",
      "Sequence 841850/3263200 (epoch 25), batch 85819, train_loss = 1.137, time/batch = 0.099\n",
      "Sequence 842830/3263200 (epoch 25), batch 85919, train_loss = 1.173, time/batch = 0.098\n",
      "Sequence 843830/3263200 (epoch 25), batch 86019, train_loss = 1.457, time/batch = 0.096\n",
      "Sequence 844810/3263200 (epoch 25), batch 86119, train_loss = 1.916, time/batch = 0.096\n",
      "Sequence 845800/3263200 (epoch 25), batch 86219, train_loss = 1.218, time/batch = 0.097\n",
      "Sequence 846770/3263200 (epoch 25), batch 86319, train_loss = 1.008, time/batch = 0.097\n",
      "Sequence 847770/3263200 (epoch 25), batch 86419, train_loss = 1.292, time/batch = 0.098\n",
      "Epoch 25 completed, average train loss 1.384008, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 848772/3263200 (epoch 26), batch 86519, train_loss = 1.028, time/batch = 0.099\n",
      "Sequence 849762/3263200 (epoch 26), batch 86619, train_loss = 0.858, time/batch = 0.099\n",
      "Sequence 850752/3263200 (epoch 26), batch 86719, train_loss = 1.078, time/batch = 0.099\n",
      "Sequence 851732/3263200 (epoch 26), batch 86819, train_loss = 0.727, time/batch = 0.099\n",
      "Sequence 852732/3263200 (epoch 26), batch 86919, train_loss = 1.554, time/batch = 0.101\n",
      "Sequence 853702/3263200 (epoch 26), batch 87019, train_loss = 1.305, time/batch = 0.097\n",
      "Sequence 854692/3263200 (epoch 26), batch 87119, train_loss = 2.283, time/batch = 0.099\n",
      "Sequence 855682/3263200 (epoch 26), batch 87219, train_loss = 1.513, time/batch = 0.100\n",
      "Sequence 856652/3263200 (epoch 26), batch 87319, train_loss = 1.713, time/batch = 0.100\n",
      "Sequence 857642/3263200 (epoch 26), batch 87419, train_loss = 1.706, time/batch = 0.097\n",
      "Sequence 858612/3263200 (epoch 26), batch 87519, train_loss = 1.333, time/batch = 0.096\n",
      "Sequence 859602/3263200 (epoch 26), batch 87619, train_loss = 0.943, time/batch = 0.096\n",
      "Sequence 860582/3263200 (epoch 26), batch 87719, train_loss = 1.805, time/batch = 0.098\n",
      "Sequence 861572/3263200 (epoch 26), batch 87819, train_loss = 1.299, time/batch = 0.098\n",
      "Sequence 862562/3263200 (epoch 26), batch 87919, train_loss = 1.239, time/batch = 0.097\n",
      "Sequence 863532/3263200 (epoch 26), batch 88019, train_loss = 0.802, time/batch = 0.098\n",
      "Sequence 864522/3263200 (epoch 26), batch 88119, train_loss = 1.788, time/batch = 0.102\n",
      "Sequence 865492/3263200 (epoch 26), batch 88219, train_loss = 1.328, time/batch = 0.102\n",
      "Sequence 866472/3263200 (epoch 26), batch 88319, train_loss = 1.382, time/batch = 0.103\n",
      "Sequence 867442/3263200 (epoch 26), batch 88419, train_loss = 1.424, time/batch = 0.101\n",
      "Sequence 868422/3263200 (epoch 26), batch 88519, train_loss = 1.518, time/batch = 0.099\n",
      "Sequence 869392/3263200 (epoch 26), batch 88619, train_loss = 0.933, time/batch = 0.100\n",
      "Sequence 870382/3263200 (epoch 26), batch 88719, train_loss = 1.657, time/batch = 0.098\n",
      "Sequence 871362/3263200 (epoch 26), batch 88819, train_loss = 1.372, time/batch = 0.099\n",
      "Sequence 872332/3263200 (epoch 26), batch 88919, train_loss = 1.221, time/batch = 0.100\n",
      "Sequence 873312/3263200 (epoch 26), batch 89019, train_loss = 1.137, time/batch = 0.099\n",
      "Sequence 874312/3263200 (epoch 26), batch 89119, train_loss = 1.513, time/batch = 0.099\n",
      "Sequence 875292/3263200 (epoch 26), batch 89219, train_loss = 1.641, time/batch = 0.099\n",
      "Sequence 876272/3263200 (epoch 26), batch 89319, train_loss = 1.263, time/batch = 0.100\n",
      "Sequence 877242/3263200 (epoch 26), batch 89419, train_loss = 1.794, time/batch = 0.099\n",
      "Sequence 878232/3263200 (epoch 26), batch 89519, train_loss = 1.066, time/batch = 0.101\n",
      "Sequence 879212/3263200 (epoch 26), batch 89619, train_loss = 1.779, time/batch = 0.099\n",
      "Sequence 880192/3263200 (epoch 26), batch 89719, train_loss = 1.719, time/batch = 0.100\n",
      "Epoch 26 completed, average train loss 1.376370, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 881134/3263200 (epoch 27), batch 89819, train_loss = 1.135, time/batch = 0.100\n",
      "Sequence 882114/3263200 (epoch 27), batch 89919, train_loss = 0.946, time/batch = 0.100\n",
      "Sequence 883104/3263200 (epoch 27), batch 90019, train_loss = 1.360, time/batch = 0.098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 884064/3263200 (epoch 27), batch 90119, train_loss = 2.146, time/batch = 0.098\n",
      "Sequence 885044/3263200 (epoch 27), batch 90219, train_loss = 1.887, time/batch = 0.097\n",
      "Sequence 886034/3263200 (epoch 27), batch 90319, train_loss = 1.937, time/batch = 0.098\n",
      "Sequence 886984/3263200 (epoch 27), batch 90419, train_loss = 1.067, time/batch = 0.098\n",
      "Sequence 887984/3263200 (epoch 27), batch 90519, train_loss = 1.501, time/batch = 0.100\n",
      "Sequence 888954/3263200 (epoch 27), batch 90619, train_loss = 1.165, time/batch = 0.099\n",
      "Sequence 889924/3263200 (epoch 27), batch 90719, train_loss = 0.818, time/batch = 0.099\n",
      "Sequence 890904/3263200 (epoch 27), batch 90819, train_loss = 1.607, time/batch = 0.099\n",
      "Sequence 891884/3263200 (epoch 27), batch 90919, train_loss = 1.156, time/batch = 0.100\n",
      "Sequence 892864/3263200 (epoch 27), batch 91019, train_loss = 1.033, time/batch = 0.099\n",
      "Sequence 893844/3263200 (epoch 27), batch 91119, train_loss = 1.465, time/batch = 0.096\n",
      "Sequence 894834/3263200 (epoch 27), batch 91219, train_loss = 1.506, time/batch = 0.098\n",
      "Sequence 895834/3263200 (epoch 27), batch 91319, train_loss = 2.192, time/batch = 0.097\n",
      "Sequence 896804/3263200 (epoch 27), batch 91419, train_loss = 0.992, time/batch = 0.100\n",
      "Sequence 897794/3263200 (epoch 27), batch 91519, train_loss = 1.049, time/batch = 0.101\n",
      "Sequence 898794/3263200 (epoch 27), batch 91619, train_loss = 1.314, time/batch = 0.101\n",
      "Sequence 899784/3263200 (epoch 27), batch 91719, train_loss = 1.160, time/batch = 0.102\n",
      "Sequence 900764/3263200 (epoch 27), batch 91819, train_loss = 0.891, time/batch = 0.099\n",
      "Sequence 901764/3263200 (epoch 27), batch 91919, train_loss = 1.695, time/batch = 0.103\n",
      "Sequence 902734/3263200 (epoch 27), batch 92019, train_loss = 0.972, time/batch = 0.097\n",
      "Sequence 903694/3263200 (epoch 27), batch 92119, train_loss = 1.099, time/batch = 0.099\n",
      "Sequence 904674/3263200 (epoch 27), batch 92219, train_loss = 1.054, time/batch = 0.098\n",
      "Sequence 905644/3263200 (epoch 27), batch 92319, train_loss = 1.310, time/batch = 0.099\n",
      "Sequence 906634/3263200 (epoch 27), batch 92419, train_loss = 0.996, time/batch = 0.098\n",
      "Sequence 907604/3263200 (epoch 27), batch 92519, train_loss = 1.497, time/batch = 0.098\n",
      "Sequence 908574/3263200 (epoch 27), batch 92619, train_loss = 1.570, time/batch = 0.099\n",
      "Sequence 909574/3263200 (epoch 27), batch 92719, train_loss = 1.517, time/batch = 0.102\n",
      "Sequence 910534/3263200 (epoch 27), batch 92819, train_loss = 1.923, time/batch = 0.099\n",
      "Sequence 911534/3263200 (epoch 27), batch 92919, train_loss = 1.198, time/batch = 0.099\n",
      "Sequence 912504/3263200 (epoch 27), batch 93019, train_loss = 1.135, time/batch = 0.097\n",
      "Sequence 913504/3263200 (epoch 27), batch 93119, train_loss = 1.299, time/batch = 0.098\n",
      "Epoch 27 completed, average train loss 1.368495, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 914466/3263200 (epoch 28), batch 93220, train_loss = 0.661, time/batch = 0.102\n",
      "Sequence 915466/3263200 (epoch 28), batch 93320, train_loss = 0.840, time/batch = 0.100\n",
      "Sequence 916446/3263200 (epoch 28), batch 93420, train_loss = 1.221, time/batch = 0.097\n",
      "Sequence 917426/3263200 (epoch 28), batch 93520, train_loss = 0.787, time/batch = 0.096\n",
      "Sequence 918416/3263200 (epoch 28), batch 93620, train_loss = 1.355, time/batch = 0.096\n",
      "Sequence 919406/3263200 (epoch 28), batch 93720, train_loss = 1.865, time/batch = 0.096\n",
      "Sequence 920386/3263200 (epoch 28), batch 93820, train_loss = 1.720, time/batch = 0.097\n",
      "Sequence 921356/3263200 (epoch 28), batch 93920, train_loss = 1.429, time/batch = 0.102\n",
      "Sequence 922326/3263200 (epoch 28), batch 94020, train_loss = 1.254, time/batch = 0.100\n",
      "Sequence 923316/3263200 (epoch 28), batch 94120, train_loss = 0.754, time/batch = 0.101\n",
      "Sequence 924306/3263200 (epoch 28), batch 94220, train_loss = 1.480, time/batch = 0.098\n",
      "Sequence 925286/3263200 (epoch 28), batch 94320, train_loss = 1.334, time/batch = 0.098\n",
      "Sequence 926256/3263200 (epoch 28), batch 94420, train_loss = 1.184, time/batch = 0.100\n",
      "Sequence 927236/3263200 (epoch 28), batch 94520, train_loss = 0.917, time/batch = 0.097\n",
      "Sequence 928196/3263200 (epoch 28), batch 94620, train_loss = 1.010, time/batch = 0.103\n",
      "Sequence 929176/3263200 (epoch 28), batch 94720, train_loss = 1.859, time/batch = 0.097\n",
      "Sequence 930166/3263200 (epoch 28), batch 94821, train_loss = 0.967, time/batch = 0.096\n",
      "Sequence 931166/3263200 (epoch 28), batch 94921, train_loss = 1.943, time/batch = 0.098\n",
      "Sequence 932156/3263200 (epoch 28), batch 95021, train_loss = 1.232, time/batch = 0.099\n",
      "Sequence 933116/3263200 (epoch 28), batch 95121, train_loss = 1.325, time/batch = 0.098\n",
      "Sequence 934116/3263200 (epoch 28), batch 95221, train_loss = 1.529, time/batch = 0.098\n",
      "Sequence 935096/3263200 (epoch 28), batch 95321, train_loss = 1.225, time/batch = 0.098\n",
      "Sequence 936086/3263200 (epoch 28), batch 95421, train_loss = 1.251, time/batch = 0.097\n",
      "Sequence 937076/3263200 (epoch 28), batch 95521, train_loss = 1.254, time/batch = 0.099\n",
      "Sequence 938076/3263200 (epoch 28), batch 95621, train_loss = 1.393, time/batch = 0.098\n",
      "Sequence 939066/3263200 (epoch 28), batch 95721, train_loss = 1.363, time/batch = 0.100\n",
      "Sequence 940056/3263200 (epoch 28), batch 95821, train_loss = 0.910, time/batch = 0.101\n",
      "Sequence 941016/3263200 (epoch 28), batch 95921, train_loss = 1.164, time/batch = 0.101\n",
      "Sequence 941986/3263200 (epoch 28), batch 96021, train_loss = 1.371, time/batch = 0.098\n",
      "Sequence 942966/3263200 (epoch 28), batch 96121, train_loss = 1.293, time/batch = 0.098\n",
      "Sequence 943926/3263200 (epoch 28), batch 96221, train_loss = 2.126, time/batch = 0.097\n",
      "Sequence 944916/3263200 (epoch 28), batch 96321, train_loss = 1.327, time/batch = 0.098\n",
      "Sequence 945886/3263200 (epoch 28), batch 96421, train_loss = 1.357, time/batch = 0.096\n",
      "Epoch 28 completed, average train loss 1.362638, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 946878/3263200 (epoch 29), batch 96521, train_loss = 1.322, time/batch = 0.099\n",
      "Sequence 947868/3263200 (epoch 29), batch 96621, train_loss = 1.043, time/batch = 0.100\n",
      "Sequence 948858/3263200 (epoch 29), batch 96721, train_loss = 1.579, time/batch = 0.101\n",
      "Sequence 949838/3263200 (epoch 29), batch 96821, train_loss = 1.340, time/batch = 0.101\n",
      "Sequence 950838/3263200 (epoch 29), batch 96921, train_loss = 1.126, time/batch = 0.097\n",
      "Sequence 951808/3263200 (epoch 29), batch 97021, train_loss = 1.608, time/batch = 0.098\n",
      "Sequence 952768/3263200 (epoch 29), batch 97121, train_loss = 1.589, time/batch = 0.099\n",
      "Sequence 953728/3263200 (epoch 29), batch 97221, train_loss = 1.770, time/batch = 0.102\n",
      "Sequence 954718/3263200 (epoch 29), batch 97321, train_loss = 1.811, time/batch = 0.101\n",
      "Sequence 955708/3263200 (epoch 29), batch 97421, train_loss = 1.437, time/batch = 0.101\n",
      "Sequence 956698/3263200 (epoch 29), batch 97521, train_loss = 1.283, time/batch = 0.099\n",
      "Sequence 957678/3263200 (epoch 29), batch 97621, train_loss = 1.331, time/batch = 0.101\n",
      "Sequence 958668/3263200 (epoch 29), batch 97721, train_loss = 1.455, time/batch = 0.103\n",
      "Sequence 959668/3263200 (epoch 29), batch 97821, train_loss = 0.973, time/batch = 0.106\n",
      "Sequence 960648/3263200 (epoch 29), batch 97921, train_loss = 1.167, time/batch = 0.098\n",
      "Sequence 961608/3263200 (epoch 29), batch 98021, train_loss = 1.966, time/batch = 0.101\n",
      "Sequence 962608/3263200 (epoch 29), batch 98121, train_loss = 1.369, time/batch = 0.102\n",
      "Sequence 963588/3263200 (epoch 29), batch 98221, train_loss = 1.048, time/batch = 0.098\n",
      "Sequence 964538/3263200 (epoch 29), batch 98321, train_loss = 1.271, time/batch = 0.095\n",
      "Sequence 965518/3263200 (epoch 29), batch 98421, train_loss = 1.190, time/batch = 0.095\n",
      "Sequence 966508/3263200 (epoch 29), batch 98521, train_loss = 1.305, time/batch = 0.097\n",
      "Sequence 967488/3263200 (epoch 29), batch 98621, train_loss = 1.585, time/batch = 0.097\n",
      "Sequence 968468/3263200 (epoch 29), batch 98721, train_loss = 1.109, time/batch = 0.097\n",
      "Sequence 969438/3263200 (epoch 29), batch 98821, train_loss = 1.131, time/batch = 0.099\n",
      "Sequence 970408/3263200 (epoch 29), batch 98921, train_loss = 1.265, time/batch = 0.098\n",
      "Sequence 971398/3263200 (epoch 29), batch 99022, train_loss = 0.827, time/batch = 0.097\n",
      "Sequence 972398/3263200 (epoch 29), batch 99122, train_loss = 1.452, time/batch = 0.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 973398/3263200 (epoch 29), batch 99222, train_loss = 1.438, time/batch = 0.101\n",
      "Sequence 974368/3263200 (epoch 29), batch 99322, train_loss = 1.200, time/batch = 0.117\n",
      "Sequence 975348/3263200 (epoch 29), batch 99422, train_loss = 1.260, time/batch = 0.118\n",
      "Sequence 976328/3263200 (epoch 29), batch 99522, train_loss = 1.227, time/batch = 0.118\n",
      "Sequence 977318/3263200 (epoch 29), batch 99622, train_loss = 1.193, time/batch = 0.115\n",
      "Sequence 978298/3263200 (epoch 29), batch 99722, train_loss = 1.184, time/batch = 0.118\n",
      "Epoch 29 completed, average train loss 1.355694, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 979270/3263200 (epoch 30), batch 99823, train_loss = 0.744, time/batch = 0.118\n",
      "Sequence 980250/3263200 (epoch 30), batch 99923, train_loss = 1.566, time/batch = 0.118\n",
      "Sequence 981250/3263200 (epoch 30), batch 100023, train_loss = 1.044, time/batch = 0.119\n",
      "Sequence 982240/3263200 (epoch 30), batch 100123, train_loss = 1.604, time/batch = 0.114\n",
      "Sequence 983220/3263200 (epoch 30), batch 100223, train_loss = 0.925, time/batch = 0.099\n",
      "Sequence 984200/3263200 (epoch 30), batch 100323, train_loss = 1.126, time/batch = 0.097\n",
      "Sequence 985190/3263200 (epoch 30), batch 100423, train_loss = 1.219, time/batch = 0.095\n",
      "Sequence 986130/3263200 (epoch 30), batch 100523, train_loss = 1.332, time/batch = 0.096\n",
      "Sequence 987130/3263200 (epoch 30), batch 100623, train_loss = 0.651, time/batch = 0.097\n",
      "Sequence 988120/3263200 (epoch 30), batch 100723, train_loss = 1.168, time/batch = 0.097\n",
      "Sequence 989070/3263200 (epoch 30), batch 100823, train_loss = 0.875, time/batch = 0.102\n",
      "Sequence 990070/3263200 (epoch 30), batch 100923, train_loss = 0.984, time/batch = 0.099\n",
      "Sequence 991070/3263200 (epoch 30), batch 101023, train_loss = 1.152, time/batch = 0.096\n",
      "Sequence 992050/3263200 (epoch 30), batch 101123, train_loss = 2.421, time/batch = 0.097\n",
      "Sequence 993050/3263200 (epoch 30), batch 101223, train_loss = 1.039, time/batch = 0.099\n",
      "Sequence 994030/3263200 (epoch 30), batch 101323, train_loss = 0.958, time/batch = 0.096\n",
      "Sequence 995010/3263200 (epoch 30), batch 101423, train_loss = 1.836, time/batch = 0.096\n",
      "Sequence 995990/3263200 (epoch 30), batch 101523, train_loss = 1.328, time/batch = 0.098\n",
      "Sequence 996980/3263200 (epoch 30), batch 101623, train_loss = 1.118, time/batch = 0.101\n",
      "Sequence 997980/3263200 (epoch 30), batch 101723, train_loss = 1.226, time/batch = 0.101\n",
      "Sequence 998920/3263200 (epoch 30), batch 101823, train_loss = 1.979, time/batch = 0.098\n",
      "Sequence 999920/3263200 (epoch 30), batch 101923, train_loss = 1.507, time/batch = 0.098\n",
      "Sequence 1000910/3263200 (epoch 30), batch 102023, train_loss = 1.139, time/batch = 0.101\n",
      "Sequence 1001870/3263200 (epoch 30), batch 102123, train_loss = 1.540, time/batch = 0.099\n",
      "Sequence 1002840/3263200 (epoch 30), batch 102223, train_loss = 0.735, time/batch = 0.098\n",
      "Sequence 1003820/3263200 (epoch 30), batch 102323, train_loss = 1.683, time/batch = 0.099\n",
      "Sequence 1004760/3263200 (epoch 30), batch 102423, train_loss = 1.599, time/batch = 0.100\n",
      "Sequence 1005730/3263200 (epoch 30), batch 102523, train_loss = 1.316, time/batch = 0.100\n",
      "Sequence 1006710/3263200 (epoch 30), batch 102623, train_loss = 1.965, time/batch = 0.100\n",
      "Sequence 1007670/3263200 (epoch 30), batch 102723, train_loss = 1.022, time/batch = 0.101\n",
      "Sequence 1008660/3263200 (epoch 30), batch 102823, train_loss = 1.058, time/batch = 0.100\n",
      "Sequence 1009650/3263200 (epoch 30), batch 102923, train_loss = 1.160, time/batch = 0.099\n",
      "Sequence 1010640/3263200 (epoch 30), batch 103023, train_loss = 1.352, time/batch = 0.098\n",
      "Epoch 30 completed, average train loss 1.346365, learning rate 0.0010\n",
      "Sequence 1011642/3263200 (epoch 31), batch 103123, train_loss = 1.042, time/batch = 0.099\n",
      "Shuffling training data...\n",
      "Sequence 1012642/3263200 (epoch 31), batch 103223, train_loss = 1.256, time/batch = 0.102\n",
      "Sequence 1013622/3263200 (epoch 31), batch 103323, train_loss = 1.070, time/batch = 0.098\n",
      "Sequence 1014592/3263200 (epoch 31), batch 103423, train_loss = 0.711, time/batch = 0.100\n",
      "Sequence 1015592/3263200 (epoch 31), batch 103523, train_loss = 1.237, time/batch = 0.098\n",
      "Sequence 1016592/3263200 (epoch 31), batch 103623, train_loss = 1.207, time/batch = 0.100\n",
      "Sequence 1017572/3263200 (epoch 31), batch 103723, train_loss = 1.838, time/batch = 0.103\n",
      "Sequence 1018562/3263200 (epoch 31), batch 103823, train_loss = 1.028, time/batch = 0.097\n",
      "Sequence 1019532/3263200 (epoch 31), batch 103923, train_loss = 1.329, time/batch = 0.098\n",
      "Sequence 1020512/3263200 (epoch 31), batch 104023, train_loss = 1.071, time/batch = 0.097\n",
      "Sequence 1021482/3263200 (epoch 31), batch 104123, train_loss = 1.847, time/batch = 0.100\n",
      "Sequence 1022442/3263200 (epoch 31), batch 104223, train_loss = 2.285, time/batch = 0.098\n",
      "Sequence 1023412/3263200 (epoch 31), batch 104324, train_loss = 0.804, time/batch = 0.100\n",
      "Sequence 1024402/3263200 (epoch 31), batch 104424, train_loss = 1.017, time/batch = 0.096\n",
      "Sequence 1025372/3263200 (epoch 31), batch 104524, train_loss = 0.962, time/batch = 0.099\n",
      "Sequence 1026352/3263200 (epoch 31), batch 104624, train_loss = 1.198, time/batch = 0.100\n",
      "Sequence 1027352/3263200 (epoch 31), batch 104724, train_loss = 1.525, time/batch = 0.101\n",
      "Sequence 1028302/3263200 (epoch 31), batch 104824, train_loss = 0.971, time/batch = 0.099\n",
      "Sequence 1029252/3263200 (epoch 31), batch 104924, train_loss = 1.809, time/batch = 0.100\n",
      "Sequence 1030222/3263200 (epoch 31), batch 105024, train_loss = 1.569, time/batch = 0.097\n",
      "Sequence 1031222/3263200 (epoch 31), batch 105124, train_loss = 0.743, time/batch = 0.098\n",
      "Sequence 1032202/3263200 (epoch 31), batch 105224, train_loss = 0.999, time/batch = 0.098\n",
      "Sequence 1033192/3263200 (epoch 31), batch 105324, train_loss = 1.564, time/batch = 0.103\n",
      "Sequence 1034152/3263200 (epoch 31), batch 105424, train_loss = 1.044, time/batch = 0.099\n",
      "Sequence 1035132/3263200 (epoch 31), batch 105524, train_loss = 0.916, time/batch = 0.099\n",
      "Sequence 1036132/3263200 (epoch 31), batch 105624, train_loss = 1.119, time/batch = 0.099\n",
      "Sequence 1037132/3263200 (epoch 31), batch 105724, train_loss = 1.346, time/batch = 0.097\n",
      "Sequence 1038112/3263200 (epoch 31), batch 105824, train_loss = 1.459, time/batch = 0.098\n",
      "Sequence 1039102/3263200 (epoch 31), batch 105924, train_loss = 0.883, time/batch = 0.096\n",
      "Sequence 1040072/3263200 (epoch 31), batch 106024, train_loss = 0.844, time/batch = 0.096\n",
      "Sequence 1041042/3263200 (epoch 31), batch 106124, train_loss = 1.462, time/batch = 0.098\n",
      "Sequence 1042032/3263200 (epoch 31), batch 106224, train_loss = 1.123, time/batch = 0.099\n",
      "Sequence 1043022/3263200 (epoch 31), batch 106324, train_loss = 1.361, time/batch = 0.099\n",
      "Sequence 1044012/3263200 (epoch 31), batch 106424, train_loss = 1.215, time/batch = 0.099\n",
      "Epoch 31 completed, average train loss 1.340211, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1044974/3263200 (epoch 32), batch 106524, train_loss = 1.222, time/batch = 0.101\n",
      "Sequence 1045964/3263200 (epoch 32), batch 106624, train_loss = 1.401, time/batch = 0.100\n",
      "Sequence 1046924/3263200 (epoch 32), batch 106724, train_loss = 1.563, time/batch = 0.099\n",
      "Sequence 1047914/3263200 (epoch 32), batch 106824, train_loss = 1.192, time/batch = 0.099\n",
      "Sequence 1048874/3263200 (epoch 32), batch 106924, train_loss = 0.844, time/batch = 0.100\n",
      "Sequence 1049864/3263200 (epoch 32), batch 107024, train_loss = 1.223, time/batch = 0.098\n",
      "Sequence 1050824/3263200 (epoch 32), batch 107124, train_loss = 1.510, time/batch = 0.099\n",
      "Sequence 1051794/3263200 (epoch 32), batch 107224, train_loss = 1.127, time/batch = 0.099\n",
      "Sequence 1052794/3263200 (epoch 32), batch 107324, train_loss = 1.294, time/batch = 0.098\n",
      "Sequence 1053784/3263200 (epoch 32), batch 107424, train_loss = 0.982, time/batch = 0.097\n",
      "Sequence 1054754/3263200 (epoch 32), batch 107524, train_loss = 0.961, time/batch = 0.097\n",
      "Sequence 1055744/3263200 (epoch 32), batch 107624, train_loss = 1.695, time/batch = 0.099\n",
      "Sequence 1056704/3263200 (epoch 32), batch 107724, train_loss = 2.810, time/batch = 0.100\n",
      "Sequence 1057684/3263200 (epoch 32), batch 107824, train_loss = 1.316, time/batch = 0.098\n",
      "Sequence 1058674/3263200 (epoch 32), batch 107924, train_loss = 1.199, time/batch = 0.098\n",
      "Sequence 1059664/3263200 (epoch 32), batch 108024, train_loss = 1.641, time/batch = 0.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1060664/3263200 (epoch 32), batch 108124, train_loss = 0.838, time/batch = 0.101\n",
      "Sequence 1061634/3263200 (epoch 32), batch 108224, train_loss = 1.180, time/batch = 0.101\n",
      "Sequence 1062624/3263200 (epoch 32), batch 108324, train_loss = 1.164, time/batch = 0.101\n",
      "Sequence 1063624/3263200 (epoch 32), batch 108424, train_loss = 0.946, time/batch = 0.100\n",
      "Sequence 1064594/3263200 (epoch 32), batch 108524, train_loss = 1.170, time/batch = 0.100\n",
      "Sequence 1065584/3263200 (epoch 32), batch 108624, train_loss = 1.766, time/batch = 0.099\n",
      "Sequence 1066584/3263200 (epoch 32), batch 108724, train_loss = 1.117, time/batch = 0.098\n",
      "Sequence 1067574/3263200 (epoch 32), batch 108824, train_loss = 1.450, time/batch = 0.101\n",
      "Sequence 1068574/3263200 (epoch 32), batch 108925, train_loss = 0.635, time/batch = 0.101\n",
      "Sequence 1069574/3263200 (epoch 32), batch 109025, train_loss = 0.974, time/batch = 0.098\n",
      "Sequence 1070514/3263200 (epoch 32), batch 109125, train_loss = 1.307, time/batch = 0.100\n",
      "Sequence 1071494/3263200 (epoch 32), batch 109225, train_loss = 1.444, time/batch = 0.102\n",
      "Sequence 1072484/3263200 (epoch 32), batch 109325, train_loss = 1.746, time/batch = 0.097\n",
      "Sequence 1073444/3263200 (epoch 32), batch 109425, train_loss = 0.931, time/batch = 0.098\n",
      "Sequence 1074444/3263200 (epoch 32), batch 109525, train_loss = 1.366, time/batch = 0.100\n",
      "Sequence 1075424/3263200 (epoch 32), batch 109625, train_loss = 1.205, time/batch = 0.100\n",
      "Sequence 1076414/3263200 (epoch 32), batch 109725, train_loss = 2.021, time/batch = 0.100\n",
      "Epoch 32 completed, average train loss 1.334950, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1077376/3263200 (epoch 33), batch 109825, train_loss = 1.427, time/batch = 0.102\n",
      "Sequence 1078366/3263200 (epoch 33), batch 109925, train_loss = 1.152, time/batch = 0.099\n",
      "Sequence 1079326/3263200 (epoch 33), batch 110025, train_loss = 1.388, time/batch = 0.100\n",
      "Sequence 1080306/3263200 (epoch 33), batch 110125, train_loss = 1.119, time/batch = 0.102\n",
      "Sequence 1081296/3263200 (epoch 33), batch 110225, train_loss = 2.013, time/batch = 0.101\n",
      "Sequence 1082266/3263200 (epoch 33), batch 110325, train_loss = 1.432, time/batch = 0.103\n",
      "Sequence 1083246/3263200 (epoch 33), batch 110425, train_loss = 1.641, time/batch = 0.099\n",
      "Sequence 1084226/3263200 (epoch 33), batch 110525, train_loss = 1.687, time/batch = 0.099\n",
      "Sequence 1085206/3263200 (epoch 33), batch 110625, train_loss = 1.029, time/batch = 0.099\n",
      "Sequence 1086176/3263200 (epoch 33), batch 110725, train_loss = 1.541, time/batch = 0.098\n",
      "Sequence 1087156/3263200 (epoch 33), batch 110825, train_loss = 1.615, time/batch = 0.097\n",
      "Sequence 1088146/3263200 (epoch 33), batch 110925, train_loss = 1.285, time/batch = 0.097\n",
      "Sequence 1089126/3263200 (epoch 33), batch 111025, train_loss = 1.473, time/batch = 0.098\n",
      "Sequence 1090086/3263200 (epoch 33), batch 111125, train_loss = 1.395, time/batch = 0.097\n",
      "Sequence 1091046/3263200 (epoch 33), batch 111225, train_loss = 1.193, time/batch = 0.101\n",
      "Sequence 1092026/3263200 (epoch 33), batch 111325, train_loss = 1.260, time/batch = 0.101\n",
      "Sequence 1093016/3263200 (epoch 33), batch 111425, train_loss = 1.364, time/batch = 0.100\n",
      "Sequence 1093976/3263200 (epoch 33), batch 111525, train_loss = 1.724, time/batch = 0.097\n",
      "Sequence 1094966/3263200 (epoch 33), batch 111625, train_loss = 1.439, time/batch = 0.097\n",
      "Sequence 1095946/3263200 (epoch 33), batch 111725, train_loss = 0.976, time/batch = 0.099\n",
      "Sequence 1096946/3263200 (epoch 33), batch 111825, train_loss = 1.610, time/batch = 0.097\n",
      "Sequence 1097926/3263200 (epoch 33), batch 111925, train_loss = 1.484, time/batch = 0.100\n",
      "Sequence 1098916/3263200 (epoch 33), batch 112025, train_loss = 2.117, time/batch = 0.100\n",
      "Sequence 1099896/3263200 (epoch 33), batch 112125, train_loss = 1.052, time/batch = 0.100\n",
      "Sequence 1100886/3263200 (epoch 33), batch 112225, train_loss = 0.848, time/batch = 0.095\n",
      "Sequence 1101866/3263200 (epoch 33), batch 112326, train_loss = 0.594, time/batch = 0.095\n",
      "Sequence 1102856/3263200 (epoch 33), batch 112426, train_loss = 1.474, time/batch = 0.098\n",
      "Sequence 1103846/3263200 (epoch 33), batch 112526, train_loss = 1.039, time/batch = 0.099\n",
      "Sequence 1104846/3263200 (epoch 33), batch 112626, train_loss = 1.389, time/batch = 0.100\n",
      "Sequence 1105826/3263200 (epoch 33), batch 112726, train_loss = 0.983, time/batch = 0.099\n",
      "Sequence 1106826/3263200 (epoch 33), batch 112826, train_loss = 1.488, time/batch = 0.098\n",
      "Sequence 1107806/3263200 (epoch 33), batch 112926, train_loss = 1.118, time/batch = 0.097\n",
      "Sequence 1108786/3263200 (epoch 33), batch 113026, train_loss = 1.210, time/batch = 0.097\n",
      "Epoch 33 completed, average train loss 1.329168, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1109768/3263200 (epoch 34), batch 113126, train_loss = 1.823, time/batch = 0.096\n",
      "Sequence 1110748/3263200 (epoch 34), batch 113226, train_loss = 1.028, time/batch = 0.096\n",
      "Sequence 1111688/3263200 (epoch 34), batch 113326, train_loss = 1.189, time/batch = 0.098\n",
      "Sequence 1112678/3263200 (epoch 34), batch 113426, train_loss = 1.209, time/batch = 0.100\n",
      "Sequence 1113668/3263200 (epoch 34), batch 113526, train_loss = 1.326, time/batch = 0.096\n",
      "Sequence 1114628/3263200 (epoch 34), batch 113626, train_loss = 1.469, time/batch = 0.097\n",
      "Sequence 1115628/3263200 (epoch 34), batch 113726, train_loss = 1.225, time/batch = 0.097\n",
      "Sequence 1116608/3263200 (epoch 34), batch 113826, train_loss = 1.414, time/batch = 0.098\n",
      "Sequence 1117588/3263200 (epoch 34), batch 113926, train_loss = 1.218, time/batch = 0.096\n",
      "Sequence 1118588/3263200 (epoch 34), batch 114026, train_loss = 1.407, time/batch = 0.096\n",
      "Sequence 1119578/3263200 (epoch 34), batch 114126, train_loss = 1.518, time/batch = 0.095\n",
      "Sequence 1120568/3263200 (epoch 34), batch 114226, train_loss = 2.009, time/batch = 0.098\n",
      "Sequence 1121568/3263200 (epoch 34), batch 114326, train_loss = 1.527, time/batch = 0.099\n",
      "Sequence 1122568/3263200 (epoch 34), batch 114426, train_loss = 0.559, time/batch = 0.100\n",
      "Sequence 1123548/3263200 (epoch 34), batch 114526, train_loss = 1.432, time/batch = 0.102\n",
      "Sequence 1124548/3263200 (epoch 34), batch 114626, train_loss = 1.885, time/batch = 0.101\n",
      "Sequence 1125528/3263200 (epoch 34), batch 114726, train_loss = 1.533, time/batch = 0.099\n",
      "Sequence 1126498/3263200 (epoch 34), batch 114826, train_loss = 1.923, time/batch = 0.100\n",
      "Sequence 1127478/3263200 (epoch 34), batch 114926, train_loss = 1.394, time/batch = 0.097\n",
      "Sequence 1128458/3263200 (epoch 34), batch 115026, train_loss = 1.107, time/batch = 0.099\n",
      "Sequence 1129418/3263200 (epoch 34), batch 115126, train_loss = 1.953, time/batch = 0.097\n",
      "Sequence 1130388/3263200 (epoch 34), batch 115226, train_loss = 1.822, time/batch = 0.099\n",
      "Sequence 1131358/3263200 (epoch 34), batch 115326, train_loss = 0.800, time/batch = 0.099\n",
      "Sequence 1132338/3263200 (epoch 34), batch 115426, train_loss = 1.379, time/batch = 0.096\n",
      "Sequence 1133328/3263200 (epoch 34), batch 115526, train_loss = 1.941, time/batch = 0.096\n",
      "Sequence 1134308/3263200 (epoch 34), batch 115626, train_loss = 1.509, time/batch = 0.096\n",
      "Sequence 1135308/3263200 (epoch 34), batch 115726, train_loss = 1.834, time/batch = 0.100\n",
      "Sequence 1136288/3263200 (epoch 34), batch 115826, train_loss = 1.248, time/batch = 0.099\n",
      "Sequence 1137228/3263200 (epoch 34), batch 115926, train_loss = 1.022, time/batch = 0.098\n",
      "Sequence 1138218/3263200 (epoch 34), batch 116026, train_loss = 1.939, time/batch = 0.098\n",
      "Sequence 1139178/3263200 (epoch 34), batch 116126, train_loss = 1.323, time/batch = 0.101\n",
      "Sequence 1140178/3263200 (epoch 34), batch 116226, train_loss = 1.294, time/batch = 0.098\n",
      "Sequence 1141148/3263200 (epoch 34), batch 116326, train_loss = 1.492, time/batch = 0.098\n",
      "Epoch 34 completed, average train loss 1.323367, learning rate 0.0010\n",
      "model saved.\n",
      "Sequence 1142140/3263200 (epoch 35), batch 116426, train_loss = 1.467, time/batch = 0.101\n",
      "Shuffling training data...\n",
      "Sequence 1143110/3263200 (epoch 35), batch 116526, train_loss = 1.069, time/batch = 0.098\n",
      "Sequence 1144100/3263200 (epoch 35), batch 116626, train_loss = 1.033, time/batch = 0.098\n",
      "Sequence 1145090/3263200 (epoch 35), batch 116726, train_loss = 1.142, time/batch = 0.100\n",
      "Sequence 1146060/3263200 (epoch 35), batch 116826, train_loss = 0.874, time/batch = 0.098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1147040/3263200 (epoch 35), batch 116926, train_loss = 0.625, time/batch = 0.098\n",
      "Sequence 1148020/3263200 (epoch 35), batch 117026, train_loss = 1.220, time/batch = 0.100\n",
      "Sequence 1148990/3263200 (epoch 35), batch 117126, train_loss = 1.105, time/batch = 0.098\n",
      "Sequence 1149960/3263200 (epoch 35), batch 117226, train_loss = 1.309, time/batch = 0.099\n",
      "Sequence 1150930/3263200 (epoch 35), batch 117326, train_loss = 1.442, time/batch = 0.099\n",
      "Sequence 1151910/3263200 (epoch 35), batch 117426, train_loss = 1.221, time/batch = 0.099\n",
      "Sequence 1152890/3263200 (epoch 35), batch 117526, train_loss = 1.732, time/batch = 0.100\n",
      "Sequence 1153870/3263200 (epoch 35), batch 117626, train_loss = 1.690, time/batch = 0.098\n",
      "Sequence 1154850/3263200 (epoch 35), batch 117726, train_loss = 1.296, time/batch = 0.099\n",
      "Sequence 1155840/3263200 (epoch 35), batch 117826, train_loss = 1.407, time/batch = 0.100\n",
      "Sequence 1156830/3263200 (epoch 35), batch 117926, train_loss = 1.846, time/batch = 0.100\n",
      "Sequence 1157820/3263200 (epoch 35), batch 118026, train_loss = 1.474, time/batch = 0.098\n",
      "Sequence 1158820/3263200 (epoch 35), batch 118126, train_loss = 1.502, time/batch = 0.096\n",
      "Sequence 1159780/3263200 (epoch 35), batch 118226, train_loss = 0.939, time/batch = 0.098\n",
      "Sequence 1160770/3263200 (epoch 35), batch 118326, train_loss = 2.162, time/batch = 0.099\n",
      "Sequence 1161760/3263200 (epoch 35), batch 118426, train_loss = 1.104, time/batch = 0.098\n",
      "Sequence 1162750/3263200 (epoch 35), batch 118526, train_loss = 1.372, time/batch = 0.100\n",
      "Sequence 1163710/3263200 (epoch 35), batch 118626, train_loss = 0.791, time/batch = 0.103\n",
      "Sequence 1164680/3263200 (epoch 35), batch 118726, train_loss = 1.945, time/batch = 0.100\n",
      "Sequence 1165670/3263200 (epoch 35), batch 118826, train_loss = 1.355, time/batch = 0.101\n",
      "Sequence 1166650/3263200 (epoch 35), batch 118926, train_loss = 1.730, time/batch = 0.099\n",
      "Sequence 1167630/3263200 (epoch 35), batch 119026, train_loss = 2.115, time/batch = 0.100\n",
      "Sequence 1168620/3263200 (epoch 35), batch 119126, train_loss = 1.035, time/batch = 0.100\n",
      "Sequence 1169580/3263200 (epoch 35), batch 119226, train_loss = 1.608, time/batch = 0.099\n",
      "Sequence 1170560/3263200 (epoch 35), batch 119326, train_loss = 0.778, time/batch = 0.100\n",
      "Sequence 1171530/3263200 (epoch 35), batch 119426, train_loss = 1.749, time/batch = 0.097\n",
      "Sequence 1172530/3263200 (epoch 35), batch 119526, train_loss = 1.447, time/batch = 0.097\n",
      "Sequence 1173530/3263200 (epoch 35), batch 119626, train_loss = 1.259, time/batch = 0.099\n",
      "Sequence 1174510/3263200 (epoch 35), batch 119726, train_loss = 1.038, time/batch = 0.101\n",
      "Epoch 35 completed, average train loss 1.316266, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1175492/3263200 (epoch 36), batch 119826, train_loss = 1.843, time/batch = 0.102\n",
      "Sequence 1176482/3263200 (epoch 36), batch 119926, train_loss = 1.080, time/batch = 0.100\n",
      "Sequence 1177472/3263200 (epoch 36), batch 120026, train_loss = 1.221, time/batch = 0.099\n",
      "Sequence 1178412/3263200 (epoch 36), batch 120126, train_loss = 1.066, time/batch = 0.100\n",
      "Sequence 1179382/3263200 (epoch 36), batch 120227, train_loss = 0.716, time/batch = 0.101\n",
      "Sequence 1180362/3263200 (epoch 36), batch 120327, train_loss = 0.887, time/batch = 0.100\n",
      "Sequence 1181332/3263200 (epoch 36), batch 120427, train_loss = 1.339, time/batch = 0.100\n",
      "Sequence 1182322/3263200 (epoch 36), batch 120527, train_loss = 0.828, time/batch = 0.097\n",
      "Sequence 1183292/3263200 (epoch 36), batch 120627, train_loss = 0.955, time/batch = 0.101\n",
      "Sequence 1184262/3263200 (epoch 36), batch 120727, train_loss = 1.116, time/batch = 0.100\n",
      "Sequence 1185252/3263200 (epoch 36), batch 120827, train_loss = 1.674, time/batch = 0.101\n",
      "Sequence 1186242/3263200 (epoch 36), batch 120927, train_loss = 1.403, time/batch = 0.099\n",
      "Sequence 1187232/3263200 (epoch 36), batch 121027, train_loss = 1.349, time/batch = 0.098\n",
      "Sequence 1188212/3263200 (epoch 36), batch 121127, train_loss = 1.609, time/batch = 0.099\n",
      "Sequence 1189202/3263200 (epoch 36), batch 121227, train_loss = 1.441, time/batch = 0.096\n",
      "Sequence 1190202/3263200 (epoch 36), batch 121327, train_loss = 1.559, time/batch = 0.095\n",
      "Sequence 1191182/3263200 (epoch 36), batch 121427, train_loss = 1.053, time/batch = 0.094\n",
      "Sequence 1192152/3263200 (epoch 36), batch 121527, train_loss = 1.383, time/batch = 0.095\n",
      "Sequence 1193142/3263200 (epoch 36), batch 121627, train_loss = 1.569, time/batch = 0.096\n",
      "Sequence 1194132/3263200 (epoch 36), batch 121727, train_loss = 1.249, time/batch = 0.097\n",
      "Sequence 1195132/3263200 (epoch 36), batch 121827, train_loss = 1.233, time/batch = 0.099\n",
      "Sequence 1196132/3263200 (epoch 36), batch 121927, train_loss = 0.862, time/batch = 0.097\n",
      "Sequence 1197122/3263200 (epoch 36), batch 122027, train_loss = 1.027, time/batch = 0.097\n",
      "Sequence 1198112/3263200 (epoch 36), batch 122127, train_loss = 0.760, time/batch = 0.102\n",
      "Sequence 1199072/3263200 (epoch 36), batch 122227, train_loss = 1.335, time/batch = 0.101\n",
      "Sequence 1200032/3263200 (epoch 36), batch 122327, train_loss = 1.652, time/batch = 0.098\n",
      "Sequence 1201022/3263200 (epoch 36), batch 122427, train_loss = 0.926, time/batch = 0.099\n",
      "Sequence 1201972/3263200 (epoch 36), batch 122527, train_loss = 0.853, time/batch = 0.100\n",
      "Sequence 1202952/3263200 (epoch 36), batch 122627, train_loss = 1.112, time/batch = 0.100\n",
      "Sequence 1203932/3263200 (epoch 36), batch 122727, train_loss = 2.359, time/batch = 0.098\n",
      "Sequence 1204932/3263200 (epoch 36), batch 122827, train_loss = 1.433, time/batch = 0.099\n",
      "Sequence 1205932/3263200 (epoch 36), batch 122927, train_loss = 0.850, time/batch = 0.101\n",
      "Sequence 1206912/3263200 (epoch 36), batch 123027, train_loss = 2.109, time/batch = 0.102\n",
      "Epoch 36 completed, average train loss 1.312816, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1207894/3263200 (epoch 37), batch 123127, train_loss = 0.985, time/batch = 0.100\n",
      "Sequence 1208844/3263200 (epoch 37), batch 123227, train_loss = 1.323, time/batch = 0.098\n",
      "Sequence 1209814/3263200 (epoch 37), batch 123327, train_loss = 0.779, time/batch = 0.100\n",
      "Sequence 1210804/3263200 (epoch 37), batch 123427, train_loss = 1.873, time/batch = 0.100\n",
      "Sequence 1211784/3263200 (epoch 37), batch 123527, train_loss = 1.093, time/batch = 0.103\n",
      "Sequence 1212774/3263200 (epoch 37), batch 123627, train_loss = 1.131, time/batch = 0.101\n",
      "Sequence 1213744/3263200 (epoch 37), batch 123727, train_loss = 1.701, time/batch = 0.102\n",
      "Sequence 1214714/3263200 (epoch 37), batch 123827, train_loss = 1.357, time/batch = 0.100\n",
      "Sequence 1215704/3263200 (epoch 37), batch 123927, train_loss = 1.555, time/batch = 0.097\n",
      "Sequence 1216694/3263200 (epoch 37), batch 124027, train_loss = 1.401, time/batch = 0.097\n",
      "Sequence 1217674/3263200 (epoch 37), batch 124127, train_loss = 1.276, time/batch = 0.099\n",
      "Sequence 1218654/3263200 (epoch 37), batch 124227, train_loss = 1.022, time/batch = 0.099\n",
      "Sequence 1219634/3263200 (epoch 37), batch 124327, train_loss = 1.210, time/batch = 0.099\n",
      "Sequence 1220574/3263200 (epoch 37), batch 124427, train_loss = 1.261, time/batch = 0.099\n",
      "Sequence 1221564/3263200 (epoch 37), batch 124527, train_loss = 1.909, time/batch = 0.100\n",
      "Sequence 1222554/3263200 (epoch 37), batch 124627, train_loss = 1.059, time/batch = 0.098\n",
      "Sequence 1223524/3263200 (epoch 37), batch 124727, train_loss = 0.756, time/batch = 0.096\n",
      "Sequence 1224524/3263200 (epoch 37), batch 124827, train_loss = 0.982, time/batch = 0.097\n",
      "Sequence 1225514/3263200 (epoch 37), batch 124927, train_loss = 1.483, time/batch = 0.098\n",
      "Sequence 1226504/3263200 (epoch 37), batch 125027, train_loss = 1.434, time/batch = 0.096\n",
      "Sequence 1227454/3263200 (epoch 37), batch 125127, train_loss = 1.511, time/batch = 0.097\n",
      "Sequence 1228434/3263200 (epoch 37), batch 125227, train_loss = 2.119, time/batch = 0.098\n",
      "Sequence 1229424/3263200 (epoch 37), batch 125327, train_loss = 1.422, time/batch = 0.098\n",
      "Sequence 1230404/3263200 (epoch 37), batch 125427, train_loss = 1.302, time/batch = 0.097\n",
      "Sequence 1231374/3263200 (epoch 37), batch 125527, train_loss = 1.707, time/batch = 0.101\n",
      "Sequence 1232354/3263200 (epoch 37), batch 125627, train_loss = 1.424, time/batch = 0.099\n",
      "Sequence 1233354/3263200 (epoch 37), batch 125727, train_loss = 1.068, time/batch = 0.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1234354/3263200 (epoch 37), batch 125827, train_loss = 1.350, time/batch = 0.096\n",
      "Sequence 1235344/3263200 (epoch 37), batch 125927, train_loss = 1.606, time/batch = 0.096\n",
      "Sequence 1236324/3263200 (epoch 37), batch 126027, train_loss = 1.109, time/batch = 0.097\n",
      "Sequence 1237314/3263200 (epoch 37), batch 126127, train_loss = 1.411, time/batch = 0.098\n",
      "Sequence 1238294/3263200 (epoch 37), batch 126227, train_loss = 1.774, time/batch = 0.097\n",
      "Sequence 1239274/3263200 (epoch 37), batch 126327, train_loss = 1.410, time/batch = 0.098\n",
      "Epoch 37 completed, average train loss 1.306156, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1240276/3263200 (epoch 38), batch 126427, train_loss = 1.385, time/batch = 0.098\n",
      "Sequence 1241236/3263200 (epoch 38), batch 126527, train_loss = 0.979, time/batch = 0.099\n",
      "Sequence 1242206/3263200 (epoch 38), batch 126627, train_loss = 1.073, time/batch = 0.103\n",
      "Sequence 1243176/3263200 (epoch 38), batch 126727, train_loss = 1.331, time/batch = 0.099\n",
      "Sequence 1244156/3263200 (epoch 38), batch 126827, train_loss = 1.447, time/batch = 0.101\n",
      "Sequence 1245136/3263200 (epoch 38), batch 126927, train_loss = 1.344, time/batch = 0.100\n",
      "Sequence 1246126/3263200 (epoch 38), batch 127027, train_loss = 0.863, time/batch = 0.101\n",
      "Sequence 1247126/3263200 (epoch 38), batch 127127, train_loss = 1.720, time/batch = 0.100\n",
      "Sequence 1248126/3263200 (epoch 38), batch 127227, train_loss = 1.445, time/batch = 0.101\n",
      "Sequence 1249106/3263200 (epoch 38), batch 127327, train_loss = 1.568, time/batch = 0.101\n",
      "Sequence 1250106/3263200 (epoch 38), batch 127427, train_loss = 1.762, time/batch = 0.101\n",
      "Sequence 1251096/3263200 (epoch 38), batch 127527, train_loss = 2.475, time/batch = 0.098\n",
      "Sequence 1252066/3263200 (epoch 38), batch 127627, train_loss = 1.147, time/batch = 0.101\n",
      "Sequence 1253046/3263200 (epoch 38), batch 127727, train_loss = 1.762, time/batch = 0.099\n",
      "Sequence 1254026/3263200 (epoch 38), batch 127827, train_loss = 1.070, time/batch = 0.097\n",
      "Sequence 1255006/3263200 (epoch 38), batch 127927, train_loss = 1.209, time/batch = 0.098\n",
      "Sequence 1255986/3263200 (epoch 38), batch 128027, train_loss = 2.326, time/batch = 0.097\n",
      "Sequence 1256966/3263200 (epoch 38), batch 128127, train_loss = 1.192, time/batch = 0.098\n",
      "Sequence 1257946/3263200 (epoch 38), batch 128227, train_loss = 0.914, time/batch = 0.098\n",
      "Sequence 1258916/3263200 (epoch 38), batch 128327, train_loss = 1.043, time/batch = 0.098\n",
      "Sequence 1259906/3263200 (epoch 38), batch 128427, train_loss = 1.022, time/batch = 0.100\n",
      "Sequence 1260866/3263200 (epoch 38), batch 128527, train_loss = 1.211, time/batch = 0.099\n",
      "Sequence 1261846/3263200 (epoch 38), batch 128627, train_loss = 1.747, time/batch = 0.099\n",
      "Sequence 1262806/3263200 (epoch 38), batch 128727, train_loss = 1.722, time/batch = 0.100\n",
      "Sequence 1263786/3263200 (epoch 38), batch 128827, train_loss = 1.200, time/batch = 0.101\n",
      "Sequence 1264756/3263200 (epoch 38), batch 128927, train_loss = 1.335, time/batch = 0.098\n",
      "Sequence 1265726/3263200 (epoch 38), batch 129027, train_loss = 0.885, time/batch = 0.098\n",
      "Sequence 1266716/3263200 (epoch 38), batch 129127, train_loss = 1.568, time/batch = 0.097\n",
      "Sequence 1267716/3263200 (epoch 38), batch 129227, train_loss = 1.321, time/batch = 0.099\n",
      "Sequence 1268696/3263200 (epoch 38), batch 129327, train_loss = 1.793, time/batch = 0.097\n",
      "Sequence 1269646/3263200 (epoch 38), batch 129427, train_loss = 0.703, time/batch = 0.098\n",
      "Sequence 1270646/3263200 (epoch 38), batch 129527, train_loss = 1.676, time/batch = 0.098\n",
      "Sequence 1271646/3263200 (epoch 38), batch 129627, train_loss = 1.323, time/batch = 0.097\n",
      "Sequence 1272636/3263200 (epoch 38), batch 129727, train_loss = 1.669, time/batch = 0.099\n",
      "Epoch 38 completed, average train loss 1.302380, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1273618/3263200 (epoch 39), batch 129827, train_loss = 1.593, time/batch = 0.098\n",
      "Sequence 1274608/3263200 (epoch 39), batch 129927, train_loss = 0.890, time/batch = 0.098\n",
      "Sequence 1275598/3263200 (epoch 39), batch 130027, train_loss = 1.128, time/batch = 0.098\n",
      "Sequence 1276598/3263200 (epoch 39), batch 130127, train_loss = 1.758, time/batch = 0.097\n",
      "Sequence 1277588/3263200 (epoch 39), batch 130227, train_loss = 1.331, time/batch = 0.097\n",
      "Sequence 1278588/3263200 (epoch 39), batch 130327, train_loss = 2.165, time/batch = 0.097\n",
      "Sequence 1279528/3263200 (epoch 39), batch 130427, train_loss = 1.213, time/batch = 0.098\n",
      "Sequence 1280488/3263200 (epoch 39), batch 130527, train_loss = 0.825, time/batch = 0.098\n",
      "Sequence 1281468/3263200 (epoch 39), batch 130627, train_loss = 1.219, time/batch = 0.098\n",
      "Sequence 1282468/3263200 (epoch 39), batch 130727, train_loss = 0.769, time/batch = 0.099\n",
      "Sequence 1283418/3263200 (epoch 39), batch 130827, train_loss = 1.365, time/batch = 0.100\n",
      "Sequence 1284388/3263200 (epoch 39), batch 130927, train_loss = 1.088, time/batch = 0.099\n",
      "Sequence 1285378/3263200 (epoch 39), batch 131027, train_loss = 1.301, time/batch = 0.100\n",
      "Sequence 1286348/3263200 (epoch 39), batch 131127, train_loss = 1.409, time/batch = 0.101\n",
      "Sequence 1287338/3263200 (epoch 39), batch 131227, train_loss = 1.203, time/batch = 0.102\n",
      "Sequence 1288308/3263200 (epoch 39), batch 131327, train_loss = 1.499, time/batch = 0.100\n",
      "Sequence 1289288/3263200 (epoch 39), batch 131427, train_loss = 1.768, time/batch = 0.101\n",
      "Sequence 1290268/3263200 (epoch 39), batch 131527, train_loss = 1.294, time/batch = 0.099\n",
      "Sequence 1291248/3263200 (epoch 39), batch 131627, train_loss = 1.442, time/batch = 0.101\n",
      "Sequence 1292238/3263200 (epoch 39), batch 131727, train_loss = 1.178, time/batch = 0.098\n",
      "Sequence 1293218/3263200 (epoch 39), batch 131827, train_loss = 1.351, time/batch = 0.098\n",
      "Sequence 1294208/3263200 (epoch 39), batch 131927, train_loss = 1.532, time/batch = 0.097\n",
      "Sequence 1295198/3263200 (epoch 39), batch 132027, train_loss = 1.368, time/batch = 0.101\n",
      "Sequence 1296178/3263200 (epoch 39), batch 132127, train_loss = 0.835, time/batch = 0.098\n",
      "Sequence 1297158/3263200 (epoch 39), batch 132227, train_loss = 1.051, time/batch = 0.097\n",
      "Sequence 1298108/3263200 (epoch 39), batch 132327, train_loss = 1.286, time/batch = 0.100\n",
      "Sequence 1299108/3263200 (epoch 39), batch 132427, train_loss = 1.148, time/batch = 0.096\n",
      "Sequence 1300108/3263200 (epoch 39), batch 132527, train_loss = 0.979, time/batch = 0.096\n",
      "Sequence 1301098/3263200 (epoch 39), batch 132627, train_loss = 1.241, time/batch = 0.096\n",
      "Sequence 1302068/3263200 (epoch 39), batch 132727, train_loss = 1.047, time/batch = 0.098\n",
      "Sequence 1303058/3263200 (epoch 39), batch 132827, train_loss = 1.218, time/batch = 0.102\n",
      "Sequence 1304028/3263200 (epoch 39), batch 132927, train_loss = 1.386, time/batch = 0.101\n",
      "Sequence 1305018/3263200 (epoch 39), batch 133027, train_loss = 1.813, time/batch = 0.102\n",
      "Epoch 39 completed, average train loss 1.298860, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 1305980/3263200 (epoch 40), batch 133127, train_loss = 0.957, time/batch = 0.101\n",
      "Sequence 1306970/3263200 (epoch 40), batch 133227, train_loss = 1.250, time/batch = 0.097\n",
      "Sequence 1307940/3263200 (epoch 40), batch 133327, train_loss = 1.857, time/batch = 0.098\n",
      "Sequence 1308930/3263200 (epoch 40), batch 133427, train_loss = 1.033, time/batch = 0.095\n",
      "Sequence 1309900/3263200 (epoch 40), batch 133527, train_loss = 1.054, time/batch = 0.097\n",
      "Sequence 1310870/3263200 (epoch 40), batch 133627, train_loss = 0.964, time/batch = 0.095\n",
      "Sequence 1311870/3263200 (epoch 40), batch 133727, train_loss = 1.565, time/batch = 0.096\n",
      "Sequence 1312840/3263200 (epoch 40), batch 133827, train_loss = 1.726, time/batch = 0.097\n",
      "Sequence 1313830/3263200 (epoch 40), batch 133927, train_loss = 1.371, time/batch = 0.097\n",
      "Sequence 1314800/3263200 (epoch 40), batch 134027, train_loss = 1.299, time/batch = 0.099\n",
      "Sequence 1315780/3263200 (epoch 40), batch 134127, train_loss = 0.897, time/batch = 0.097\n",
      "Sequence 1316770/3263200 (epoch 40), batch 134227, train_loss = 1.396, time/batch = 0.097\n",
      "Sequence 1317760/3263200 (epoch 40), batch 134327, train_loss = 1.298, time/batch = 0.098\n",
      "Sequence 1318730/3263200 (epoch 40), batch 134427, train_loss = 1.824, time/batch = 0.099\n",
      "Sequence 1319710/3263200 (epoch 40), batch 134527, train_loss = 1.425, time/batch = 0.098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1320650/3263200 (epoch 40), batch 134627, train_loss = 0.871, time/batch = 0.100\n",
      "Sequence 1321640/3263200 (epoch 40), batch 134727, train_loss = 1.310, time/batch = 0.100\n",
      "Sequence 1322640/3263200 (epoch 40), batch 134827, train_loss = 1.169, time/batch = 0.101\n",
      "Sequence 1323630/3263200 (epoch 40), batch 134927, train_loss = 0.812, time/batch = 0.100\n",
      "Sequence 1324620/3263200 (epoch 40), batch 135027, train_loss = 1.439, time/batch = 0.102\n",
      "Sequence 1325600/3263200 (epoch 40), batch 135127, train_loss = 1.695, time/batch = 0.099\n",
      "Sequence 1326570/3263200 (epoch 40), batch 135227, train_loss = 1.137, time/batch = 0.099\n",
      "Sequence 1327560/3263200 (epoch 40), batch 135327, train_loss = 0.949, time/batch = 0.099\n",
      "Sequence 1328550/3263200 (epoch 40), batch 135427, train_loss = 1.653, time/batch = 0.098\n",
      "Sequence 1329530/3263200 (epoch 40), batch 135527, train_loss = 1.328, time/batch = 0.099\n",
      "Sequence 1330520/3263200 (epoch 40), batch 135627, train_loss = 0.922, time/batch = 0.099\n",
      "Sequence 1331520/3263200 (epoch 40), batch 135727, train_loss = 0.933, time/batch = 0.100\n",
      "Sequence 1332500/3263200 (epoch 40), batch 135827, train_loss = 0.855, time/batch = 0.099\n",
      "Sequence 1333470/3263200 (epoch 40), batch 135927, train_loss = 1.403, time/batch = 0.099\n",
      "Sequence 1334450/3263200 (epoch 40), batch 136027, train_loss = 0.762, time/batch = 0.099\n",
      "Sequence 1335430/3263200 (epoch 40), batch 136127, train_loss = 1.588, time/batch = 0.098\n",
      "Sequence 1336390/3263200 (epoch 40), batch 136227, train_loss = 1.264, time/batch = 0.101\n",
      "Sequence 1337380/3263200 (epoch 40), batch 136327, train_loss = 1.107, time/batch = 0.098\n",
      "Epoch 40 completed, average train loss 1.292667, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1338372/3263200 (epoch 41), batch 136427, train_loss = 1.294, time/batch = 0.100\n",
      "Sequence 1339362/3263200 (epoch 41), batch 136527, train_loss = 0.855, time/batch = 0.099\n",
      "Sequence 1340362/3263200 (epoch 41), batch 136627, train_loss = 1.668, time/batch = 0.098\n",
      "Sequence 1341342/3263200 (epoch 41), batch 136727, train_loss = 1.181, time/batch = 0.101\n",
      "Sequence 1342322/3263200 (epoch 41), batch 136827, train_loss = 2.460, time/batch = 0.099\n",
      "Sequence 1343322/3263200 (epoch 41), batch 136927, train_loss = 1.001, time/batch = 0.100\n",
      "Sequence 1344302/3263200 (epoch 41), batch 137027, train_loss = 1.564, time/batch = 0.099\n",
      "Sequence 1345262/3263200 (epoch 41), batch 137127, train_loss = 1.127, time/batch = 0.098\n",
      "Sequence 1346242/3263200 (epoch 41), batch 137227, train_loss = 0.988, time/batch = 0.097\n",
      "Sequence 1347242/3263200 (epoch 41), batch 137327, train_loss = 1.382, time/batch = 0.101\n",
      "Sequence 1348222/3263200 (epoch 41), batch 137427, train_loss = 1.324, time/batch = 0.096\n",
      "Sequence 1349212/3263200 (epoch 41), batch 137527, train_loss = 1.081, time/batch = 0.099\n",
      "Sequence 1350182/3263200 (epoch 41), batch 137627, train_loss = 0.857, time/batch = 0.097\n",
      "Sequence 1351142/3263200 (epoch 41), batch 137727, train_loss = 1.674, time/batch = 0.096\n",
      "Sequence 1352102/3263200 (epoch 41), batch 137827, train_loss = 1.277, time/batch = 0.100\n",
      "Sequence 1353082/3263200 (epoch 41), batch 137927, train_loss = 0.843, time/batch = 0.097\n",
      "Sequence 1354042/3263200 (epoch 41), batch 138027, train_loss = 1.806, time/batch = 0.099\n",
      "Sequence 1355042/3263200 (epoch 41), batch 138127, train_loss = 1.216, time/batch = 0.097\n",
      "Sequence 1356022/3263200 (epoch 41), batch 138227, train_loss = 1.169, time/batch = 0.097\n",
      "Sequence 1356982/3263200 (epoch 41), batch 138327, train_loss = 1.109, time/batch = 0.097\n",
      "Sequence 1357962/3263200 (epoch 41), batch 138427, train_loss = 1.531, time/batch = 0.098\n",
      "Sequence 1358952/3263200 (epoch 41), batch 138527, train_loss = 2.326, time/batch = 0.099\n",
      "Sequence 1359942/3263200 (epoch 41), batch 138627, train_loss = 1.439, time/batch = 0.098\n",
      "Sequence 1360922/3263200 (epoch 41), batch 138727, train_loss = 2.374, time/batch = 0.097\n",
      "Sequence 1361892/3263200 (epoch 41), batch 138827, train_loss = 1.187, time/batch = 0.096\n",
      "Sequence 1362882/3263200 (epoch 41), batch 138927, train_loss = 0.952, time/batch = 0.096\n",
      "Sequence 1363862/3263200 (epoch 41), batch 139027, train_loss = 1.980, time/batch = 0.097\n",
      "Sequence 1364842/3263200 (epoch 41), batch 139127, train_loss = 1.990, time/batch = 0.097\n",
      "Sequence 1365822/3263200 (epoch 41), batch 139227, train_loss = 1.980, time/batch = 0.098\n",
      "Sequence 1366822/3263200 (epoch 41), batch 139327, train_loss = 1.913, time/batch = 0.097\n",
      "Sequence 1367792/3263200 (epoch 41), batch 139427, train_loss = 1.375, time/batch = 0.100\n",
      "Sequence 1368762/3263200 (epoch 41), batch 139527, train_loss = 2.531, time/batch = 0.097\n",
      "Sequence 1369742/3263200 (epoch 41), batch 139627, train_loss = 1.669, time/batch = 0.096\n",
      "Epoch 41 completed, average train loss 1.285932, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1370734/3263200 (epoch 42), batch 139727, train_loss = 1.469, time/batch = 0.097\n",
      "Sequence 1371714/3263200 (epoch 42), batch 139827, train_loss = 1.050, time/batch = 0.097\n",
      "Sequence 1372714/3263200 (epoch 42), batch 139927, train_loss = 0.961, time/batch = 0.098\n",
      "Sequence 1373684/3263200 (epoch 42), batch 140027, train_loss = 1.279, time/batch = 0.098\n",
      "Sequence 1374664/3263200 (epoch 42), batch 140127, train_loss = 1.475, time/batch = 0.098\n",
      "Sequence 1375624/3263200 (epoch 42), batch 140227, train_loss = 1.065, time/batch = 0.097\n",
      "Sequence 1376604/3263200 (epoch 42), batch 140327, train_loss = 1.488, time/batch = 0.099\n",
      "Sequence 1377594/3263200 (epoch 42), batch 140427, train_loss = 0.948, time/batch = 0.099\n",
      "Sequence 1378584/3263200 (epoch 42), batch 140527, train_loss = 0.971, time/batch = 0.100\n",
      "Sequence 1379564/3263200 (epoch 42), batch 140627, train_loss = 1.281, time/batch = 0.098\n",
      "Sequence 1380544/3263200 (epoch 42), batch 140727, train_loss = 1.090, time/batch = 0.102\n",
      "Sequence 1381514/3263200 (epoch 42), batch 140827, train_loss = 1.089, time/batch = 0.102\n",
      "Sequence 1382494/3263200 (epoch 42), batch 140927, train_loss = 0.844, time/batch = 0.101\n",
      "Sequence 1383474/3263200 (epoch 42), batch 141027, train_loss = 1.341, time/batch = 0.100\n",
      "Sequence 1384464/3263200 (epoch 42), batch 141127, train_loss = 1.558, time/batch = 0.101\n",
      "Sequence 1385444/3263200 (epoch 42), batch 141227, train_loss = 1.217, time/batch = 0.099\n",
      "Sequence 1386424/3263200 (epoch 42), batch 141327, train_loss = 1.473, time/batch = 0.097\n",
      "Sequence 1387374/3263200 (epoch 42), batch 141427, train_loss = 1.146, time/batch = 0.098\n",
      "Sequence 1388324/3263200 (epoch 42), batch 141527, train_loss = 3.153, time/batch = 0.097\n",
      "Sequence 1389314/3263200 (epoch 42), batch 141627, train_loss = 1.478, time/batch = 0.097\n",
      "Sequence 1390274/3263200 (epoch 42), batch 141727, train_loss = 1.256, time/batch = 0.098\n",
      "Sequence 1391274/3263200 (epoch 42), batch 141827, train_loss = 0.942, time/batch = 0.099\n",
      "Sequence 1392274/3263200 (epoch 42), batch 141927, train_loss = 1.237, time/batch = 0.097\n",
      "Sequence 1393274/3263200 (epoch 42), batch 142027, train_loss = 0.962, time/batch = 0.097\n",
      "Sequence 1394244/3263200 (epoch 42), batch 142127, train_loss = 1.071, time/batch = 0.099\n",
      "Sequence 1395234/3263200 (epoch 42), batch 142227, train_loss = 1.222, time/batch = 0.098\n",
      "Sequence 1396194/3263200 (epoch 42), batch 142327, train_loss = 1.380, time/batch = 0.100\n",
      "Sequence 1397184/3263200 (epoch 42), batch 142427, train_loss = 1.025, time/batch = 0.100\n",
      "Sequence 1398164/3263200 (epoch 42), batch 142527, train_loss = 0.723, time/batch = 0.099\n",
      "Sequence 1399144/3263200 (epoch 42), batch 142627, train_loss = 1.922, time/batch = 0.095\n",
      "Sequence 1400134/3263200 (epoch 42), batch 142727, train_loss = 1.577, time/batch = 0.097\n",
      "Sequence 1401114/3263200 (epoch 42), batch 142827, train_loss = 1.313, time/batch = 0.097\n",
      "Sequence 1402104/3263200 (epoch 42), batch 142927, train_loss = 0.972, time/batch = 0.098\n",
      "Sequence 1403094/3263200 (epoch 42), batch 143027, train_loss = 1.222, time/batch = 0.098\n",
      "Epoch 42 completed, average train loss 1.282879, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1404056/3263200 (epoch 43), batch 143127, train_loss = 1.277, time/batch = 0.095\n",
      "Sequence 1405056/3263200 (epoch 43), batch 143227, train_loss = 1.127, time/batch = 0.097\n",
      "Sequence 1406036/3263200 (epoch 43), batch 143327, train_loss = 1.273, time/batch = 0.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1407006/3263200 (epoch 43), batch 143427, train_loss = 1.560, time/batch = 0.098\n",
      "Sequence 1408006/3263200 (epoch 43), batch 143527, train_loss = 1.066, time/batch = 0.099\n",
      "Sequence 1408976/3263200 (epoch 43), batch 143627, train_loss = 1.203, time/batch = 0.101\n",
      "Sequence 1409956/3263200 (epoch 43), batch 143727, train_loss = 1.000, time/batch = 0.102\n",
      "Sequence 1410916/3263200 (epoch 43), batch 143827, train_loss = 1.428, time/batch = 0.100\n",
      "Sequence 1411896/3263200 (epoch 43), batch 143927, train_loss = 1.653, time/batch = 0.098\n",
      "Sequence 1412896/3263200 (epoch 43), batch 144027, train_loss = 1.073, time/batch = 0.098\n",
      "Sequence 1413866/3263200 (epoch 43), batch 144127, train_loss = 1.082, time/batch = 0.099\n",
      "Sequence 1414846/3263200 (epoch 43), batch 144227, train_loss = 0.887, time/batch = 0.098\n",
      "Sequence 1415826/3263200 (epoch 43), batch 144327, train_loss = 1.381, time/batch = 0.100\n",
      "Sequence 1416816/3263200 (epoch 43), batch 144427, train_loss = 1.157, time/batch = 0.100\n",
      "Sequence 1417816/3263200 (epoch 43), batch 144527, train_loss = 1.241, time/batch = 0.098\n",
      "Sequence 1418796/3263200 (epoch 43), batch 144627, train_loss = 0.884, time/batch = 0.098\n",
      "Sequence 1419786/3263200 (epoch 43), batch 144727, train_loss = 1.482, time/batch = 0.099\n",
      "Sequence 1420766/3263200 (epoch 43), batch 144827, train_loss = 1.149, time/batch = 0.101\n",
      "Sequence 1421716/3263200 (epoch 43), batch 144927, train_loss = 2.092, time/batch = 0.101\n",
      "Sequence 1422706/3263200 (epoch 43), batch 145027, train_loss = 1.275, time/batch = 0.100\n",
      "Sequence 1423666/3263200 (epoch 43), batch 145127, train_loss = 0.989, time/batch = 0.099\n",
      "Sequence 1424656/3263200 (epoch 43), batch 145227, train_loss = 1.046, time/batch = 0.099\n",
      "Sequence 1425626/3263200 (epoch 43), batch 145327, train_loss = 1.019, time/batch = 0.101\n",
      "Sequence 1426616/3263200 (epoch 43), batch 145427, train_loss = 1.026, time/batch = 0.102\n",
      "Sequence 1427596/3263200 (epoch 43), batch 145527, train_loss = 1.088, time/batch = 0.100\n",
      "Sequence 1428586/3263200 (epoch 43), batch 145627, train_loss = 1.856, time/batch = 0.097\n",
      "Sequence 1429566/3263200 (epoch 43), batch 145727, train_loss = 1.081, time/batch = 0.098\n",
      "Sequence 1430556/3263200 (epoch 43), batch 145827, train_loss = 1.812, time/batch = 0.099\n",
      "Sequence 1431556/3263200 (epoch 43), batch 145927, train_loss = 1.165, time/batch = 0.100\n",
      "Sequence 1432526/3263200 (epoch 43), batch 146027, train_loss = 1.300, time/batch = 0.100\n",
      "Sequence 1433516/3263200 (epoch 43), batch 146127, train_loss = 1.066, time/batch = 0.100\n",
      "Sequence 1434506/3263200 (epoch 43), batch 146227, train_loss = 1.178, time/batch = 0.099\n",
      "Sequence 1435466/3263200 (epoch 43), batch 146327, train_loss = 1.073, time/batch = 0.098\n",
      "Epoch 43 completed, average train loss 1.277358, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1436438/3263200 (epoch 44), batch 146427, train_loss = 1.318, time/batch = 0.101\n",
      "Sequence 1437428/3263200 (epoch 44), batch 146527, train_loss = 0.959, time/batch = 0.103\n",
      "Sequence 1438408/3263200 (epoch 44), batch 146627, train_loss = 1.375, time/batch = 0.098\n",
      "Sequence 1439388/3263200 (epoch 44), batch 146727, train_loss = 1.476, time/batch = 0.100\n",
      "Sequence 1440378/3263200 (epoch 44), batch 146827, train_loss = 1.702, time/batch = 0.097\n",
      "Sequence 1441368/3263200 (epoch 44), batch 146927, train_loss = 1.245, time/batch = 0.098\n",
      "Sequence 1442358/3263200 (epoch 44), batch 147027, train_loss = 0.899, time/batch = 0.097\n",
      "Sequence 1443318/3263200 (epoch 44), batch 147127, train_loss = 2.288, time/batch = 0.097\n",
      "Sequence 1444288/3263200 (epoch 44), batch 147227, train_loss = 1.573, time/batch = 0.098\n",
      "Sequence 1445288/3263200 (epoch 44), batch 147327, train_loss = 1.192, time/batch = 0.099\n",
      "Sequence 1446248/3263200 (epoch 44), batch 147427, train_loss = 1.571, time/batch = 0.102\n",
      "Sequence 1447238/3263200 (epoch 44), batch 147527, train_loss = 1.005, time/batch = 0.100\n",
      "Sequence 1448208/3263200 (epoch 44), batch 147627, train_loss = 1.965, time/batch = 0.099\n",
      "Sequence 1449198/3263200 (epoch 44), batch 147727, train_loss = 1.153, time/batch = 0.097\n",
      "Sequence 1450178/3263200 (epoch 44), batch 147828, train_loss = 0.554, time/batch = 0.098\n",
      "Sequence 1451148/3263200 (epoch 44), batch 147928, train_loss = 1.526, time/batch = 0.095\n",
      "Sequence 1452128/3263200 (epoch 44), batch 148028, train_loss = 0.805, time/batch = 0.097\n",
      "Sequence 1453088/3263200 (epoch 44), batch 148128, train_loss = 1.457, time/batch = 0.096\n",
      "Sequence 1454088/3263200 (epoch 44), batch 148228, train_loss = 1.790, time/batch = 0.097\n",
      "Sequence 1455068/3263200 (epoch 44), batch 148328, train_loss = 1.424, time/batch = 0.099\n",
      "Sequence 1456068/3263200 (epoch 44), batch 148428, train_loss = 1.097, time/batch = 0.096\n",
      "Sequence 1457058/3263200 (epoch 44), batch 148528, train_loss = 0.998, time/batch = 0.099\n",
      "Sequence 1458018/3263200 (epoch 44), batch 148628, train_loss = 1.332, time/batch = 0.098\n",
      "Sequence 1459008/3263200 (epoch 44), batch 148728, train_loss = 0.711, time/batch = 0.097\n",
      "Sequence 1459988/3263200 (epoch 44), batch 148828, train_loss = 1.258, time/batch = 0.100\n",
      "Sequence 1460948/3263200 (epoch 44), batch 148928, train_loss = 1.635, time/batch = 0.096\n",
      "Sequence 1461928/3263200 (epoch 44), batch 149028, train_loss = 1.134, time/batch = 0.097\n",
      "Sequence 1462918/3263200 (epoch 44), batch 149128, train_loss = 0.939, time/batch = 0.099\n",
      "Sequence 1463898/3263200 (epoch 44), batch 149228, train_loss = 1.445, time/batch = 0.097\n",
      "Sequence 1464878/3263200 (epoch 44), batch 149328, train_loss = 1.141, time/batch = 0.098\n",
      "Sequence 1465858/3263200 (epoch 44), batch 149428, train_loss = 0.500, time/batch = 0.098\n",
      "Sequence 1466848/3263200 (epoch 44), batch 149528, train_loss = 2.201, time/batch = 0.098\n",
      "Sequence 1467838/3263200 (epoch 44), batch 149628, train_loss = 0.933, time/batch = 0.098\n",
      "Epoch 44 completed, average train loss 1.273335, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 1468820/3263200 (epoch 45), batch 149728, train_loss = 1.018, time/batch = 0.101\n",
      "Sequence 1469810/3263200 (epoch 45), batch 149828, train_loss = 1.208, time/batch = 0.102\n",
      "Sequence 1470760/3263200 (epoch 45), batch 149928, train_loss = 0.837, time/batch = 0.099\n",
      "Sequence 1471720/3263200 (epoch 45), batch 150028, train_loss = 1.504, time/batch = 0.100\n",
      "Sequence 1472710/3263200 (epoch 45), batch 150128, train_loss = 1.083, time/batch = 0.102\n",
      "Sequence 1473700/3263200 (epoch 45), batch 150228, train_loss = 1.746, time/batch = 0.100\n",
      "Sequence 1474680/3263200 (epoch 45), batch 150328, train_loss = 1.080, time/batch = 0.099\n",
      "Sequence 1475680/3263200 (epoch 45), batch 150428, train_loss = 0.790, time/batch = 0.101\n",
      "Sequence 1476670/3263200 (epoch 45), batch 150528, train_loss = 1.764, time/batch = 0.100\n",
      "Sequence 1477650/3263200 (epoch 45), batch 150628, train_loss = 0.970, time/batch = 0.099\n",
      "Sequence 1478630/3263200 (epoch 45), batch 150728, train_loss = 1.088, time/batch = 0.099\n",
      "Sequence 1479620/3263200 (epoch 45), batch 150828, train_loss = 1.352, time/batch = 0.099\n",
      "Sequence 1480590/3263200 (epoch 45), batch 150929, train_loss = 0.632, time/batch = 0.100\n",
      "Sequence 1481560/3263200 (epoch 45), batch 151029, train_loss = 1.629, time/batch = 0.099\n",
      "Sequence 1482530/3263200 (epoch 45), batch 151129, train_loss = 1.549, time/batch = 0.098\n",
      "Sequence 1483510/3263200 (epoch 45), batch 151229, train_loss = 0.967, time/batch = 0.098\n",
      "Sequence 1484490/3263200 (epoch 45), batch 151329, train_loss = 0.905, time/batch = 0.096\n",
      "Sequence 1485450/3263200 (epoch 45), batch 151429, train_loss = 1.698, time/batch = 0.098\n",
      "Sequence 1486420/3263200 (epoch 45), batch 151529, train_loss = 1.298, time/batch = 0.100\n",
      "Sequence 1487420/3263200 (epoch 45), batch 151629, train_loss = 0.845, time/batch = 0.099\n",
      "Sequence 1488410/3263200 (epoch 45), batch 151729, train_loss = 1.400, time/batch = 0.096\n",
      "Sequence 1489400/3263200 (epoch 45), batch 151829, train_loss = 1.668, time/batch = 0.096\n",
      "Sequence 1490390/3263200 (epoch 45), batch 151929, train_loss = 1.213, time/batch = 0.097\n",
      "Sequence 1491380/3263200 (epoch 45), batch 152029, train_loss = 1.378, time/batch = 0.096\n",
      "Sequence 1492380/3263200 (epoch 45), batch 152129, train_loss = 1.846, time/batch = 0.098\n",
      "Sequence 1493360/3263200 (epoch 45), batch 152229, train_loss = 2.377, time/batch = 0.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1494350/3263200 (epoch 45), batch 152329, train_loss = 1.209, time/batch = 0.099\n",
      "Sequence 1495340/3263200 (epoch 45), batch 152429, train_loss = 1.108, time/batch = 0.099\n",
      "Sequence 1496340/3263200 (epoch 45), batch 152529, train_loss = 1.022, time/batch = 0.101\n",
      "Sequence 1497320/3263200 (epoch 45), batch 152629, train_loss = 1.302, time/batch = 0.099\n",
      "Sequence 1498310/3263200 (epoch 45), batch 152729, train_loss = 1.245, time/batch = 0.100\n",
      "Sequence 1499270/3263200 (epoch 45), batch 152829, train_loss = 0.915, time/batch = 0.098\n",
      "Sequence 1500250/3263200 (epoch 45), batch 152929, train_loss = 1.263, time/batch = 0.097\n",
      "Epoch 45 completed, average train loss 1.267943, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1501212/3263200 (epoch 46), batch 153029, train_loss = 1.283, time/batch = 0.099\n",
      "Sequence 1502192/3263200 (epoch 46), batch 153129, train_loss = 1.233, time/batch = 0.097\n",
      "Sequence 1503182/3263200 (epoch 46), batch 153229, train_loss = 1.184, time/batch = 0.098\n",
      "Sequence 1504142/3263200 (epoch 46), batch 153329, train_loss = 1.300, time/batch = 0.099\n",
      "Sequence 1505142/3263200 (epoch 46), batch 153429, train_loss = 0.746, time/batch = 0.098\n",
      "Sequence 1506132/3263200 (epoch 46), batch 153529, train_loss = 1.700, time/batch = 0.098\n",
      "Sequence 1507122/3263200 (epoch 46), batch 153629, train_loss = 1.098, time/batch = 0.097\n",
      "Sequence 1508122/3263200 (epoch 46), batch 153729, train_loss = 1.358, time/batch = 0.096\n",
      "Sequence 1509112/3263200 (epoch 46), batch 153829, train_loss = 1.407, time/batch = 0.096\n",
      "Sequence 1510102/3263200 (epoch 46), batch 153929, train_loss = 0.861, time/batch = 0.097\n",
      "Sequence 1511082/3263200 (epoch 46), batch 154029, train_loss = 1.644, time/batch = 0.097\n",
      "Sequence 1512072/3263200 (epoch 46), batch 154129, train_loss = 1.199, time/batch = 0.097\n",
      "Sequence 1513052/3263200 (epoch 46), batch 154229, train_loss = 1.651, time/batch = 0.097\n",
      "Sequence 1514042/3263200 (epoch 46), batch 154329, train_loss = 0.918, time/batch = 0.098\n",
      "Sequence 1515022/3263200 (epoch 46), batch 154429, train_loss = 1.182, time/batch = 0.096\n",
      "Sequence 1516012/3263200 (epoch 46), batch 154529, train_loss = 1.176, time/batch = 0.097\n",
      "Sequence 1516982/3263200 (epoch 46), batch 154629, train_loss = 0.813, time/batch = 0.099\n",
      "Sequence 1517952/3263200 (epoch 46), batch 154729, train_loss = 1.588, time/batch = 0.097\n",
      "Sequence 1518942/3263200 (epoch 46), batch 154829, train_loss = 0.859, time/batch = 0.096\n",
      "Sequence 1519912/3263200 (epoch 46), batch 154929, train_loss = 1.050, time/batch = 0.097\n",
      "Sequence 1520892/3263200 (epoch 46), batch 155029, train_loss = 2.183, time/batch = 0.097\n",
      "Sequence 1521882/3263200 (epoch 46), batch 155129, train_loss = 1.396, time/batch = 0.098\n",
      "Sequence 1522822/3263200 (epoch 46), batch 155229, train_loss = 1.811, time/batch = 0.097\n",
      "Sequence 1523802/3263200 (epoch 46), batch 155329, train_loss = 0.676, time/batch = 0.102\n",
      "Sequence 1524782/3263200 (epoch 46), batch 155429, train_loss = 0.983, time/batch = 0.099\n",
      "Sequence 1525752/3263200 (epoch 46), batch 155529, train_loss = 1.217, time/batch = 0.098\n",
      "Sequence 1526742/3263200 (epoch 46), batch 155629, train_loss = 0.950, time/batch = 0.098\n",
      "Sequence 1527702/3263200 (epoch 46), batch 155729, train_loss = 1.370, time/batch = 0.099\n",
      "Sequence 1528682/3263200 (epoch 46), batch 155829, train_loss = 1.116, time/batch = 0.099\n",
      "Sequence 1529682/3263200 (epoch 46), batch 155929, train_loss = 1.404, time/batch = 0.099\n",
      "Sequence 1530652/3263200 (epoch 46), batch 156029, train_loss = 1.583, time/batch = 0.100\n",
      "Sequence 1531622/3263200 (epoch 46), batch 156129, train_loss = 0.827, time/batch = 0.099\n",
      "Sequence 1532592/3263200 (epoch 46), batch 156229, train_loss = 1.662, time/batch = 0.098\n",
      "Sequence 1533572/3263200 (epoch 46), batch 156329, train_loss = 1.274, time/batch = 0.098\n",
      "Epoch 46 completed, average train loss 1.266518, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1534554/3263200 (epoch 47), batch 156429, train_loss = 1.251, time/batch = 0.101\n",
      "Sequence 1535544/3263200 (epoch 47), batch 156529, train_loss = 1.665, time/batch = 0.098\n",
      "Sequence 1536524/3263200 (epoch 47), batch 156629, train_loss = 1.194, time/batch = 0.098\n",
      "Sequence 1537514/3263200 (epoch 47), batch 156729, train_loss = 1.190, time/batch = 0.099\n",
      "Sequence 1538484/3263200 (epoch 47), batch 156829, train_loss = 1.278, time/batch = 0.097\n",
      "Sequence 1539484/3263200 (epoch 47), batch 156929, train_loss = 1.957, time/batch = 0.096\n",
      "Sequence 1540484/3263200 (epoch 47), batch 157029, train_loss = 1.321, time/batch = 0.096\n",
      "Sequence 1541444/3263200 (epoch 47), batch 157129, train_loss = 1.460, time/batch = 0.107\n",
      "Sequence 1542414/3263200 (epoch 47), batch 157229, train_loss = 1.382, time/batch = 0.116\n",
      "Sequence 1543394/3263200 (epoch 47), batch 157329, train_loss = 1.215, time/batch = 0.114\n",
      "Sequence 1544384/3263200 (epoch 47), batch 157429, train_loss = 1.128, time/batch = 0.115\n",
      "Sequence 1545374/3263200 (epoch 47), batch 157529, train_loss = 0.958, time/batch = 0.116\n",
      "Sequence 1546354/3263200 (epoch 47), batch 157629, train_loss = 1.386, time/batch = 0.115\n",
      "Sequence 1547324/3263200 (epoch 47), batch 157729, train_loss = 0.794, time/batch = 0.115\n",
      "Sequence 1548304/3263200 (epoch 47), batch 157829, train_loss = 1.725, time/batch = 0.115\n",
      "Sequence 1549284/3263200 (epoch 47), batch 157929, train_loss = 1.526, time/batch = 0.115\n",
      "Sequence 1550264/3263200 (epoch 47), batch 158029, train_loss = 1.439, time/batch = 0.115\n",
      "Sequence 1551254/3263200 (epoch 47), batch 158129, train_loss = 1.064, time/batch = 0.115\n",
      "Sequence 1552214/3263200 (epoch 47), batch 158229, train_loss = 1.277, time/batch = 0.114\n",
      "Sequence 1553184/3263200 (epoch 47), batch 158329, train_loss = 0.812, time/batch = 0.115\n",
      "Sequence 1554164/3263200 (epoch 47), batch 158429, train_loss = 1.804, time/batch = 0.114\n",
      "Sequence 1555154/3263200 (epoch 47), batch 158529, train_loss = 0.924, time/batch = 0.115\n",
      "Sequence 1556144/3263200 (epoch 47), batch 158629, train_loss = 0.995, time/batch = 0.113\n",
      "Sequence 1557144/3263200 (epoch 47), batch 158729, train_loss = 1.190, time/batch = 0.118\n",
      "Sequence 1558104/3263200 (epoch 47), batch 158829, train_loss = 1.092, time/batch = 0.113\n",
      "Sequence 1559074/3263200 (epoch 47), batch 158929, train_loss = 1.576, time/batch = 0.114\n",
      "Sequence 1560064/3263200 (epoch 47), batch 159029, train_loss = 2.100, time/batch = 0.117\n",
      "Sequence 1561044/3263200 (epoch 47), batch 159129, train_loss = 1.100, time/batch = 0.116\n",
      "Sequence 1562034/3263200 (epoch 47), batch 159229, train_loss = 0.979, time/batch = 0.113\n",
      "Sequence 1563004/3263200 (epoch 47), batch 159329, train_loss = 0.817, time/batch = 0.115\n",
      "Sequence 1563984/3263200 (epoch 47), batch 159429, train_loss = 1.006, time/batch = 0.114\n",
      "Sequence 1564954/3263200 (epoch 47), batch 159529, train_loss = 1.331, time/batch = 0.114\n",
      "Sequence 1565934/3263200 (epoch 47), batch 159629, train_loss = 1.554, time/batch = 0.114\n",
      "Epoch 47 completed, average train loss 1.261175, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1566926/3263200 (epoch 48), batch 159729, train_loss = 1.767, time/batch = 0.113\n",
      "Sequence 1567886/3263200 (epoch 48), batch 159829, train_loss = 1.317, time/batch = 0.113\n",
      "Sequence 1568866/3263200 (epoch 48), batch 159929, train_loss = 1.130, time/batch = 0.115\n",
      "Sequence 1569866/3263200 (epoch 48), batch 160029, train_loss = 2.314, time/batch = 0.115\n",
      "Sequence 1570836/3263200 (epoch 48), batch 160129, train_loss = 1.465, time/batch = 0.116\n",
      "Sequence 1571816/3263200 (epoch 48), batch 160229, train_loss = 1.465, time/batch = 0.115\n",
      "Sequence 1572806/3263200 (epoch 48), batch 160329, train_loss = 0.959, time/batch = 0.114\n",
      "Sequence 1573796/3263200 (epoch 48), batch 160429, train_loss = 0.813, time/batch = 0.115\n",
      "Sequence 1574786/3263200 (epoch 48), batch 160529, train_loss = 1.487, time/batch = 0.113\n",
      "Sequence 1575746/3263200 (epoch 48), batch 160629, train_loss = 2.180, time/batch = 0.116\n",
      "Sequence 1576726/3263200 (epoch 48), batch 160729, train_loss = 1.216, time/batch = 0.117\n",
      "Sequence 1577706/3263200 (epoch 48), batch 160829, train_loss = 1.069, time/batch = 0.113\n",
      "Sequence 1578706/3263200 (epoch 48), batch 160929, train_loss = 0.999, time/batch = 0.114\n",
      "Sequence 1579696/3263200 (epoch 48), batch 161029, train_loss = 1.340, time/batch = 0.114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1580676/3263200 (epoch 48), batch 161129, train_loss = 0.996, time/batch = 0.115\n",
      "Sequence 1581666/3263200 (epoch 48), batch 161229, train_loss = 1.627, time/batch = 0.114\n",
      "Sequence 1582646/3263200 (epoch 48), batch 161329, train_loss = 1.115, time/batch = 0.112\n",
      "Sequence 1583646/3263200 (epoch 48), batch 161429, train_loss = 1.555, time/batch = 0.115\n",
      "Sequence 1584626/3263200 (epoch 48), batch 161529, train_loss = 1.298, time/batch = 0.117\n",
      "Sequence 1585616/3263200 (epoch 48), batch 161629, train_loss = 1.146, time/batch = 0.114\n",
      "Sequence 1586606/3263200 (epoch 48), batch 161729, train_loss = 1.199, time/batch = 0.117\n",
      "Sequence 1587586/3263200 (epoch 48), batch 161829, train_loss = 1.591, time/batch = 0.117\n",
      "Sequence 1588556/3263200 (epoch 48), batch 161929, train_loss = 1.029, time/batch = 0.117\n",
      "Sequence 1589546/3263200 (epoch 48), batch 162029, train_loss = 1.227, time/batch = 0.113\n",
      "Sequence 1590516/3263200 (epoch 48), batch 162129, train_loss = 1.223, time/batch = 0.113\n",
      "Sequence 1591486/3263200 (epoch 48), batch 162229, train_loss = 1.286, time/batch = 0.114\n",
      "Sequence 1592446/3263200 (epoch 48), batch 162329, train_loss = 1.535, time/batch = 0.116\n",
      "Sequence 1593426/3263200 (epoch 48), batch 162429, train_loss = 1.075, time/batch = 0.116\n",
      "Sequence 1594426/3263200 (epoch 48), batch 162529, train_loss = 1.365, time/batch = 0.114\n",
      "Sequence 1595406/3263200 (epoch 48), batch 162630, train_loss = 1.080, time/batch = 0.114\n",
      "Sequence 1596376/3263200 (epoch 48), batch 162730, train_loss = 0.940, time/batch = 0.116\n",
      "Sequence 1597346/3263200 (epoch 48), batch 162830, train_loss = 1.073, time/batch = 0.116\n",
      "Sequence 1598326/3263200 (epoch 48), batch 162930, train_loss = 1.822, time/batch = 0.114\n",
      "Epoch 48 completed, average train loss 1.256536, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1599318/3263200 (epoch 49), batch 163030, train_loss = 1.133, time/batch = 0.116\n",
      "Sequence 1600308/3263200 (epoch 49), batch 163130, train_loss = 1.089, time/batch = 0.117\n",
      "Sequence 1601288/3263200 (epoch 49), batch 163230, train_loss = 1.235, time/batch = 0.116\n",
      "Sequence 1602238/3263200 (epoch 49), batch 163330, train_loss = 0.913, time/batch = 0.115\n",
      "Sequence 1603228/3263200 (epoch 49), batch 163430, train_loss = 1.402, time/batch = 0.116\n",
      "Sequence 1604198/3263200 (epoch 49), batch 163530, train_loss = 1.227, time/batch = 0.114\n",
      "Sequence 1605168/3263200 (epoch 49), batch 163630, train_loss = 1.122, time/batch = 0.114\n",
      "Sequence 1606148/3263200 (epoch 49), batch 163730, train_loss = 1.324, time/batch = 0.115\n",
      "Sequence 1607118/3263200 (epoch 49), batch 163830, train_loss = 0.989, time/batch = 0.116\n",
      "Sequence 1608108/3263200 (epoch 49), batch 163930, train_loss = 1.270, time/batch = 0.115\n",
      "Sequence 1609078/3263200 (epoch 49), batch 164030, train_loss = 1.021, time/batch = 0.113\n",
      "Sequence 1610068/3263200 (epoch 49), batch 164130, train_loss = 0.806, time/batch = 0.114\n",
      "Sequence 1611038/3263200 (epoch 49), batch 164230, train_loss = 0.676, time/batch = 0.114\n",
      "Sequence 1612028/3263200 (epoch 49), batch 164330, train_loss = 1.042, time/batch = 0.115\n",
      "Sequence 1613008/3263200 (epoch 49), batch 164430, train_loss = 1.053, time/batch = 0.115\n",
      "Sequence 1613998/3263200 (epoch 49), batch 164530, train_loss = 1.530, time/batch = 0.115\n",
      "Sequence 1614988/3263200 (epoch 49), batch 164630, train_loss = 1.321, time/batch = 0.115\n",
      "Sequence 1615978/3263200 (epoch 49), batch 164730, train_loss = 1.200, time/batch = 0.114\n",
      "Sequence 1616968/3263200 (epoch 49), batch 164830, train_loss = 1.480, time/batch = 0.114\n",
      "Sequence 1617938/3263200 (epoch 49), batch 164930, train_loss = 1.190, time/batch = 0.114\n",
      "Sequence 1618908/3263200 (epoch 49), batch 165030, train_loss = 1.142, time/batch = 0.116\n",
      "Sequence 1619888/3263200 (epoch 49), batch 165130, train_loss = 0.892, time/batch = 0.113\n",
      "Sequence 1620858/3263200 (epoch 49), batch 165230, train_loss = 1.306, time/batch = 0.115\n",
      "Sequence 1621838/3263200 (epoch 49), batch 165330, train_loss = 1.222, time/batch = 0.113\n",
      "Sequence 1622808/3263200 (epoch 49), batch 165430, train_loss = 0.870, time/batch = 0.115\n",
      "Sequence 1623798/3263200 (epoch 49), batch 165530, train_loss = 1.269, time/batch = 0.115\n",
      "Sequence 1624778/3263200 (epoch 49), batch 165630, train_loss = 1.233, time/batch = 0.114\n",
      "Sequence 1625758/3263200 (epoch 49), batch 165730, train_loss = 0.992, time/batch = 0.115\n",
      "Sequence 1626748/3263200 (epoch 49), batch 165830, train_loss = 1.357, time/batch = 0.116\n",
      "Sequence 1627718/3263200 (epoch 49), batch 165930, train_loss = 0.880, time/batch = 0.114\n",
      "Sequence 1628718/3263200 (epoch 49), batch 166030, train_loss = 1.472, time/batch = 0.114\n",
      "Sequence 1629708/3263200 (epoch 49), batch 166130, train_loss = 2.459, time/batch = 0.114\n",
      "Sequence 1630698/3263200 (epoch 49), batch 166230, train_loss = 1.577, time/batch = 0.114\n",
      "Epoch 49 completed, average train loss 1.253284, learning rate 0.0010\n",
      "model saved.\n",
      "Sequence 1631680/3263200 (epoch 50), batch 166330, train_loss = 1.206, time/batch = 0.119\n",
      "Shuffling training data...\n",
      "Sequence 1632670/3263200 (epoch 50), batch 166430, train_loss = 1.252, time/batch = 0.117\n",
      "Sequence 1633670/3263200 (epoch 50), batch 166530, train_loss = 1.467, time/batch = 0.117\n",
      "Sequence 1634660/3263200 (epoch 50), batch 166630, train_loss = 1.694, time/batch = 0.116\n",
      "Sequence 1635630/3263200 (epoch 50), batch 166730, train_loss = 0.899, time/batch = 0.116\n",
      "Sequence 1636610/3263200 (epoch 50), batch 166830, train_loss = 1.240, time/batch = 0.116\n",
      "Sequence 1637590/3263200 (epoch 50), batch 166930, train_loss = 1.546, time/batch = 0.115\n",
      "Sequence 1638580/3263200 (epoch 50), batch 167030, train_loss = 1.579, time/batch = 0.113\n",
      "Sequence 1639570/3263200 (epoch 50), batch 167130, train_loss = 1.082, time/batch = 0.114\n",
      "Sequence 1640560/3263200 (epoch 50), batch 167230, train_loss = 1.132, time/batch = 0.114\n",
      "Sequence 1641550/3263200 (epoch 50), batch 167330, train_loss = 1.400, time/batch = 0.111\n",
      "Sequence 1642520/3263200 (epoch 50), batch 167430, train_loss = 1.451, time/batch = 0.113\n",
      "Sequence 1643480/3263200 (epoch 50), batch 167530, train_loss = 1.274, time/batch = 0.114\n",
      "Sequence 1644460/3263200 (epoch 50), batch 167630, train_loss = 1.103, time/batch = 0.112\n",
      "Sequence 1645440/3263200 (epoch 50), batch 167730, train_loss = 1.107, time/batch = 0.114\n",
      "Sequence 1646430/3263200 (epoch 50), batch 167830, train_loss = 1.328, time/batch = 0.115\n",
      "Sequence 1647410/3263200 (epoch 50), batch 167930, train_loss = 1.701, time/batch = 0.116\n",
      "Sequence 1648370/3263200 (epoch 50), batch 168030, train_loss = 1.534, time/batch = 0.116\n",
      "Sequence 1649350/3263200 (epoch 50), batch 168130, train_loss = 1.162, time/batch = 0.114\n",
      "Sequence 1650330/3263200 (epoch 50), batch 168230, train_loss = 1.204, time/batch = 0.116\n",
      "Sequence 1651320/3263200 (epoch 50), batch 168330, train_loss = 1.287, time/batch = 0.118\n",
      "Sequence 1652300/3263200 (epoch 50), batch 168430, train_loss = 1.774, time/batch = 0.114\n",
      "Sequence 1653290/3263200 (epoch 50), batch 168530, train_loss = 1.766, time/batch = 0.114\n",
      "Sequence 1654260/3263200 (epoch 50), batch 168630, train_loss = 1.253, time/batch = 0.115\n",
      "Sequence 1655240/3263200 (epoch 50), batch 168730, train_loss = 1.527, time/batch = 0.118\n",
      "Sequence 1656230/3263200 (epoch 50), batch 168830, train_loss = 1.198, time/batch = 0.115\n",
      "Sequence 1657220/3263200 (epoch 50), batch 168930, train_loss = 1.330, time/batch = 0.118\n",
      "Sequence 1658220/3263200 (epoch 50), batch 169030, train_loss = 1.475, time/batch = 0.116\n",
      "Sequence 1659160/3263200 (epoch 50), batch 169131, train_loss = 0.707, time/batch = 0.114\n",
      "Sequence 1660160/3263200 (epoch 50), batch 169231, train_loss = 0.962, time/batch = 0.112\n",
      "Sequence 1661140/3263200 (epoch 50), batch 169331, train_loss = 1.332, time/batch = 0.113\n",
      "Sequence 1662100/3263200 (epoch 50), batch 169431, train_loss = 0.997, time/batch = 0.114\n",
      "Sequence 1663080/3263200 (epoch 50), batch 169531, train_loss = 1.891, time/batch = 0.114\n",
      "Sequence 1664060/3263200 (epoch 50), batch 169631, train_loss = 1.555, time/batch = 0.115\n",
      "Epoch 50 completed, average train loss 1.251480, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1665052/3263200 (epoch 51), batch 169731, train_loss = 1.617, time/batch = 0.116\n",
      "Sequence 1666032/3263200 (epoch 51), batch 169831, train_loss = 1.510, time/batch = 0.114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1667032/3263200 (epoch 51), batch 169931, train_loss = 1.468, time/batch = 0.115\n",
      "Sequence 1668022/3263200 (epoch 51), batch 170031, train_loss = 1.247, time/batch = 0.114\n",
      "Sequence 1669002/3263200 (epoch 51), batch 170131, train_loss = 1.026, time/batch = 0.113\n",
      "Sequence 1669972/3263200 (epoch 51), batch 170231, train_loss = 0.632, time/batch = 0.114\n",
      "Sequence 1670932/3263200 (epoch 51), batch 170331, train_loss = 1.194, time/batch = 0.113\n",
      "Sequence 1671912/3263200 (epoch 51), batch 170431, train_loss = 1.146, time/batch = 0.114\n",
      "Sequence 1672892/3263200 (epoch 51), batch 170531, train_loss = 1.325, time/batch = 0.116\n",
      "Sequence 1673882/3263200 (epoch 51), batch 170631, train_loss = 1.311, time/batch = 0.115\n",
      "Sequence 1674872/3263200 (epoch 51), batch 170731, train_loss = 1.260, time/batch = 0.113\n",
      "Sequence 1675842/3263200 (epoch 51), batch 170831, train_loss = 1.292, time/batch = 0.118\n",
      "Sequence 1676822/3263200 (epoch 51), batch 170931, train_loss = 1.174, time/batch = 0.112\n",
      "Sequence 1677802/3263200 (epoch 51), batch 171031, train_loss = 1.476, time/batch = 0.115\n",
      "Sequence 1678762/3263200 (epoch 51), batch 171131, train_loss = 1.197, time/batch = 0.114\n",
      "Sequence 1679732/3263200 (epoch 51), batch 171231, train_loss = 0.903, time/batch = 0.115\n",
      "Sequence 1680712/3263200 (epoch 51), batch 171331, train_loss = 1.222, time/batch = 0.115\n",
      "Sequence 1681692/3263200 (epoch 51), batch 171431, train_loss = 1.316, time/batch = 0.114\n",
      "Sequence 1682662/3263200 (epoch 51), batch 171531, train_loss = 1.380, time/batch = 0.117\n",
      "Sequence 1683652/3263200 (epoch 51), batch 171631, train_loss = 1.664, time/batch = 0.116\n",
      "Sequence 1684622/3263200 (epoch 51), batch 171731, train_loss = 1.356, time/batch = 0.113\n",
      "Sequence 1685592/3263200 (epoch 51), batch 171831, train_loss = 1.181, time/batch = 0.115\n",
      "Sequence 1686592/3263200 (epoch 51), batch 171931, train_loss = 1.676, time/batch = 0.115\n",
      "Sequence 1687582/3263200 (epoch 51), batch 172031, train_loss = 1.564, time/batch = 0.116\n",
      "Sequence 1688562/3263200 (epoch 51), batch 172131, train_loss = 1.500, time/batch = 0.114\n",
      "Sequence 1689542/3263200 (epoch 51), batch 172231, train_loss = 0.873, time/batch = 0.117\n",
      "Sequence 1690512/3263200 (epoch 51), batch 172331, train_loss = 1.089, time/batch = 0.114\n",
      "Sequence 1691512/3263200 (epoch 51), batch 172431, train_loss = 1.126, time/batch = 0.114\n",
      "Sequence 1692492/3263200 (epoch 51), batch 172531, train_loss = 1.000, time/batch = 0.112\n",
      "Sequence 1693452/3263200 (epoch 51), batch 172631, train_loss = 0.953, time/batch = 0.115\n",
      "Sequence 1694442/3263200 (epoch 51), batch 172731, train_loss = 1.269, time/batch = 0.114\n",
      "Sequence 1695432/3263200 (epoch 51), batch 172831, train_loss = 0.955, time/batch = 0.113\n",
      "Sequence 1696412/3263200 (epoch 51), batch 172931, train_loss = 1.014, time/batch = 0.116\n",
      "Epoch 51 completed, average train loss 1.246033, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1697414/3263200 (epoch 52), batch 173031, train_loss = 1.148, time/batch = 0.114\n",
      "Sequence 1698384/3263200 (epoch 52), batch 173131, train_loss = 0.864, time/batch = 0.117\n",
      "Sequence 1699384/3263200 (epoch 52), batch 173231, train_loss = 1.045, time/batch = 0.116\n",
      "Sequence 1700354/3263200 (epoch 52), batch 173331, train_loss = 1.045, time/batch = 0.115\n",
      "Sequence 1701334/3263200 (epoch 52), batch 173431, train_loss = 1.158, time/batch = 0.113\n",
      "Sequence 1702324/3263200 (epoch 52), batch 173531, train_loss = 1.916, time/batch = 0.116\n",
      "Sequence 1703314/3263200 (epoch 52), batch 173631, train_loss = 1.172, time/batch = 0.117\n",
      "Sequence 1704284/3263200 (epoch 52), batch 173731, train_loss = 1.481, time/batch = 0.115\n",
      "Sequence 1705254/3263200 (epoch 52), batch 173831, train_loss = 1.176, time/batch = 0.117\n",
      "Sequence 1706234/3263200 (epoch 52), batch 173931, train_loss = 1.157, time/batch = 0.115\n",
      "Sequence 1707194/3263200 (epoch 52), batch 174031, train_loss = 1.010, time/batch = 0.115\n",
      "Sequence 1708194/3263200 (epoch 52), batch 174131, train_loss = 0.936, time/batch = 0.113\n",
      "Sequence 1709174/3263200 (epoch 52), batch 174231, train_loss = 1.151, time/batch = 0.113\n",
      "Sequence 1710134/3263200 (epoch 52), batch 174331, train_loss = 1.177, time/batch = 0.116\n",
      "Sequence 1711104/3263200 (epoch 52), batch 174431, train_loss = 1.761, time/batch = 0.114\n",
      "Sequence 1712094/3263200 (epoch 52), batch 174531, train_loss = 1.678, time/batch = 0.115\n",
      "Sequence 1713074/3263200 (epoch 52), batch 174631, train_loss = 1.758, time/batch = 0.112\n",
      "Sequence 1714044/3263200 (epoch 52), batch 174731, train_loss = 0.936, time/batch = 0.116\n",
      "Sequence 1715034/3263200 (epoch 52), batch 174831, train_loss = 1.176, time/batch = 0.117\n",
      "Sequence 1716024/3263200 (epoch 52), batch 174931, train_loss = 1.292, time/batch = 0.115\n",
      "Sequence 1717014/3263200 (epoch 52), batch 175031, train_loss = 2.280, time/batch = 0.114\n",
      "Sequence 1718004/3263200 (epoch 52), batch 175131, train_loss = 2.267, time/batch = 0.116\n",
      "Sequence 1718964/3263200 (epoch 52), batch 175231, train_loss = 0.677, time/batch = 0.115\n",
      "Sequence 1719964/3263200 (epoch 52), batch 175331, train_loss = 1.643, time/batch = 0.115\n",
      "Sequence 1720964/3263200 (epoch 52), batch 175431, train_loss = 0.983, time/batch = 0.115\n",
      "Sequence 1721934/3263200 (epoch 52), batch 175531, train_loss = 1.172, time/batch = 0.114\n",
      "Sequence 1722914/3263200 (epoch 52), batch 175631, train_loss = 1.370, time/batch = 0.118\n",
      "Sequence 1723884/3263200 (epoch 52), batch 175731, train_loss = 1.012, time/batch = 0.115\n",
      "Sequence 1724884/3263200 (epoch 52), batch 175831, train_loss = 1.311, time/batch = 0.112\n",
      "Sequence 1725864/3263200 (epoch 52), batch 175931, train_loss = 1.485, time/batch = 0.116\n",
      "Sequence 1726824/3263200 (epoch 52), batch 176031, train_loss = 1.217, time/batch = 0.112\n",
      "Sequence 1727804/3263200 (epoch 52), batch 176131, train_loss = 1.206, time/batch = 0.118\n",
      "Sequence 1728794/3263200 (epoch 52), batch 176231, train_loss = 0.983, time/batch = 0.113\n",
      "Epoch 52 completed, average train loss 1.242583, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1729786/3263200 (epoch 53), batch 176331, train_loss = 1.123, time/batch = 0.115\n",
      "Sequence 1730786/3263200 (epoch 53), batch 176431, train_loss = 1.044, time/batch = 0.116\n",
      "Sequence 1731766/3263200 (epoch 53), batch 176531, train_loss = 1.419, time/batch = 0.118\n",
      "Sequence 1732736/3263200 (epoch 53), batch 176631, train_loss = 0.659, time/batch = 0.116\n",
      "Sequence 1733726/3263200 (epoch 53), batch 176731, train_loss = 1.002, time/batch = 0.118\n",
      "Sequence 1734706/3263200 (epoch 53), batch 176831, train_loss = 1.035, time/batch = 0.116\n",
      "Sequence 1735696/3263200 (epoch 53), batch 176931, train_loss = 1.071, time/batch = 0.116\n",
      "Sequence 1736686/3263200 (epoch 53), batch 177031, train_loss = 0.911, time/batch = 0.114\n",
      "Sequence 1737646/3263200 (epoch 53), batch 177131, train_loss = 0.457, time/batch = 0.116\n",
      "Sequence 1738636/3263200 (epoch 53), batch 177231, train_loss = 1.190, time/batch = 0.117\n",
      "Sequence 1739596/3263200 (epoch 53), batch 177331, train_loss = 1.757, time/batch = 0.115\n",
      "Sequence 1740566/3263200 (epoch 53), batch 177431, train_loss = 1.770, time/batch = 0.117\n",
      "Sequence 1741566/3263200 (epoch 53), batch 177531, train_loss = 1.118, time/batch = 0.115\n",
      "Sequence 1742536/3263200 (epoch 53), batch 177631, train_loss = 1.304, time/batch = 0.116\n",
      "Sequence 1743516/3263200 (epoch 53), batch 177731, train_loss = 1.346, time/batch = 0.115\n",
      "Sequence 1744506/3263200 (epoch 53), batch 177831, train_loss = 0.962, time/batch = 0.114\n",
      "Sequence 1745486/3263200 (epoch 53), batch 177931, train_loss = 0.867, time/batch = 0.115\n",
      "Sequence 1746476/3263200 (epoch 53), batch 178031, train_loss = 1.172, time/batch = 0.117\n",
      "Sequence 1747426/3263200 (epoch 53), batch 178131, train_loss = 0.998, time/batch = 0.114\n",
      "Sequence 1748406/3263200 (epoch 53), batch 178231, train_loss = 1.192, time/batch = 0.114\n",
      "Sequence 1749396/3263200 (epoch 53), batch 178331, train_loss = 1.307, time/batch = 0.116\n",
      "Sequence 1750336/3263200 (epoch 53), batch 178431, train_loss = 1.191, time/batch = 0.113\n",
      "Sequence 1751326/3263200 (epoch 53), batch 178531, train_loss = 1.477, time/batch = 0.113\n",
      "Sequence 1752306/3263200 (epoch 53), batch 178631, train_loss = 1.197, time/batch = 0.113\n",
      "Sequence 1753296/3263200 (epoch 53), batch 178731, train_loss = 0.835, time/batch = 0.113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1754286/3263200 (epoch 53), batch 178831, train_loss = 1.169, time/batch = 0.117\n",
      "Sequence 1755286/3263200 (epoch 53), batch 178931, train_loss = 1.477, time/batch = 0.114\n",
      "Sequence 1756266/3263200 (epoch 53), batch 179031, train_loss = 1.054, time/batch = 0.114\n",
      "Sequence 1757266/3263200 (epoch 53), batch 179131, train_loss = 1.269, time/batch = 0.112\n",
      "Sequence 1758246/3263200 (epoch 53), batch 179231, train_loss = 1.154, time/batch = 0.113\n",
      "Sequence 1759216/3263200 (epoch 53), batch 179331, train_loss = 1.640, time/batch = 0.118\n",
      "Sequence 1760196/3263200 (epoch 53), batch 179431, train_loss = 0.923, time/batch = 0.116\n",
      "Sequence 1761176/3263200 (epoch 53), batch 179531, train_loss = 1.135, time/batch = 0.114\n",
      "Epoch 53 completed, average train loss 1.239161, learning rate 0.0010\n",
      "Sequence 1762158/3263200 (epoch 54), batch 179631, train_loss = 0.835, time/batch = 0.112\n",
      "Shuffling training data...\n",
      "Sequence 1763138/3263200 (epoch 54), batch 179731, train_loss = 1.240, time/batch = 0.116\n",
      "Sequence 1764138/3263200 (epoch 54), batch 179831, train_loss = 1.308, time/batch = 0.114\n",
      "Sequence 1765118/3263200 (epoch 54), batch 179931, train_loss = 1.402, time/batch = 0.113\n",
      "Sequence 1766088/3263200 (epoch 54), batch 180031, train_loss = 0.963, time/batch = 0.115\n",
      "Sequence 1767088/3263200 (epoch 54), batch 180131, train_loss = 1.037, time/batch = 0.115\n",
      "Sequence 1768068/3263200 (epoch 54), batch 180231, train_loss = 1.518, time/batch = 0.113\n",
      "Sequence 1769058/3263200 (epoch 54), batch 180331, train_loss = 1.513, time/batch = 0.115\n",
      "Sequence 1770038/3263200 (epoch 54), batch 180431, train_loss = 1.145, time/batch = 0.116\n",
      "Sequence 1771018/3263200 (epoch 54), batch 180531, train_loss = 2.536, time/batch = 0.116\n",
      "Sequence 1771988/3263200 (epoch 54), batch 180631, train_loss = 1.347, time/batch = 0.118\n",
      "Sequence 1772978/3263200 (epoch 54), batch 180731, train_loss = 2.276, time/batch = 0.112\n",
      "Sequence 1773958/3263200 (epoch 54), batch 180831, train_loss = 1.175, time/batch = 0.116\n",
      "Sequence 1774938/3263200 (epoch 54), batch 180931, train_loss = 0.850, time/batch = 0.116\n",
      "Sequence 1775918/3263200 (epoch 54), batch 181031, train_loss = 1.246, time/batch = 0.114\n",
      "Sequence 1776878/3263200 (epoch 54), batch 181131, train_loss = 0.943, time/batch = 0.117\n",
      "Sequence 1777848/3263200 (epoch 54), batch 181231, train_loss = 0.689, time/batch = 0.116\n",
      "Sequence 1778828/3263200 (epoch 54), batch 181331, train_loss = 1.171, time/batch = 0.113\n",
      "Sequence 1779828/3263200 (epoch 54), batch 181431, train_loss = 1.065, time/batch = 0.113\n",
      "Sequence 1780798/3263200 (epoch 54), batch 181531, train_loss = 0.953, time/batch = 0.116\n",
      "Sequence 1781788/3263200 (epoch 54), batch 181631, train_loss = 1.446, time/batch = 0.117\n",
      "Sequence 1782738/3263200 (epoch 54), batch 181731, train_loss = 1.066, time/batch = 0.114\n",
      "Sequence 1783708/3263200 (epoch 54), batch 181831, train_loss = 1.703, time/batch = 0.113\n",
      "Sequence 1784708/3263200 (epoch 54), batch 181931, train_loss = 1.187, time/batch = 0.113\n",
      "Sequence 1785688/3263200 (epoch 54), batch 182031, train_loss = 1.449, time/batch = 0.116\n",
      "Sequence 1786688/3263200 (epoch 54), batch 182131, train_loss = 0.864, time/batch = 0.117\n",
      "Sequence 1787648/3263200 (epoch 54), batch 182231, train_loss = 1.463, time/batch = 0.116\n",
      "Sequence 1788638/3263200 (epoch 54), batch 182331, train_loss = 1.290, time/batch = 0.114\n",
      "Sequence 1789608/3263200 (epoch 54), batch 182431, train_loss = 1.385, time/batch = 0.115\n",
      "Sequence 1790598/3263200 (epoch 54), batch 182531, train_loss = 1.453, time/batch = 0.115\n",
      "Sequence 1791598/3263200 (epoch 54), batch 182631, train_loss = 1.374, time/batch = 0.114\n",
      "Sequence 1792588/3263200 (epoch 54), batch 182732, train_loss = 0.598, time/batch = 0.116\n",
      "Sequence 1793558/3263200 (epoch 54), batch 182832, train_loss = 1.226, time/batch = 0.116\n",
      "Sequence 1794548/3263200 (epoch 54), batch 182932, train_loss = 1.270, time/batch = 0.114\n",
      "Epoch 54 completed, average train loss 1.235600, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 1795520/3263200 (epoch 55), batch 183032, train_loss = 0.942, time/batch = 0.113\n",
      "Sequence 1796490/3263200 (epoch 55), batch 183132, train_loss = 0.877, time/batch = 0.113\n",
      "Sequence 1797460/3263200 (epoch 55), batch 183232, train_loss = 1.209, time/batch = 0.115\n",
      "Sequence 1798440/3263200 (epoch 55), batch 183332, train_loss = 1.182, time/batch = 0.113\n",
      "Sequence 1799410/3263200 (epoch 55), batch 183432, train_loss = 1.092, time/batch = 0.114\n",
      "Sequence 1800380/3263200 (epoch 55), batch 183533, train_loss = 0.659, time/batch = 0.117\n",
      "Sequence 1801360/3263200 (epoch 55), batch 183633, train_loss = 1.050, time/batch = 0.114\n",
      "Sequence 1802320/3263200 (epoch 55), batch 183733, train_loss = 1.720, time/batch = 0.115\n",
      "Sequence 1803320/3263200 (epoch 55), batch 183833, train_loss = 1.708, time/batch = 0.115\n",
      "Sequence 1804300/3263200 (epoch 55), batch 183933, train_loss = 0.733, time/batch = 0.114\n",
      "Sequence 1805270/3263200 (epoch 55), batch 184033, train_loss = 1.607, time/batch = 0.114\n",
      "Sequence 1806260/3263200 (epoch 55), batch 184133, train_loss = 1.420, time/batch = 0.113\n",
      "Sequence 1807250/3263200 (epoch 55), batch 184233, train_loss = 1.417, time/batch = 0.114\n",
      "Sequence 1808240/3263200 (epoch 55), batch 184333, train_loss = 1.537, time/batch = 0.114\n",
      "Sequence 1809240/3263200 (epoch 55), batch 184433, train_loss = 0.858, time/batch = 0.115\n",
      "Sequence 1810230/3263200 (epoch 55), batch 184533, train_loss = 0.749, time/batch = 0.117\n",
      "Sequence 1811190/3263200 (epoch 55), batch 184633, train_loss = 1.121, time/batch = 0.117\n",
      "Sequence 1812160/3263200 (epoch 55), batch 184733, train_loss = 0.942, time/batch = 0.116\n",
      "Sequence 1813140/3263200 (epoch 55), batch 184833, train_loss = 1.651, time/batch = 0.115\n",
      "Sequence 1814130/3263200 (epoch 55), batch 184933, train_loss = 1.476, time/batch = 0.116\n",
      "Sequence 1815120/3263200 (epoch 55), batch 185033, train_loss = 1.505, time/batch = 0.114\n",
      "Sequence 1816090/3263200 (epoch 55), batch 185133, train_loss = 0.870, time/batch = 0.116\n",
      "Sequence 1817080/3263200 (epoch 55), batch 185233, train_loss = 1.322, time/batch = 0.115\n",
      "Sequence 1818060/3263200 (epoch 55), batch 185333, train_loss = 0.965, time/batch = 0.115\n",
      "Sequence 1819040/3263200 (epoch 55), batch 185433, train_loss = 0.851, time/batch = 0.116\n",
      "Sequence 1820040/3263200 (epoch 55), batch 185533, train_loss = 1.096, time/batch = 0.116\n",
      "Sequence 1821020/3263200 (epoch 55), batch 185633, train_loss = 1.151, time/batch = 0.115\n",
      "Sequence 1822020/3263200 (epoch 55), batch 185733, train_loss = 1.010, time/batch = 0.118\n",
      "Sequence 1823020/3263200 (epoch 55), batch 185833, train_loss = 1.005, time/batch = 0.115\n",
      "Sequence 1823990/3263200 (epoch 55), batch 185933, train_loss = 2.015, time/batch = 0.116\n",
      "Sequence 1824970/3263200 (epoch 55), batch 186033, train_loss = 1.888, time/batch = 0.115\n",
      "Sequence 1825950/3263200 (epoch 55), batch 186133, train_loss = 2.041, time/batch = 0.113\n",
      "Sequence 1826920/3263200 (epoch 55), batch 186233, train_loss = 1.424, time/batch = 0.117\n",
      "Epoch 55 completed, average train loss 1.232873, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1827902/3263200 (epoch 56), batch 186333, train_loss = 0.939, time/batch = 0.115\n",
      "Sequence 1828902/3263200 (epoch 56), batch 186433, train_loss = 1.041, time/batch = 0.113\n",
      "Sequence 1829892/3263200 (epoch 56), batch 186533, train_loss = 1.434, time/batch = 0.113\n",
      "Sequence 1830892/3263200 (epoch 56), batch 186633, train_loss = 1.658, time/batch = 0.114\n",
      "Sequence 1831882/3263200 (epoch 56), batch 186733, train_loss = 1.197, time/batch = 0.114\n",
      "Sequence 1832862/3263200 (epoch 56), batch 186833, train_loss = 1.056, time/batch = 0.117\n",
      "Sequence 1833852/3263200 (epoch 56), batch 186933, train_loss = 0.944, time/batch = 0.114\n",
      "Sequence 1834822/3263200 (epoch 56), batch 187033, train_loss = 1.329, time/batch = 0.116\n",
      "Sequence 1835802/3263200 (epoch 56), batch 187133, train_loss = 0.953, time/batch = 0.115\n",
      "Sequence 1836772/3263200 (epoch 56), batch 187233, train_loss = 1.752, time/batch = 0.115\n",
      "Sequence 1837752/3263200 (epoch 56), batch 187333, train_loss = 2.568, time/batch = 0.113\n",
      "Sequence 1838712/3263200 (epoch 56), batch 187433, train_loss = 1.502, time/batch = 0.112\n",
      "Sequence 1839692/3263200 (epoch 56), batch 187533, train_loss = 1.224, time/batch = 0.116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1840682/3263200 (epoch 56), batch 187633, train_loss = 0.883, time/batch = 0.113\n",
      "Sequence 1841662/3263200 (epoch 56), batch 187733, train_loss = 1.281, time/batch = 0.113\n",
      "Sequence 1842662/3263200 (epoch 56), batch 187833, train_loss = 0.834, time/batch = 0.117\n",
      "Sequence 1843652/3263200 (epoch 56), batch 187933, train_loss = 0.935, time/batch = 0.113\n",
      "Sequence 1844642/3263200 (epoch 56), batch 188033, train_loss = 1.368, time/batch = 0.117\n",
      "Sequence 1845602/3263200 (epoch 56), batch 188133, train_loss = 1.049, time/batch = 0.114\n",
      "Sequence 1846592/3263200 (epoch 56), batch 188233, train_loss = 1.008, time/batch = 0.116\n",
      "Sequence 1847582/3263200 (epoch 56), batch 188333, train_loss = 0.964, time/batch = 0.116\n",
      "Sequence 1848552/3263200 (epoch 56), batch 188433, train_loss = 1.202, time/batch = 0.114\n",
      "Sequence 1849522/3263200 (epoch 56), batch 188533, train_loss = 1.150, time/batch = 0.114\n",
      "Sequence 1850512/3263200 (epoch 56), batch 188633, train_loss = 0.861, time/batch = 0.116\n",
      "Sequence 1851472/3263200 (epoch 56), batch 188733, train_loss = 0.312, time/batch = 0.116\n",
      "Sequence 1852432/3263200 (epoch 56), batch 188833, train_loss = 0.493, time/batch = 0.118\n",
      "Sequence 1853432/3263200 (epoch 56), batch 188933, train_loss = 1.316, time/batch = 0.114\n",
      "Sequence 1854402/3263200 (epoch 56), batch 189033, train_loss = 1.364, time/batch = 0.117\n",
      "Sequence 1855382/3263200 (epoch 56), batch 189133, train_loss = 1.318, time/batch = 0.117\n",
      "Sequence 1856372/3263200 (epoch 56), batch 189233, train_loss = 0.927, time/batch = 0.114\n",
      "Sequence 1857352/3263200 (epoch 56), batch 189333, train_loss = 1.153, time/batch = 0.113\n",
      "Sequence 1858322/3263200 (epoch 56), batch 189433, train_loss = 1.285, time/batch = 0.116\n",
      "Sequence 1859292/3263200 (epoch 56), batch 189533, train_loss = 1.452, time/batch = 0.113\n",
      "Epoch 56 completed, average train loss 1.228555, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1860264/3263200 (epoch 57), batch 189633, train_loss = 0.990, time/batch = 0.113\n",
      "Sequence 1861254/3263200 (epoch 57), batch 189733, train_loss = 0.961, time/batch = 0.112\n",
      "Sequence 1862214/3263200 (epoch 57), batch 189834, train_loss = 0.790, time/batch = 0.118\n",
      "Sequence 1863204/3263200 (epoch 57), batch 189934, train_loss = 1.232, time/batch = 0.114\n",
      "Sequence 1864194/3263200 (epoch 57), batch 190034, train_loss = 1.039, time/batch = 0.113\n",
      "Sequence 1865184/3263200 (epoch 57), batch 190134, train_loss = 1.503, time/batch = 0.114\n",
      "Sequence 1866164/3263200 (epoch 57), batch 190234, train_loss = 1.664, time/batch = 0.116\n",
      "Sequence 1867134/3263200 (epoch 57), batch 190334, train_loss = 1.041, time/batch = 0.116\n",
      "Sequence 1868104/3263200 (epoch 57), batch 190434, train_loss = 1.273, time/batch = 0.114\n",
      "Sequence 1869074/3263200 (epoch 57), batch 190534, train_loss = 0.897, time/batch = 0.115\n",
      "Sequence 1870054/3263200 (epoch 57), batch 190634, train_loss = 1.546, time/batch = 0.114\n",
      "Sequence 1871024/3263200 (epoch 57), batch 190734, train_loss = 1.020, time/batch = 0.114\n",
      "Sequence 1872004/3263200 (epoch 57), batch 190834, train_loss = 1.224, time/batch = 0.114\n",
      "Sequence 1872994/3263200 (epoch 57), batch 190934, train_loss = 0.832, time/batch = 0.112\n",
      "Sequence 1873974/3263200 (epoch 57), batch 191034, train_loss = 1.507, time/batch = 0.114\n",
      "Sequence 1874974/3263200 (epoch 57), batch 191134, train_loss = 1.859, time/batch = 0.117\n",
      "Sequence 1875954/3263200 (epoch 57), batch 191234, train_loss = 1.168, time/batch = 0.112\n",
      "Sequence 1876924/3263200 (epoch 57), batch 191334, train_loss = 1.753, time/batch = 0.117\n",
      "Sequence 1877904/3263200 (epoch 57), batch 191434, train_loss = 0.955, time/batch = 0.114\n",
      "Sequence 1878894/3263200 (epoch 57), batch 191534, train_loss = 0.970, time/batch = 0.114\n",
      "Sequence 1879894/3263200 (epoch 57), batch 191634, train_loss = 1.624, time/batch = 0.116\n",
      "Sequence 1880874/3263200 (epoch 57), batch 191734, train_loss = 1.225, time/batch = 0.115\n",
      "Sequence 1881834/3263200 (epoch 57), batch 191834, train_loss = 1.331, time/batch = 0.113\n",
      "Sequence 1882804/3263200 (epoch 57), batch 191934, train_loss = 1.088, time/batch = 0.116\n",
      "Sequence 1883794/3263200 (epoch 57), batch 192034, train_loss = 1.259, time/batch = 0.114\n",
      "Sequence 1884774/3263200 (epoch 57), batch 192134, train_loss = 1.295, time/batch = 0.114\n",
      "Sequence 1885764/3263200 (epoch 57), batch 192234, train_loss = 1.176, time/batch = 0.115\n",
      "Sequence 1886744/3263200 (epoch 57), batch 192334, train_loss = 1.063, time/batch = 0.116\n",
      "Sequence 1887724/3263200 (epoch 57), batch 192434, train_loss = 1.436, time/batch = 0.115\n",
      "Sequence 1888714/3263200 (epoch 57), batch 192534, train_loss = 1.214, time/batch = 0.113\n",
      "Sequence 1889694/3263200 (epoch 57), batch 192634, train_loss = 1.182, time/batch = 0.113\n",
      "Sequence 1890684/3263200 (epoch 57), batch 192734, train_loss = 1.117, time/batch = 0.114\n",
      "Sequence 1891664/3263200 (epoch 57), batch 192834, train_loss = 1.002, time/batch = 0.118\n",
      "Epoch 57 completed, average train loss 1.226230, learning rate 0.0010\n",
      "Sequence 1892656/3263200 (epoch 58), batch 192934, train_loss = 1.007, time/batch = 0.113\n",
      "Shuffling training data...\n",
      "Sequence 1893656/3263200 (epoch 58), batch 193034, train_loss = 1.746, time/batch = 0.115\n",
      "Sequence 1894636/3263200 (epoch 58), batch 193134, train_loss = 0.655, time/batch = 0.115\n",
      "Sequence 1895636/3263200 (epoch 58), batch 193234, train_loss = 1.025, time/batch = 0.118\n",
      "Sequence 1896616/3263200 (epoch 58), batch 193334, train_loss = 1.863, time/batch = 0.119\n",
      "Sequence 1897586/3263200 (epoch 58), batch 193434, train_loss = 1.689, time/batch = 0.119\n",
      "Sequence 1898586/3263200 (epoch 58), batch 193534, train_loss = 1.565, time/batch = 0.116\n",
      "Sequence 1899576/3263200 (epoch 58), batch 193634, train_loss = 1.449, time/batch = 0.119\n",
      "Sequence 1900536/3263200 (epoch 58), batch 193734, train_loss = 1.317, time/batch = 0.116\n",
      "Sequence 1901516/3263200 (epoch 58), batch 193834, train_loss = 1.473, time/batch = 0.118\n",
      "Sequence 1902516/3263200 (epoch 58), batch 193934, train_loss = 1.117, time/batch = 0.119\n",
      "Sequence 1903486/3263200 (epoch 58), batch 194034, train_loss = 1.181, time/batch = 0.118\n",
      "Sequence 1904456/3263200 (epoch 58), batch 194134, train_loss = 1.088, time/batch = 0.119\n",
      "Sequence 1905446/3263200 (epoch 58), batch 194234, train_loss = 1.349, time/batch = 0.116\n",
      "Sequence 1906446/3263200 (epoch 58), batch 194334, train_loss = 0.899, time/batch = 0.120\n",
      "Sequence 1907426/3263200 (epoch 58), batch 194434, train_loss = 0.950, time/batch = 0.116\n",
      "Sequence 1908406/3263200 (epoch 58), batch 194534, train_loss = 1.551, time/batch = 0.121\n",
      "Sequence 1909376/3263200 (epoch 58), batch 194634, train_loss = 1.140, time/batch = 0.118\n",
      "Sequence 1910366/3263200 (epoch 58), batch 194734, train_loss = 0.975, time/batch = 0.118\n",
      "Sequence 1911346/3263200 (epoch 58), batch 194834, train_loss = 1.023, time/batch = 0.118\n",
      "Sequence 1912316/3263200 (epoch 58), batch 194934, train_loss = 1.159, time/batch = 0.118\n",
      "Sequence 1913276/3263200 (epoch 58), batch 195034, train_loss = 1.427, time/batch = 0.118\n",
      "Sequence 1914246/3263200 (epoch 58), batch 195134, train_loss = 1.127, time/batch = 0.118\n",
      "Sequence 1915226/3263200 (epoch 58), batch 195234, train_loss = 0.894, time/batch = 0.118\n",
      "Sequence 1916206/3263200 (epoch 58), batch 195334, train_loss = 0.884, time/batch = 0.119\n",
      "Sequence 1917196/3263200 (epoch 58), batch 195434, train_loss = 1.062, time/batch = 0.119\n",
      "Sequence 1918186/3263200 (epoch 58), batch 195534, train_loss = 1.247, time/batch = 0.119\n",
      "Sequence 1919136/3263200 (epoch 58), batch 195634, train_loss = 1.868, time/batch = 0.119\n",
      "Sequence 1920106/3263200 (epoch 58), batch 195734, train_loss = 1.238, time/batch = 0.116\n",
      "Sequence 1921096/3263200 (epoch 58), batch 195834, train_loss = 1.003, time/batch = 0.117\n",
      "Sequence 1922086/3263200 (epoch 58), batch 195934, train_loss = 1.603, time/batch = 0.118\n",
      "Sequence 1923076/3263200 (epoch 58), batch 196034, train_loss = 1.032, time/batch = 0.122\n",
      "Sequence 1924056/3263200 (epoch 58), batch 196134, train_loss = 2.034, time/batch = 0.119\n",
      "Sequence 1925026/3263200 (epoch 58), batch 196234, train_loss = 0.895, time/batch = 0.118\n",
      "Epoch 58 completed, average train loss 1.222823, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1926018/3263200 (epoch 59), batch 196334, train_loss = 0.964, time/batch = 0.120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1926978/3263200 (epoch 59), batch 196434, train_loss = 1.412, time/batch = 0.120\n",
      "Sequence 1927958/3263200 (epoch 59), batch 196534, train_loss = 1.250, time/batch = 0.117\n",
      "Sequence 1928908/3263200 (epoch 59), batch 196634, train_loss = 2.248, time/batch = 0.121\n",
      "Sequence 1929898/3263200 (epoch 59), batch 196734, train_loss = 1.113, time/batch = 0.120\n",
      "Sequence 1930888/3263200 (epoch 59), batch 196834, train_loss = 0.857, time/batch = 0.119\n",
      "Sequence 1931868/3263200 (epoch 59), batch 196934, train_loss = 1.258, time/batch = 0.114\n",
      "Sequence 1932858/3263200 (epoch 59), batch 197034, train_loss = 1.166, time/batch = 0.117\n",
      "Sequence 1933848/3263200 (epoch 59), batch 197134, train_loss = 1.693, time/batch = 0.120\n",
      "Sequence 1934848/3263200 (epoch 59), batch 197234, train_loss = 0.794, time/batch = 0.117\n",
      "Sequence 1935838/3263200 (epoch 59), batch 197334, train_loss = 1.167, time/batch = 0.118\n",
      "Sequence 1936818/3263200 (epoch 59), batch 197434, train_loss = 1.435, time/batch = 0.118\n",
      "Sequence 1937778/3263200 (epoch 59), batch 197534, train_loss = 1.187, time/batch = 0.119\n",
      "Sequence 1938758/3263200 (epoch 59), batch 197634, train_loss = 1.315, time/batch = 0.119\n",
      "Sequence 1939718/3263200 (epoch 59), batch 197734, train_loss = 1.672, time/batch = 0.116\n",
      "Sequence 1940708/3263200 (epoch 59), batch 197834, train_loss = 0.854, time/batch = 0.111\n",
      "Sequence 1941688/3263200 (epoch 59), batch 197934, train_loss = 1.672, time/batch = 0.120\n",
      "Sequence 1942668/3263200 (epoch 59), batch 198034, train_loss = 1.298, time/batch = 0.117\n",
      "Sequence 1943648/3263200 (epoch 59), batch 198134, train_loss = 1.663, time/batch = 0.118\n",
      "Sequence 1944618/3263200 (epoch 59), batch 198234, train_loss = 0.927, time/batch = 0.118\n",
      "Sequence 1945618/3263200 (epoch 59), batch 198334, train_loss = 1.433, time/batch = 0.118\n",
      "Sequence 1946598/3263200 (epoch 59), batch 198434, train_loss = 1.314, time/batch = 0.120\n",
      "Sequence 1947588/3263200 (epoch 59), batch 198534, train_loss = 1.703, time/batch = 0.119\n",
      "Sequence 1948568/3263200 (epoch 59), batch 198634, train_loss = 0.886, time/batch = 0.121\n",
      "Sequence 1949528/3263200 (epoch 59), batch 198734, train_loss = 1.927, time/batch = 0.118\n",
      "Sequence 1950498/3263200 (epoch 59), batch 198834, train_loss = 0.839, time/batch = 0.120\n",
      "Sequence 1951498/3263200 (epoch 59), batch 198934, train_loss = 1.740, time/batch = 0.118\n",
      "Sequence 1952478/3263200 (epoch 59), batch 199034, train_loss = 2.006, time/batch = 0.119\n",
      "Sequence 1953458/3263200 (epoch 59), batch 199134, train_loss = 0.899, time/batch = 0.117\n",
      "Sequence 1954448/3263200 (epoch 59), batch 199234, train_loss = 1.056, time/batch = 0.120\n",
      "Sequence 1955418/3263200 (epoch 59), batch 199334, train_loss = 1.260, time/batch = 0.118\n",
      "Sequence 1956408/3263200 (epoch 59), batch 199434, train_loss = 2.249, time/batch = 0.119\n",
      "Sequence 1957378/3263200 (epoch 59), batch 199534, train_loss = 1.747, time/batch = 0.118\n",
      "Epoch 59 completed, average train loss 1.220203, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 1958380/3263200 (epoch 60), batch 199634, train_loss = 1.512, time/batch = 0.116\n",
      "Sequence 1959370/3263200 (epoch 60), batch 199734, train_loss = 1.018, time/batch = 0.115\n",
      "Sequence 1960360/3263200 (epoch 60), batch 199834, train_loss = 0.927, time/batch = 0.120\n",
      "Sequence 1961350/3263200 (epoch 60), batch 199934, train_loss = 1.477, time/batch = 0.116\n",
      "Sequence 1962310/3263200 (epoch 60), batch 200034, train_loss = 1.282, time/batch = 0.118\n",
      "Sequence 1963280/3263200 (epoch 60), batch 200134, train_loss = 1.710, time/batch = 0.115\n",
      "Sequence 1964280/3263200 (epoch 60), batch 200234, train_loss = 1.133, time/batch = 0.119\n",
      "Sequence 1965230/3263200 (epoch 60), batch 200334, train_loss = 1.136, time/batch = 0.119\n",
      "Sequence 1966200/3263200 (epoch 60), batch 200434, train_loss = 1.335, time/batch = 0.119\n",
      "Sequence 1967190/3263200 (epoch 60), batch 200534, train_loss = 0.923, time/batch = 0.120\n",
      "Sequence 1968170/3263200 (epoch 60), batch 200634, train_loss = 1.080, time/batch = 0.118\n",
      "Sequence 1969150/3263200 (epoch 60), batch 200734, train_loss = 0.958, time/batch = 0.119\n",
      "Sequence 1970140/3263200 (epoch 60), batch 200834, train_loss = 1.606, time/batch = 0.118\n",
      "Sequence 1971140/3263200 (epoch 60), batch 200934, train_loss = 1.606, time/batch = 0.120\n",
      "Sequence 1972130/3263200 (epoch 60), batch 201034, train_loss = 0.586, time/batch = 0.117\n",
      "Sequence 1973090/3263200 (epoch 60), batch 201134, train_loss = 1.381, time/batch = 0.118\n",
      "Sequence 1974080/3263200 (epoch 60), batch 201234, train_loss = 0.809, time/batch = 0.117\n",
      "Sequence 1975030/3263200 (epoch 60), batch 201334, train_loss = 1.412, time/batch = 0.122\n",
      "Sequence 1976010/3263200 (epoch 60), batch 201434, train_loss = 0.972, time/batch = 0.118\n",
      "Sequence 1976980/3263200 (epoch 60), batch 201534, train_loss = 1.264, time/batch = 0.119\n",
      "Sequence 1977920/3263200 (epoch 60), batch 201634, train_loss = 0.812, time/batch = 0.118\n",
      "Sequence 1978920/3263200 (epoch 60), batch 201734, train_loss = 0.789, time/batch = 0.122\n",
      "Sequence 1979900/3263200 (epoch 60), batch 201834, train_loss = 1.010, time/batch = 0.120\n",
      "Sequence 1980900/3263200 (epoch 60), batch 201934, train_loss = 1.145, time/batch = 0.118\n",
      "Sequence 1981900/3263200 (epoch 60), batch 202034, train_loss = 1.193, time/batch = 0.119\n",
      "Sequence 1982880/3263200 (epoch 60), batch 202134, train_loss = 2.056, time/batch = 0.118\n",
      "Sequence 1983860/3263200 (epoch 60), batch 202234, train_loss = 0.816, time/batch = 0.120\n",
      "Sequence 1984850/3263200 (epoch 60), batch 202334, train_loss = 0.852, time/batch = 0.119\n",
      "Sequence 1985840/3263200 (epoch 60), batch 202435, train_loss = 0.591, time/batch = 0.118\n",
      "Sequence 1986830/3263200 (epoch 60), batch 202535, train_loss = 1.713, time/batch = 0.117\n",
      "Sequence 1987820/3263200 (epoch 60), batch 202635, train_loss = 1.017, time/batch = 0.121\n",
      "Sequence 1988800/3263200 (epoch 60), batch 202735, train_loss = 1.380, time/batch = 0.118\n",
      "Sequence 1989770/3263200 (epoch 60), batch 202835, train_loss = 1.578, time/batch = 0.120\n",
      "Epoch 60 completed, average train loss 1.217053, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1990752/3263200 (epoch 61), batch 202935, train_loss = 1.040, time/batch = 0.117\n",
      "Sequence 1991732/3263200 (epoch 61), batch 203035, train_loss = 1.018, time/batch = 0.115\n",
      "Sequence 1992722/3263200 (epoch 61), batch 203135, train_loss = 0.971, time/batch = 0.115\n",
      "Sequence 1993672/3263200 (epoch 61), batch 203235, train_loss = 0.979, time/batch = 0.116\n",
      "Sequence 1994662/3263200 (epoch 61), batch 203335, train_loss = 1.079, time/batch = 0.115\n",
      "Sequence 1995652/3263200 (epoch 61), batch 203435, train_loss = 0.980, time/batch = 0.112\n",
      "Sequence 1996612/3263200 (epoch 61), batch 203535, train_loss = 1.291, time/batch = 0.118\n",
      "Sequence 1997582/3263200 (epoch 61), batch 203635, train_loss = 1.277, time/batch = 0.118\n",
      "Sequence 1998572/3263200 (epoch 61), batch 203735, train_loss = 1.096, time/batch = 0.118\n",
      "Sequence 1999562/3263200 (epoch 61), batch 203835, train_loss = 1.045, time/batch = 0.117\n",
      "Sequence 2000512/3263200 (epoch 61), batch 203935, train_loss = 1.967, time/batch = 0.121\n",
      "Sequence 2001492/3263200 (epoch 61), batch 204035, train_loss = 1.605, time/batch = 0.119\n",
      "Sequence 2002452/3263200 (epoch 61), batch 204135, train_loss = 1.340, time/batch = 0.119\n",
      "Sequence 2003442/3263200 (epoch 61), batch 204235, train_loss = 1.265, time/batch = 0.120\n",
      "Sequence 2004412/3263200 (epoch 61), batch 204335, train_loss = 0.997, time/batch = 0.119\n",
      "Sequence 2005402/3263200 (epoch 61), batch 204435, train_loss = 1.164, time/batch = 0.118\n",
      "Sequence 2006382/3263200 (epoch 61), batch 204535, train_loss = 0.832, time/batch = 0.116\n",
      "Sequence 2007352/3263200 (epoch 61), batch 204635, train_loss = 1.107, time/batch = 0.121\n",
      "Sequence 2008352/3263200 (epoch 61), batch 204735, train_loss = 1.379, time/batch = 0.119\n",
      "Sequence 2009322/3263200 (epoch 61), batch 204835, train_loss = 1.202, time/batch = 0.118\n",
      "Sequence 2010322/3263200 (epoch 61), batch 204935, train_loss = 1.127, time/batch = 0.119\n",
      "Sequence 2011282/3263200 (epoch 61), batch 205035, train_loss = 1.272, time/batch = 0.117\n",
      "Sequence 2012252/3263200 (epoch 61), batch 205135, train_loss = 1.821, time/batch = 0.116\n",
      "Sequence 2013242/3263200 (epoch 61), batch 205235, train_loss = 1.179, time/batch = 0.115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2014242/3263200 (epoch 61), batch 205335, train_loss = 1.491, time/batch = 0.117\n",
      "Sequence 2015222/3263200 (epoch 61), batch 205435, train_loss = 1.200, time/batch = 0.112\n",
      "Sequence 2016202/3263200 (epoch 61), batch 205535, train_loss = 1.033, time/batch = 0.116\n",
      "Sequence 2017192/3263200 (epoch 61), batch 205635, train_loss = 1.713, time/batch = 0.118\n",
      "Sequence 2018182/3263200 (epoch 61), batch 205735, train_loss = 1.091, time/batch = 0.119\n",
      "Sequence 2019162/3263200 (epoch 61), batch 205835, train_loss = 1.490, time/batch = 0.120\n",
      "Sequence 2020152/3263200 (epoch 61), batch 205935, train_loss = 1.449, time/batch = 0.119\n",
      "Sequence 2021152/3263200 (epoch 61), batch 206035, train_loss = 1.371, time/batch = 0.116\n",
      "Sequence 2022142/3263200 (epoch 61), batch 206135, train_loss = 1.202, time/batch = 0.116\n",
      "Sequence 2023112/3263200 (epoch 61), batch 206235, train_loss = 1.274, time/batch = 0.121\n",
      "Epoch 61 completed, average train loss 1.214059, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2024084/3263200 (epoch 62), batch 206335, train_loss = 1.429, time/batch = 0.121\n",
      "Sequence 2025084/3263200 (epoch 62), batch 206435, train_loss = 1.189, time/batch = 0.117\n",
      "Sequence 2026044/3263200 (epoch 62), batch 206535, train_loss = 1.418, time/batch = 0.118\n",
      "Sequence 2027024/3263200 (epoch 62), batch 206635, train_loss = 1.928, time/batch = 0.112\n",
      "Sequence 2028014/3263200 (epoch 62), batch 206735, train_loss = 1.871, time/batch = 0.111\n",
      "Sequence 2028994/3263200 (epoch 62), batch 206835, train_loss = 0.964, time/batch = 0.113\n",
      "Sequence 2029984/3263200 (epoch 62), batch 206935, train_loss = 1.022, time/batch = 0.119\n",
      "Sequence 2030954/3263200 (epoch 62), batch 207035, train_loss = 1.324, time/batch = 0.117\n",
      "Sequence 2031924/3263200 (epoch 62), batch 207135, train_loss = 1.173, time/batch = 0.116\n",
      "Sequence 2032904/3263200 (epoch 62), batch 207235, train_loss = 1.192, time/batch = 0.119\n",
      "Sequence 2033864/3263200 (epoch 62), batch 207335, train_loss = 1.099, time/batch = 0.120\n",
      "Sequence 2034854/3263200 (epoch 62), batch 207435, train_loss = 1.439, time/batch = 0.118\n",
      "Sequence 2035854/3263200 (epoch 62), batch 207535, train_loss = 1.196, time/batch = 0.115\n",
      "Sequence 2036844/3263200 (epoch 62), batch 207635, train_loss = 1.519, time/batch = 0.119\n",
      "Sequence 2037834/3263200 (epoch 62), batch 207735, train_loss = 0.927, time/batch = 0.121\n",
      "Sequence 2038804/3263200 (epoch 62), batch 207835, train_loss = 1.595, time/batch = 0.121\n",
      "Sequence 2039804/3263200 (epoch 62), batch 207935, train_loss = 1.241, time/batch = 0.119\n",
      "Sequence 2040784/3263200 (epoch 62), batch 208035, train_loss = 1.314, time/batch = 0.120\n",
      "Sequence 2041764/3263200 (epoch 62), batch 208135, train_loss = 1.360, time/batch = 0.119\n",
      "Sequence 2042744/3263200 (epoch 62), batch 208235, train_loss = 1.118, time/batch = 0.117\n",
      "Sequence 2043744/3263200 (epoch 62), batch 208335, train_loss = 2.869, time/batch = 0.118\n",
      "Sequence 2044724/3263200 (epoch 62), batch 208435, train_loss = 0.936, time/batch = 0.117\n",
      "Sequence 2045664/3263200 (epoch 62), batch 208535, train_loss = 0.617, time/batch = 0.119\n",
      "Sequence 2046644/3263200 (epoch 62), batch 208635, train_loss = 1.050, time/batch = 0.119\n",
      "Sequence 2047644/3263200 (epoch 62), batch 208735, train_loss = 0.710, time/batch = 0.120\n",
      "Sequence 2048634/3263200 (epoch 62), batch 208835, train_loss = 1.282, time/batch = 0.118\n",
      "Sequence 2049624/3263200 (epoch 62), batch 208935, train_loss = 1.323, time/batch = 0.120\n",
      "Sequence 2050614/3263200 (epoch 62), batch 209035, train_loss = 0.909, time/batch = 0.117\n",
      "Sequence 2051574/3263200 (epoch 62), batch 209136, train_loss = 1.858, time/batch = 0.117\n",
      "Sequence 2052544/3263200 (epoch 62), batch 209236, train_loss = 1.380, time/batch = 0.119\n",
      "Sequence 2053544/3263200 (epoch 62), batch 209336, train_loss = 0.719, time/batch = 0.119\n",
      "Sequence 2054524/3263200 (epoch 62), batch 209436, train_loss = 1.573, time/batch = 0.120\n",
      "Sequence 2055494/3263200 (epoch 62), batch 209536, train_loss = 0.896, time/batch = 0.118\n",
      "Epoch 62 completed, average train loss 1.210703, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2056476/3263200 (epoch 63), batch 209637, train_loss = 0.621, time/batch = 0.119\n",
      "Sequence 2057476/3263200 (epoch 63), batch 209737, train_loss = 0.958, time/batch = 0.119\n",
      "Sequence 2058446/3263200 (epoch 63), batch 209837, train_loss = 1.896, time/batch = 0.119\n",
      "Sequence 2059426/3263200 (epoch 63), batch 209937, train_loss = 1.356, time/batch = 0.119\n",
      "Sequence 2060406/3263200 (epoch 63), batch 210037, train_loss = 1.638, time/batch = 0.122\n",
      "Sequence 2061376/3263200 (epoch 63), batch 210137, train_loss = 0.935, time/batch = 0.120\n",
      "Sequence 2062366/3263200 (epoch 63), batch 210237, train_loss = 0.938, time/batch = 0.120\n",
      "Sequence 2063356/3263200 (epoch 63), batch 210337, train_loss = 1.856, time/batch = 0.120\n",
      "Sequence 2064336/3263200 (epoch 63), batch 210437, train_loss = 1.526, time/batch = 0.121\n",
      "Sequence 2065296/3263200 (epoch 63), batch 210537, train_loss = 1.194, time/batch = 0.118\n",
      "Sequence 2066266/3263200 (epoch 63), batch 210637, train_loss = 0.942, time/batch = 0.119\n",
      "Sequence 2067256/3263200 (epoch 63), batch 210737, train_loss = 1.053, time/batch = 0.116\n",
      "Sequence 2068256/3263200 (epoch 63), batch 210837, train_loss = 1.342, time/batch = 0.111\n",
      "Sequence 2069236/3263200 (epoch 63), batch 210937, train_loss = 2.056, time/batch = 0.111\n",
      "Sequence 2070236/3263200 (epoch 63), batch 211037, train_loss = 1.537, time/batch = 0.116\n",
      "Sequence 2071226/3263200 (epoch 63), batch 211137, train_loss = 1.621, time/batch = 0.119\n",
      "Sequence 2072216/3263200 (epoch 63), batch 211237, train_loss = 1.074, time/batch = 0.120\n",
      "Sequence 2073196/3263200 (epoch 63), batch 211337, train_loss = 0.803, time/batch = 0.119\n",
      "Sequence 2074166/3263200 (epoch 63), batch 211437, train_loss = 1.116, time/batch = 0.118\n",
      "Sequence 2075156/3263200 (epoch 63), batch 211537, train_loss = 1.358, time/batch = 0.116\n",
      "Sequence 2076156/3263200 (epoch 63), batch 211637, train_loss = 1.212, time/batch = 0.120\n",
      "Sequence 2077136/3263200 (epoch 63), batch 211737, train_loss = 1.414, time/batch = 0.119\n",
      "Sequence 2078106/3263200 (epoch 63), batch 211837, train_loss = 1.000, time/batch = 0.117\n",
      "Sequence 2079086/3263200 (epoch 63), batch 211937, train_loss = 2.166, time/batch = 0.120\n",
      "Sequence 2080066/3263200 (epoch 63), batch 212037, train_loss = 0.974, time/batch = 0.120\n",
      "Sequence 2081036/3263200 (epoch 63), batch 212137, train_loss = 0.711, time/batch = 0.120\n",
      "Sequence 2081996/3263200 (epoch 63), batch 212237, train_loss = 0.947, time/batch = 0.117\n",
      "Sequence 2082996/3263200 (epoch 63), batch 212337, train_loss = 0.748, time/batch = 0.120\n",
      "Sequence 2083986/3263200 (epoch 63), batch 212437, train_loss = 0.831, time/batch = 0.118\n",
      "Sequence 2084966/3263200 (epoch 63), batch 212537, train_loss = 1.396, time/batch = 0.118\n",
      "Sequence 2085936/3263200 (epoch 63), batch 212637, train_loss = 1.229, time/batch = 0.119\n",
      "Sequence 2086906/3263200 (epoch 63), batch 212737, train_loss = 0.693, time/batch = 0.119\n",
      "Sequence 2087886/3263200 (epoch 63), batch 212837, train_loss = 0.978, time/batch = 0.116\n",
      "Epoch 63 completed, average train loss 1.207737, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2088868/3263200 (epoch 64), batch 212937, train_loss = 0.782, time/batch = 0.118\n",
      "Sequence 2089858/3263200 (epoch 64), batch 213037, train_loss = 1.018, time/batch = 0.118\n",
      "Sequence 2090858/3263200 (epoch 64), batch 213137, train_loss = 1.400, time/batch = 0.117\n",
      "Sequence 2091838/3263200 (epoch 64), batch 213237, train_loss = 1.766, time/batch = 0.114\n",
      "Sequence 2092818/3263200 (epoch 64), batch 213337, train_loss = 1.386, time/batch = 0.113\n",
      "Sequence 2093788/3263200 (epoch 64), batch 213437, train_loss = 1.225, time/batch = 0.120\n",
      "Sequence 2094788/3263200 (epoch 64), batch 213537, train_loss = 1.838, time/batch = 0.118\n",
      "Sequence 2095788/3263200 (epoch 64), batch 213637, train_loss = 1.102, time/batch = 0.117\n",
      "Sequence 2096718/3263200 (epoch 64), batch 213737, train_loss = 0.910, time/batch = 0.121\n",
      "Sequence 2097708/3263200 (epoch 64), batch 213837, train_loss = 1.244, time/batch = 0.114\n",
      "Sequence 2098708/3263200 (epoch 64), batch 213937, train_loss = 1.129, time/batch = 0.113\n",
      "Sequence 2099678/3263200 (epoch 64), batch 214037, train_loss = 1.432, time/batch = 0.118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2100668/3263200 (epoch 64), batch 214137, train_loss = 1.027, time/batch = 0.117\n",
      "Sequence 2101628/3263200 (epoch 64), batch 214237, train_loss = 1.305, time/batch = 0.117\n",
      "Sequence 2102598/3263200 (epoch 64), batch 214337, train_loss = 0.897, time/batch = 0.118\n",
      "Sequence 2103568/3263200 (epoch 64), batch 214437, train_loss = 0.987, time/batch = 0.120\n",
      "Sequence 2104538/3263200 (epoch 64), batch 214537, train_loss = 1.111, time/batch = 0.116\n",
      "Sequence 2105528/3263200 (epoch 64), batch 214637, train_loss = 1.646, time/batch = 0.119\n",
      "Sequence 2106508/3263200 (epoch 64), batch 214737, train_loss = 0.979, time/batch = 0.119\n",
      "Sequence 2107498/3263200 (epoch 64), batch 214837, train_loss = 1.294, time/batch = 0.121\n",
      "Sequence 2108468/3263200 (epoch 64), batch 214937, train_loss = 0.965, time/batch = 0.118\n",
      "Sequence 2109448/3263200 (epoch 64), batch 215037, train_loss = 0.869, time/batch = 0.120\n",
      "Sequence 2110438/3263200 (epoch 64), batch 215137, train_loss = 1.231, time/batch = 0.120\n",
      "Sequence 2111408/3263200 (epoch 64), batch 215237, train_loss = 1.351, time/batch = 0.119\n",
      "Sequence 2112408/3263200 (epoch 64), batch 215337, train_loss = 1.640, time/batch = 0.118\n",
      "Sequence 2113388/3263200 (epoch 64), batch 215437, train_loss = 0.865, time/batch = 0.119\n",
      "Sequence 2114368/3263200 (epoch 64), batch 215537, train_loss = 0.748, time/batch = 0.120\n",
      "Sequence 2115308/3263200 (epoch 64), batch 215637, train_loss = 1.623, time/batch = 0.120\n",
      "Sequence 2116278/3263200 (epoch 64), batch 215737, train_loss = 1.086, time/batch = 0.118\n",
      "Sequence 2117268/3263200 (epoch 64), batch 215837, train_loss = 1.347, time/batch = 0.119\n",
      "Sequence 2118248/3263200 (epoch 64), batch 215937, train_loss = 0.428, time/batch = 0.118\n",
      "Sequence 2119248/3263200 (epoch 64), batch 216037, train_loss = 1.073, time/batch = 0.120\n",
      "Sequence 2120248/3263200 (epoch 64), batch 216137, train_loss = 1.286, time/batch = 0.120\n",
      "Epoch 64 completed, average train loss 1.206705, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 2121230/3263200 (epoch 65), batch 216237, train_loss = 1.178, time/batch = 0.120\n",
      "Sequence 2122210/3263200 (epoch 65), batch 216337, train_loss = 1.280, time/batch = 0.111\n",
      "Sequence 2123190/3263200 (epoch 65), batch 216437, train_loss = 2.005, time/batch = 0.114\n",
      "Sequence 2124170/3263200 (epoch 65), batch 216537, train_loss = 1.574, time/batch = 0.119\n",
      "Sequence 2125150/3263200 (epoch 65), batch 216637, train_loss = 1.032, time/batch = 0.117\n",
      "Sequence 2126120/3263200 (epoch 65), batch 216737, train_loss = 1.067, time/batch = 0.120\n",
      "Sequence 2127110/3263200 (epoch 65), batch 216837, train_loss = 0.834, time/batch = 0.119\n",
      "Sequence 2128110/3263200 (epoch 65), batch 216937, train_loss = 1.255, time/batch = 0.121\n",
      "Sequence 2129100/3263200 (epoch 65), batch 217037, train_loss = 1.436, time/batch = 0.118\n",
      "Sequence 2130080/3263200 (epoch 65), batch 217137, train_loss = 0.840, time/batch = 0.121\n",
      "Sequence 2131050/3263200 (epoch 65), batch 217237, train_loss = 1.121, time/batch = 0.118\n",
      "Sequence 2132030/3263200 (epoch 65), batch 217337, train_loss = 1.225, time/batch = 0.123\n",
      "Sequence 2133020/3263200 (epoch 65), batch 217437, train_loss = 0.885, time/batch = 0.119\n",
      "Sequence 2134020/3263200 (epoch 65), batch 217537, train_loss = 1.220, time/batch = 0.118\n",
      "Sequence 2135020/3263200 (epoch 65), batch 217637, train_loss = 1.093, time/batch = 0.119\n",
      "Sequence 2136000/3263200 (epoch 65), batch 217738, train_loss = 1.511, time/batch = 0.120\n",
      "Sequence 2136980/3263200 (epoch 65), batch 217838, train_loss = 0.884, time/batch = 0.118\n",
      "Sequence 2137960/3263200 (epoch 65), batch 217938, train_loss = 1.343, time/batch = 0.118\n",
      "Sequence 2138940/3263200 (epoch 65), batch 218038, train_loss = 0.758, time/batch = 0.118\n",
      "Sequence 2139940/3263200 (epoch 65), batch 218138, train_loss = 1.228, time/batch = 0.118\n",
      "Sequence 2140930/3263200 (epoch 65), batch 218238, train_loss = 0.888, time/batch = 0.118\n",
      "Sequence 2141900/3263200 (epoch 65), batch 218338, train_loss = 0.975, time/batch = 0.121\n",
      "Sequence 2142900/3263200 (epoch 65), batch 218438, train_loss = 2.147, time/batch = 0.122\n",
      "Sequence 2143890/3263200 (epoch 65), batch 218538, train_loss = 1.484, time/batch = 0.121\n",
      "Sequence 2144880/3263200 (epoch 65), batch 218638, train_loss = 0.850, time/batch = 0.120\n",
      "Sequence 2145860/3263200 (epoch 65), batch 218738, train_loss = 0.831, time/batch = 0.119\n",
      "Sequence 2146800/3263200 (epoch 65), batch 218838, train_loss = 1.209, time/batch = 0.119\n",
      "Sequence 2147770/3263200 (epoch 65), batch 218938, train_loss = 0.461, time/batch = 0.120\n",
      "Sequence 2148740/3263200 (epoch 65), batch 219038, train_loss = 1.048, time/batch = 0.118\n",
      "Sequence 2149720/3263200 (epoch 65), batch 219138, train_loss = 1.828, time/batch = 0.116\n",
      "Sequence 2150680/3263200 (epoch 65), batch 219238, train_loss = 1.103, time/batch = 0.114\n",
      "Sequence 2151640/3263200 (epoch 65), batch 219338, train_loss = 1.205, time/batch = 0.114\n",
      "Sequence 2152630/3263200 (epoch 65), batch 219438, train_loss = 1.044, time/batch = 0.115\n",
      "Sequence 2153620/3263200 (epoch 65), batch 219538, train_loss = 1.065, time/batch = 0.118\n",
      "Epoch 65 completed, average train loss 1.204364, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2154612/3263200 (epoch 66), batch 219638, train_loss = 0.968, time/batch = 0.121\n",
      "Sequence 2155572/3263200 (epoch 66), batch 219738, train_loss = 0.982, time/batch = 0.121\n",
      "Sequence 2156562/3263200 (epoch 66), batch 219838, train_loss = 1.329, time/batch = 0.120\n",
      "Sequence 2157532/3263200 (epoch 66), batch 219938, train_loss = 1.079, time/batch = 0.117\n",
      "Sequence 2158502/3263200 (epoch 66), batch 220038, train_loss = 1.140, time/batch = 0.120\n",
      "Sequence 2159482/3263200 (epoch 66), batch 220138, train_loss = 1.246, time/batch = 0.120\n",
      "Sequence 2160472/3263200 (epoch 66), batch 220238, train_loss = 1.149, time/batch = 0.118\n",
      "Sequence 2161462/3263200 (epoch 66), batch 220338, train_loss = 1.421, time/batch = 0.118\n",
      "Sequence 2162442/3263200 (epoch 66), batch 220438, train_loss = 1.734, time/batch = 0.120\n",
      "Sequence 2163442/3263200 (epoch 66), batch 220538, train_loss = 1.108, time/batch = 0.117\n",
      "Sequence 2164402/3263200 (epoch 66), batch 220638, train_loss = 1.257, time/batch = 0.117\n",
      "Sequence 2165382/3263200 (epoch 66), batch 220738, train_loss = 1.109, time/batch = 0.118\n",
      "Sequence 2166342/3263200 (epoch 66), batch 220838, train_loss = 0.890, time/batch = 0.118\n",
      "Sequence 2167332/3263200 (epoch 66), batch 220938, train_loss = 0.837, time/batch = 0.120\n",
      "Sequence 2168312/3263200 (epoch 66), batch 221038, train_loss = 1.443, time/batch = 0.116\n",
      "Sequence 2169272/3263200 (epoch 66), batch 221138, train_loss = 1.261, time/batch = 0.116\n",
      "Sequence 2170252/3263200 (epoch 66), batch 221238, train_loss = 0.862, time/batch = 0.120\n",
      "Sequence 2171242/3263200 (epoch 66), batch 221338, train_loss = 1.325, time/batch = 0.117\n",
      "Sequence 2172232/3263200 (epoch 66), batch 221438, train_loss = 1.108, time/batch = 0.118\n",
      "Sequence 2173232/3263200 (epoch 66), batch 221538, train_loss = 1.234, time/batch = 0.121\n",
      "Sequence 2174212/3263200 (epoch 66), batch 221638, train_loss = 1.493, time/batch = 0.120\n",
      "Sequence 2175202/3263200 (epoch 66), batch 221738, train_loss = 0.906, time/batch = 0.120\n",
      "Sequence 2176192/3263200 (epoch 66), batch 221838, train_loss = 1.542, time/batch = 0.119\n",
      "Sequence 2177182/3263200 (epoch 66), batch 221938, train_loss = 1.178, time/batch = 0.118\n",
      "Sequence 2178162/3263200 (epoch 66), batch 222038, train_loss = 1.465, time/batch = 0.120\n",
      "Sequence 2179142/3263200 (epoch 66), batch 222138, train_loss = 0.865, time/batch = 0.118\n",
      "Sequence 2180112/3263200 (epoch 66), batch 222238, train_loss = 1.587, time/batch = 0.117\n",
      "Sequence 2181082/3263200 (epoch 66), batch 222338, train_loss = 1.251, time/batch = 0.119\n",
      "Sequence 2182072/3263200 (epoch 66), batch 222438, train_loss = 1.288, time/batch = 0.116\n",
      "Sequence 2183062/3263200 (epoch 66), batch 222538, train_loss = 0.761, time/batch = 0.117\n",
      "Sequence 2184042/3263200 (epoch 66), batch 222638, train_loss = 1.328, time/batch = 0.119\n",
      "Sequence 2185032/3263200 (epoch 66), batch 222738, train_loss = 1.575, time/batch = 0.119\n",
      "Sequence 2186002/3263200 (epoch 66), batch 222838, train_loss = 1.142, time/batch = 0.120\n",
      "Epoch 66 completed, average train loss 1.200763, learning rate 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling training data...\n",
      "Sequence 2186984/3263200 (epoch 67), batch 222938, train_loss = 1.003, time/batch = 0.119\n",
      "Sequence 2187954/3263200 (epoch 67), batch 223038, train_loss = 1.105, time/batch = 0.117\n",
      "Sequence 2188944/3263200 (epoch 67), batch 223138, train_loss = 1.183, time/batch = 0.120\n",
      "Sequence 2189924/3263200 (epoch 67), batch 223238, train_loss = 0.939, time/batch = 0.122\n",
      "Sequence 2190904/3263200 (epoch 67), batch 223338, train_loss = 1.465, time/batch = 0.119\n",
      "Sequence 2191864/3263200 (epoch 67), batch 223438, train_loss = 0.782, time/batch = 0.118\n",
      "Sequence 2192834/3263200 (epoch 67), batch 223538, train_loss = 0.818, time/batch = 0.120\n",
      "Sequence 2193814/3263200 (epoch 67), batch 223638, train_loss = 0.999, time/batch = 0.121\n",
      "Sequence 2194804/3263200 (epoch 67), batch 223738, train_loss = 0.980, time/batch = 0.118\n",
      "Sequence 2195804/3263200 (epoch 67), batch 223838, train_loss = 1.501, time/batch = 0.120\n",
      "Sequence 2196774/3263200 (epoch 67), batch 223938, train_loss = 1.386, time/batch = 0.118\n",
      "Sequence 2197754/3263200 (epoch 67), batch 224038, train_loss = 1.457, time/batch = 0.118\n",
      "Sequence 2198754/3263200 (epoch 67), batch 224138, train_loss = 1.178, time/batch = 0.120\n",
      "Sequence 2199724/3263200 (epoch 67), batch 224238, train_loss = 1.049, time/batch = 0.120\n",
      "Sequence 2200704/3263200 (epoch 67), batch 224338, train_loss = 1.366, time/batch = 0.120\n",
      "Sequence 2201684/3263200 (epoch 67), batch 224438, train_loss = 1.016, time/batch = 0.118\n",
      "Sequence 2202654/3263200 (epoch 67), batch 224538, train_loss = 1.430, time/batch = 0.117\n",
      "Sequence 2203634/3263200 (epoch 67), batch 224638, train_loss = 0.841, time/batch = 0.118\n",
      "Sequence 2204634/3263200 (epoch 67), batch 224738, train_loss = 1.536, time/batch = 0.118\n",
      "Sequence 2205634/3263200 (epoch 67), batch 224838, train_loss = 1.612, time/batch = 0.119\n",
      "Sequence 2206624/3263200 (epoch 67), batch 224938, train_loss = 1.581, time/batch = 0.116\n",
      "Sequence 2207594/3263200 (epoch 67), batch 225038, train_loss = 1.354, time/batch = 0.120\n",
      "Sequence 2208584/3263200 (epoch 67), batch 225138, train_loss = 1.310, time/batch = 0.120\n",
      "Sequence 2209564/3263200 (epoch 67), batch 225238, train_loss = 0.702, time/batch = 0.118\n",
      "Sequence 2210524/3263200 (epoch 67), batch 225338, train_loss = 1.086, time/batch = 0.118\n",
      "Sequence 2211504/3263200 (epoch 67), batch 225438, train_loss = 1.020, time/batch = 0.120\n",
      "Sequence 2212474/3263200 (epoch 67), batch 225538, train_loss = 1.410, time/batch = 0.118\n",
      "Sequence 2213454/3263200 (epoch 67), batch 225638, train_loss = 0.947, time/batch = 0.119\n",
      "Sequence 2214454/3263200 (epoch 67), batch 225738, train_loss = 1.740, time/batch = 0.120\n",
      "Sequence 2215434/3263200 (epoch 67), batch 225838, train_loss = 1.103, time/batch = 0.118\n",
      "Sequence 2216414/3263200 (epoch 67), batch 225938, train_loss = 1.468, time/batch = 0.116\n",
      "Sequence 2217404/3263200 (epoch 67), batch 226038, train_loss = 0.959, time/batch = 0.118\n",
      "Sequence 2218384/3263200 (epoch 67), batch 226138, train_loss = 1.294, time/batch = 0.122\n",
      "Epoch 67 completed, average train loss 1.198668, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2219356/3263200 (epoch 68), batch 226238, train_loss = 1.217, time/batch = 0.119\n",
      "Sequence 2220336/3263200 (epoch 68), batch 226338, train_loss = 1.891, time/batch = 0.120\n",
      "Sequence 2221296/3263200 (epoch 68), batch 226438, train_loss = 1.346, time/batch = 0.118\n",
      "Sequence 2222276/3263200 (epoch 68), batch 226538, train_loss = 1.216, time/batch = 0.118\n",
      "Sequence 2223246/3263200 (epoch 68), batch 226638, train_loss = 1.064, time/batch = 0.121\n",
      "Sequence 2224246/3263200 (epoch 68), batch 226738, train_loss = 1.451, time/batch = 0.119\n",
      "Sequence 2225226/3263200 (epoch 68), batch 226838, train_loss = 2.123, time/batch = 0.119\n",
      "Sequence 2226206/3263200 (epoch 68), batch 226938, train_loss = 1.172, time/batch = 0.119\n",
      "Sequence 2227196/3263200 (epoch 68), batch 227038, train_loss = 1.078, time/batch = 0.120\n",
      "Sequence 2228196/3263200 (epoch 68), batch 227138, train_loss = 1.180, time/batch = 0.118\n",
      "Sequence 2229176/3263200 (epoch 68), batch 227238, train_loss = 0.999, time/batch = 0.117\n",
      "Sequence 2230156/3263200 (epoch 68), batch 227338, train_loss = 1.397, time/batch = 0.117\n",
      "Sequence 2231146/3263200 (epoch 68), batch 227438, train_loss = 0.972, time/batch = 0.118\n",
      "Sequence 2232126/3263200 (epoch 68), batch 227538, train_loss = 1.034, time/batch = 0.118\n",
      "Sequence 2233116/3263200 (epoch 68), batch 227638, train_loss = 1.033, time/batch = 0.121\n",
      "Sequence 2234116/3263200 (epoch 68), batch 227738, train_loss = 1.387, time/batch = 0.120\n",
      "Sequence 2235086/3263200 (epoch 68), batch 227838, train_loss = 1.507, time/batch = 0.115\n",
      "Sequence 2236076/3263200 (epoch 68), batch 227938, train_loss = 1.168, time/batch = 0.116\n",
      "Sequence 2237046/3263200 (epoch 68), batch 228038, train_loss = 0.888, time/batch = 0.118\n",
      "Sequence 2238026/3263200 (epoch 68), batch 228138, train_loss = 1.423, time/batch = 0.121\n",
      "Sequence 2239006/3263200 (epoch 68), batch 228238, train_loss = 0.866, time/batch = 0.115\n",
      "Sequence 2239966/3263200 (epoch 68), batch 228338, train_loss = 0.976, time/batch = 0.120\n",
      "Sequence 2240966/3263200 (epoch 68), batch 228438, train_loss = 1.181, time/batch = 0.119\n",
      "Sequence 2241956/3263200 (epoch 68), batch 228538, train_loss = 1.298, time/batch = 0.119\n",
      "Sequence 2242946/3263200 (epoch 68), batch 228638, train_loss = 1.475, time/batch = 0.118\n",
      "Sequence 2243906/3263200 (epoch 68), batch 228738, train_loss = 1.439, time/batch = 0.118\n",
      "Sequence 2244886/3263200 (epoch 68), batch 228839, train_loss = 0.739, time/batch = 0.120\n",
      "Sequence 2245846/3263200 (epoch 68), batch 228939, train_loss = 1.638, time/batch = 0.118\n",
      "Sequence 2246846/3263200 (epoch 68), batch 229040, train_loss = 0.569, time/batch = 0.112\n",
      "Sequence 2247806/3263200 (epoch 68), batch 229140, train_loss = 1.038, time/batch = 0.110\n",
      "Sequence 2248796/3263200 (epoch 68), batch 229240, train_loss = 1.569, time/batch = 0.117\n",
      "Sequence 2249776/3263200 (epoch 68), batch 229340, train_loss = 1.130, time/batch = 0.121\n",
      "Sequence 2250766/3263200 (epoch 68), batch 229441, train_loss = 0.708, time/batch = 0.115\n",
      "Epoch 68 completed, average train loss 1.196221, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2251748/3263200 (epoch 69), batch 229541, train_loss = 1.649, time/batch = 0.119\n",
      "Sequence 2252728/3263200 (epoch 69), batch 229641, train_loss = 0.991, time/batch = 0.121\n",
      "Sequence 2253718/3263200 (epoch 69), batch 229741, train_loss = 1.342, time/batch = 0.118\n",
      "Sequence 2254698/3263200 (epoch 69), batch 229841, train_loss = 1.224, time/batch = 0.120\n",
      "Sequence 2255678/3263200 (epoch 69), batch 229941, train_loss = 1.345, time/batch = 0.118\n",
      "Sequence 2256668/3263200 (epoch 69), batch 230041, train_loss = 1.335, time/batch = 0.119\n",
      "Sequence 2257638/3263200 (epoch 69), batch 230142, train_loss = 0.753, time/batch = 0.119\n",
      "Sequence 2258608/3263200 (epoch 69), batch 230242, train_loss = 1.123, time/batch = 0.119\n",
      "Sequence 2259588/3263200 (epoch 69), batch 230342, train_loss = 1.012, time/batch = 0.120\n",
      "Sequence 2260588/3263200 (epoch 69), batch 230442, train_loss = 1.538, time/batch = 0.119\n",
      "Sequence 2261578/3263200 (epoch 69), batch 230542, train_loss = 1.339, time/batch = 0.120\n",
      "Sequence 2262518/3263200 (epoch 69), batch 230642, train_loss = 1.233, time/batch = 0.121\n",
      "Sequence 2263498/3263200 (epoch 69), batch 230742, train_loss = 1.174, time/batch = 0.118\n",
      "Sequence 2264488/3263200 (epoch 69), batch 230842, train_loss = 1.083, time/batch = 0.120\n",
      "Sequence 2265458/3263200 (epoch 69), batch 230942, train_loss = 0.898, time/batch = 0.119\n",
      "Sequence 2266448/3263200 (epoch 69), batch 231042, train_loss = 1.331, time/batch = 0.118\n",
      "Sequence 2267438/3263200 (epoch 69), batch 231142, train_loss = 1.865, time/batch = 0.119\n",
      "Sequence 2268428/3263200 (epoch 69), batch 231242, train_loss = 1.451, time/batch = 0.118\n",
      "Sequence 2269398/3263200 (epoch 69), batch 231342, train_loss = 1.075, time/batch = 0.117\n",
      "Sequence 2270378/3263200 (epoch 69), batch 231442, train_loss = 1.094, time/batch = 0.118\n",
      "Sequence 2271368/3263200 (epoch 69), batch 231542, train_loss = 0.859, time/batch = 0.118\n",
      "Sequence 2272358/3263200 (epoch 69), batch 231642, train_loss = 1.537, time/batch = 0.120\n",
      "Sequence 2273318/3263200 (epoch 69), batch 231742, train_loss = 1.155, time/batch = 0.118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2274298/3263200 (epoch 69), batch 231842, train_loss = 0.895, time/batch = 0.118\n",
      "Sequence 2275278/3263200 (epoch 69), batch 231942, train_loss = 0.997, time/batch = 0.121\n",
      "Sequence 2276258/3263200 (epoch 69), batch 232042, train_loss = 1.029, time/batch = 0.119\n",
      "Sequence 2277238/3263200 (epoch 69), batch 232143, train_loss = 1.316, time/batch = 0.119\n",
      "Sequence 2278228/3263200 (epoch 69), batch 232243, train_loss = 0.985, time/batch = 0.117\n",
      "Sequence 2279208/3263200 (epoch 69), batch 232343, train_loss = 1.538, time/batch = 0.120\n",
      "Sequence 2280178/3263200 (epoch 69), batch 232443, train_loss = 0.756, time/batch = 0.117\n",
      "Sequence 2281158/3263200 (epoch 69), batch 232543, train_loss = 0.694, time/batch = 0.118\n",
      "Sequence 2282158/3263200 (epoch 69), batch 232643, train_loss = 1.924, time/batch = 0.119\n",
      "Sequence 2283148/3263200 (epoch 69), batch 232743, train_loss = 1.814, time/batch = 0.118\n",
      "Sequence 2284128/3263200 (epoch 69), batch 232843, train_loss = 1.210, time/batch = 0.115\n",
      "Epoch 69 completed, average train loss 1.193689, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 2285130/3263200 (epoch 70), batch 232943, train_loss = 1.422, time/batch = 0.114\n",
      "Sequence 2286120/3263200 (epoch 70), batch 233043, train_loss = 1.116, time/batch = 0.110\n",
      "Sequence 2287110/3263200 (epoch 70), batch 233143, train_loss = 1.540, time/batch = 0.114\n",
      "Sequence 2288090/3263200 (epoch 70), batch 233243, train_loss = 1.101, time/batch = 0.119\n",
      "Sequence 2289080/3263200 (epoch 70), batch 233343, train_loss = 1.402, time/batch = 0.120\n",
      "Sequence 2290050/3263200 (epoch 70), batch 233443, train_loss = 1.221, time/batch = 0.119\n",
      "Sequence 2291040/3263200 (epoch 70), batch 233543, train_loss = 1.307, time/batch = 0.118\n",
      "Sequence 2292010/3263200 (epoch 70), batch 233643, train_loss = 1.380, time/batch = 0.122\n",
      "Sequence 2292990/3263200 (epoch 70), batch 233743, train_loss = 0.786, time/batch = 0.118\n",
      "Sequence 2293980/3263200 (epoch 70), batch 233844, train_loss = 1.234, time/batch = 0.122\n",
      "Sequence 2294970/3263200 (epoch 70), batch 233944, train_loss = 1.009, time/batch = 0.117\n",
      "Sequence 2295960/3263200 (epoch 70), batch 234044, train_loss = 1.502, time/batch = 0.120\n",
      "Sequence 2296950/3263200 (epoch 70), batch 234144, train_loss = 1.138, time/batch = 0.120\n",
      "Sequence 2297930/3263200 (epoch 70), batch 234244, train_loss = 1.183, time/batch = 0.119\n",
      "Sequence 2298920/3263200 (epoch 70), batch 234344, train_loss = 1.065, time/batch = 0.118\n",
      "Sequence 2299890/3263200 (epoch 70), batch 234444, train_loss = 0.876, time/batch = 0.120\n",
      "Sequence 2300890/3263200 (epoch 70), batch 234544, train_loss = 1.054, time/batch = 0.117\n",
      "Sequence 2301870/3263200 (epoch 70), batch 234644, train_loss = 2.088, time/batch = 0.117\n",
      "Sequence 2302840/3263200 (epoch 70), batch 234744, train_loss = 1.082, time/batch = 0.119\n",
      "Sequence 2303840/3263200 (epoch 70), batch 234844, train_loss = 0.817, time/batch = 0.122\n",
      "Sequence 2304820/3263200 (epoch 70), batch 234944, train_loss = 1.398, time/batch = 0.119\n",
      "Sequence 2305810/3263200 (epoch 70), batch 235044, train_loss = 1.246, time/batch = 0.119\n",
      "Sequence 2306790/3263200 (epoch 70), batch 235144, train_loss = 1.129, time/batch = 0.119\n",
      "Sequence 2307760/3263200 (epoch 70), batch 235244, train_loss = 1.105, time/batch = 0.120\n",
      "Sequence 2308710/3263200 (epoch 70), batch 235344, train_loss = 1.023, time/batch = 0.120\n",
      "Sequence 2309670/3263200 (epoch 70), batch 235444, train_loss = 0.899, time/batch = 0.121\n",
      "Sequence 2310650/3263200 (epoch 70), batch 235544, train_loss = 1.422, time/batch = 0.120\n",
      "Sequence 2311620/3263200 (epoch 70), batch 235644, train_loss = 1.094, time/batch = 0.120\n",
      "Sequence 2312580/3263200 (epoch 70), batch 235744, train_loss = 1.869, time/batch = 0.119\n",
      "Sequence 2313570/3263200 (epoch 70), batch 235844, train_loss = 1.430, time/batch = 0.118\n",
      "Sequence 2314540/3263200 (epoch 70), batch 235944, train_loss = 1.338, time/batch = 0.118\n",
      "Sequence 2315540/3263200 (epoch 70), batch 236044, train_loss = 1.589, time/batch = 0.118\n",
      "Sequence 2316510/3263200 (epoch 70), batch 236144, train_loss = 1.279, time/batch = 0.120\n",
      "Epoch 70 completed, average train loss 1.190861, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2317502/3263200 (epoch 71), batch 236244, train_loss = 1.094, time/batch = 0.120\n",
      "Sequence 2318482/3263200 (epoch 71), batch 236344, train_loss = 0.839, time/batch = 0.119\n",
      "Sequence 2319472/3263200 (epoch 71), batch 236444, train_loss = 1.594, time/batch = 0.121\n",
      "Sequence 2320442/3263200 (epoch 71), batch 236544, train_loss = 0.861, time/batch = 0.118\n",
      "Sequence 2321442/3263200 (epoch 71), batch 236644, train_loss = 1.317, time/batch = 0.120\n",
      "Sequence 2322442/3263200 (epoch 71), batch 236744, train_loss = 1.457, time/batch = 0.119\n",
      "Sequence 2323432/3263200 (epoch 71), batch 236844, train_loss = 0.970, time/batch = 0.119\n",
      "Sequence 2324352/3263200 (epoch 71), batch 236944, train_loss = 0.952, time/batch = 0.121\n",
      "Sequence 2325342/3263200 (epoch 71), batch 237044, train_loss = 1.274, time/batch = 0.118\n",
      "Sequence 2326312/3263200 (epoch 71), batch 237144, train_loss = 1.038, time/batch = 0.118\n",
      "Sequence 2327292/3263200 (epoch 71), batch 237244, train_loss = 1.331, time/batch = 0.118\n",
      "Sequence 2328272/3263200 (epoch 71), batch 237344, train_loss = 0.657, time/batch = 0.117\n",
      "Sequence 2329242/3263200 (epoch 71), batch 237444, train_loss = 1.904, time/batch = 0.118\n",
      "Sequence 2330212/3263200 (epoch 71), batch 237544, train_loss = 1.738, time/batch = 0.116\n",
      "Sequence 2331182/3263200 (epoch 71), batch 237644, train_loss = 1.121, time/batch = 0.119\n",
      "Sequence 2332152/3263200 (epoch 71), batch 237744, train_loss = 1.099, time/batch = 0.118\n",
      "Sequence 2333132/3263200 (epoch 71), batch 237844, train_loss = 1.325, time/batch = 0.120\n",
      "Sequence 2334102/3263200 (epoch 71), batch 237944, train_loss = 1.041, time/batch = 0.117\n",
      "Sequence 2335102/3263200 (epoch 71), batch 238044, train_loss = 1.267, time/batch = 0.119\n",
      "Sequence 2336082/3263200 (epoch 71), batch 238144, train_loss = 1.161, time/batch = 0.120\n",
      "Sequence 2337052/3263200 (epoch 71), batch 238244, train_loss = 1.155, time/batch = 0.119\n",
      "Sequence 2338042/3263200 (epoch 71), batch 238344, train_loss = 1.463, time/batch = 0.124\n",
      "Sequence 2339032/3263200 (epoch 71), batch 238444, train_loss = 1.077, time/batch = 0.121\n",
      "Sequence 2340032/3263200 (epoch 71), batch 238544, train_loss = 1.060, time/batch = 0.119\n",
      "Sequence 2341012/3263200 (epoch 71), batch 238644, train_loss = 0.939, time/batch = 0.120\n",
      "Sequence 2341982/3263200 (epoch 71), batch 238744, train_loss = 1.027, time/batch = 0.116\n",
      "Sequence 2342982/3263200 (epoch 71), batch 238844, train_loss = 0.598, time/batch = 0.112\n",
      "Sequence 2343972/3263200 (epoch 71), batch 238944, train_loss = 0.759, time/batch = 0.110\n",
      "Sequence 2344942/3263200 (epoch 71), batch 239044, train_loss = 0.866, time/batch = 0.110\n",
      "Sequence 2345932/3263200 (epoch 71), batch 239144, train_loss = 1.172, time/batch = 0.116\n",
      "Sequence 2346922/3263200 (epoch 71), batch 239245, train_loss = 0.789, time/batch = 0.121\n",
      "Sequence 2347902/3263200 (epoch 71), batch 239345, train_loss = 1.755, time/batch = 0.120\n",
      "Sequence 2348892/3263200 (epoch 71), batch 239445, train_loss = 1.630, time/batch = 0.121\n",
      "Epoch 71 completed, average train loss 1.188550, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2349884/3263200 (epoch 72), batch 239545, train_loss = 0.958, time/batch = 0.119\n",
      "Sequence 2350864/3263200 (epoch 72), batch 239645, train_loss = 1.098, time/batch = 0.112\n",
      "Sequence 2351844/3263200 (epoch 72), batch 239745, train_loss = 1.145, time/batch = 0.111\n",
      "Sequence 2352804/3263200 (epoch 72), batch 239845, train_loss = 0.892, time/batch = 0.112\n",
      "Sequence 2353754/3263200 (epoch 72), batch 239945, train_loss = 1.419, time/batch = 0.118\n",
      "Sequence 2354744/3263200 (epoch 72), batch 240045, train_loss = 0.775, time/batch = 0.120\n",
      "Sequence 2355744/3263200 (epoch 72), batch 240145, train_loss = 1.017, time/batch = 0.118\n",
      "Sequence 2356734/3263200 (epoch 72), batch 240245, train_loss = 1.006, time/batch = 0.116\n",
      "Sequence 2357724/3263200 (epoch 72), batch 240345, train_loss = 0.765, time/batch = 0.118\n",
      "Sequence 2358714/3263200 (epoch 72), batch 240445, train_loss = 1.323, time/batch = 0.119\n",
      "Sequence 2359664/3263200 (epoch 72), batch 240545, train_loss = 0.882, time/batch = 0.120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2360644/3263200 (epoch 72), batch 240645, train_loss = 1.029, time/batch = 0.118\n",
      "Sequence 2361624/3263200 (epoch 72), batch 240745, train_loss = 1.049, time/batch = 0.118\n",
      "Sequence 2362574/3263200 (epoch 72), batch 240845, train_loss = 1.247, time/batch = 0.117\n",
      "Sequence 2363564/3263200 (epoch 72), batch 240945, train_loss = 1.313, time/batch = 0.117\n",
      "Sequence 2364534/3263200 (epoch 72), batch 241045, train_loss = 1.153, time/batch = 0.118\n",
      "Sequence 2365514/3263200 (epoch 72), batch 241145, train_loss = 1.440, time/batch = 0.120\n",
      "Sequence 2366494/3263200 (epoch 72), batch 241245, train_loss = 1.035, time/batch = 0.120\n",
      "Sequence 2367484/3263200 (epoch 72), batch 241345, train_loss = 1.774, time/batch = 0.119\n",
      "Sequence 2368454/3263200 (epoch 72), batch 241445, train_loss = 1.356, time/batch = 0.119\n",
      "Sequence 2369444/3263200 (epoch 72), batch 241545, train_loss = 1.350, time/batch = 0.119\n",
      "Sequence 2370434/3263200 (epoch 72), batch 241645, train_loss = 1.132, time/batch = 0.118\n",
      "Sequence 2371394/3263200 (epoch 72), batch 241745, train_loss = 0.996, time/batch = 0.117\n",
      "Sequence 2372364/3263200 (epoch 72), batch 241845, train_loss = 1.304, time/batch = 0.117\n",
      "Sequence 2373364/3263200 (epoch 72), batch 241945, train_loss = 1.333, time/batch = 0.118\n",
      "Sequence 2374344/3263200 (epoch 72), batch 242045, train_loss = 1.110, time/batch = 0.120\n",
      "Sequence 2375344/3263200 (epoch 72), batch 242145, train_loss = 1.143, time/batch = 0.119\n",
      "Sequence 2376334/3263200 (epoch 72), batch 242245, train_loss = 1.222, time/batch = 0.118\n",
      "Sequence 2377324/3263200 (epoch 72), batch 242345, train_loss = 1.533, time/batch = 0.113\n",
      "Sequence 2378324/3263200 (epoch 72), batch 242445, train_loss = 1.643, time/batch = 0.115\n",
      "Sequence 2379304/3263200 (epoch 72), batch 242545, train_loss = 1.223, time/batch = 0.116\n",
      "Sequence 2380284/3263200 (epoch 72), batch 242645, train_loss = 1.446, time/batch = 0.115\n",
      "Sequence 2381274/3263200 (epoch 72), batch 242745, train_loss = 1.046, time/batch = 0.117\n",
      "Epoch 72 completed, average train loss 1.185438, learning rate 0.0010\n",
      "Sequence 2382256/3263200 (epoch 73), batch 242846, train_loss = 1.289, time/batch = 0.119\n",
      "Shuffling training data...\n",
      "Sequence 2383236/3263200 (epoch 73), batch 242946, train_loss = 1.859, time/batch = 0.120\n",
      "Sequence 2384236/3263200 (epoch 73), batch 243046, train_loss = 0.763, time/batch = 0.120\n",
      "Sequence 2385216/3263200 (epoch 73), batch 243146, train_loss = 1.278, time/batch = 0.117\n",
      "Sequence 2386206/3263200 (epoch 73), batch 243246, train_loss = 1.304, time/batch = 0.118\n",
      "Sequence 2387166/3263200 (epoch 73), batch 243346, train_loss = 0.898, time/batch = 0.119\n",
      "Sequence 2388146/3263200 (epoch 73), batch 243446, train_loss = 1.985, time/batch = 0.122\n",
      "Sequence 2389136/3263200 (epoch 73), batch 243546, train_loss = 0.877, time/batch = 0.118\n",
      "Sequence 2390116/3263200 (epoch 73), batch 243646, train_loss = 1.095, time/batch = 0.117\n",
      "Sequence 2391116/3263200 (epoch 73), batch 243746, train_loss = 1.058, time/batch = 0.115\n",
      "Sequence 2392086/3263200 (epoch 73), batch 243846, train_loss = 0.978, time/batch = 0.115\n",
      "Sequence 2393056/3263200 (epoch 73), batch 243946, train_loss = 1.045, time/batch = 0.116\n",
      "Sequence 2394046/3263200 (epoch 73), batch 244046, train_loss = 1.161, time/batch = 0.116\n",
      "Sequence 2395016/3263200 (epoch 73), batch 244146, train_loss = 1.335, time/batch = 0.116\n",
      "Sequence 2396006/3263200 (epoch 73), batch 244246, train_loss = 0.846, time/batch = 0.118\n",
      "Sequence 2396996/3263200 (epoch 73), batch 244346, train_loss = 1.031, time/batch = 0.118\n",
      "Sequence 2397976/3263200 (epoch 73), batch 244446, train_loss = 1.527, time/batch = 0.119\n",
      "Sequence 2398966/3263200 (epoch 73), batch 244546, train_loss = 0.978, time/batch = 0.120\n",
      "Sequence 2399956/3263200 (epoch 73), batch 244646, train_loss = 1.117, time/batch = 0.119\n",
      "Sequence 2400936/3263200 (epoch 73), batch 244746, train_loss = 0.691, time/batch = 0.120\n",
      "Sequence 2401906/3263200 (epoch 73), batch 244846, train_loss = 1.862, time/batch = 0.118\n",
      "Sequence 2402876/3263200 (epoch 73), batch 244946, train_loss = 1.007, time/batch = 0.120\n",
      "Sequence 2403846/3263200 (epoch 73), batch 245046, train_loss = 0.967, time/batch = 0.119\n",
      "Sequence 2404836/3263200 (epoch 73), batch 245146, train_loss = 0.742, time/batch = 0.118\n",
      "Sequence 2405816/3263200 (epoch 73), batch 245246, train_loss = 1.000, time/batch = 0.120\n",
      "Sequence 2406796/3263200 (epoch 73), batch 245346, train_loss = 1.531, time/batch = 0.121\n",
      "Sequence 2407766/3263200 (epoch 73), batch 245446, train_loss = 1.140, time/batch = 0.118\n",
      "Sequence 2408746/3263200 (epoch 73), batch 245546, train_loss = 1.090, time/batch = 0.119\n",
      "Sequence 2409716/3263200 (epoch 73), batch 245646, train_loss = 1.054, time/batch = 0.117\n",
      "Sequence 2410696/3263200 (epoch 73), batch 245746, train_loss = 1.288, time/batch = 0.118\n",
      "Sequence 2411676/3263200 (epoch 73), batch 245846, train_loss = 1.587, time/batch = 0.119\n",
      "Sequence 2412656/3263200 (epoch 73), batch 245946, train_loss = 0.977, time/batch = 0.118\n",
      "Sequence 2413646/3263200 (epoch 73), batch 246046, train_loss = 1.340, time/batch = 0.118\n",
      "Sequence 2414616/3263200 (epoch 73), batch 246146, train_loss = 1.491, time/batch = 0.113\n",
      "Epoch 73 completed, average train loss 1.184219, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2415608/3263200 (epoch 74), batch 246246, train_loss = 1.322, time/batch = 0.120\n",
      "Sequence 2416598/3263200 (epoch 74), batch 246346, train_loss = 1.339, time/batch = 0.118\n",
      "Sequence 2417598/3263200 (epoch 74), batch 246446, train_loss = 1.493, time/batch = 0.118\n",
      "Sequence 2418568/3263200 (epoch 74), batch 246546, train_loss = 0.766, time/batch = 0.118\n",
      "Sequence 2419538/3263200 (epoch 74), batch 246646, train_loss = 1.851, time/batch = 0.120\n",
      "Sequence 2420508/3263200 (epoch 74), batch 246746, train_loss = 0.812, time/batch = 0.115\n",
      "Sequence 2421488/3263200 (epoch 74), batch 246846, train_loss = 1.360, time/batch = 0.115\n",
      "Sequence 2422488/3263200 (epoch 74), batch 246946, train_loss = 1.447, time/batch = 0.118\n",
      "Sequence 2423458/3263200 (epoch 74), batch 247047, train_loss = 0.521, time/batch = 0.120\n",
      "Sequence 2424408/3263200 (epoch 74), batch 247147, train_loss = 1.155, time/batch = 0.119\n",
      "Sequence 2425398/3263200 (epoch 74), batch 247247, train_loss = 1.273, time/batch = 0.120\n",
      "Sequence 2426368/3263200 (epoch 74), batch 247347, train_loss = 1.261, time/batch = 0.119\n",
      "Sequence 2427358/3263200 (epoch 74), batch 247447, train_loss = 0.843, time/batch = 0.114\n",
      "Sequence 2428328/3263200 (epoch 74), batch 247547, train_loss = 0.874, time/batch = 0.120\n",
      "Sequence 2429308/3263200 (epoch 74), batch 247647, train_loss = 0.802, time/batch = 0.118\n",
      "Sequence 2430258/3263200 (epoch 74), batch 247747, train_loss = 1.077, time/batch = 0.118\n",
      "Sequence 2431258/3263200 (epoch 74), batch 247847, train_loss = 1.181, time/batch = 0.118\n",
      "Sequence 2432248/3263200 (epoch 74), batch 247947, train_loss = 1.061, time/batch = 0.121\n",
      "Sequence 2433238/3263200 (epoch 74), batch 248047, train_loss = 1.035, time/batch = 0.119\n",
      "Sequence 2434228/3263200 (epoch 74), batch 248147, train_loss = 1.156, time/batch = 0.119\n",
      "Sequence 2435218/3263200 (epoch 74), batch 248247, train_loss = 1.405, time/batch = 0.117\n",
      "Sequence 2436188/3263200 (epoch 74), batch 248347, train_loss = 0.970, time/batch = 0.118\n",
      "Sequence 2437168/3263200 (epoch 74), batch 248447, train_loss = 1.052, time/batch = 0.121\n",
      "Sequence 2438158/3263200 (epoch 74), batch 248547, train_loss = 1.157, time/batch = 0.118\n",
      "Sequence 2439138/3263200 (epoch 74), batch 248647, train_loss = 1.031, time/batch = 0.117\n",
      "Sequence 2440128/3263200 (epoch 74), batch 248747, train_loss = 1.273, time/batch = 0.116\n",
      "Sequence 2441098/3263200 (epoch 74), batch 248847, train_loss = 0.863, time/batch = 0.115\n",
      "Sequence 2442078/3263200 (epoch 74), batch 248947, train_loss = 1.421, time/batch = 0.113\n",
      "Sequence 2443068/3263200 (epoch 74), batch 249047, train_loss = 0.967, time/batch = 0.116\n",
      "Sequence 2444068/3263200 (epoch 74), batch 249147, train_loss = 1.058, time/batch = 0.114\n",
      "Sequence 2445048/3263200 (epoch 74), batch 249247, train_loss = 0.739, time/batch = 0.119\n",
      "Sequence 2446048/3263200 (epoch 74), batch 249347, train_loss = 0.858, time/batch = 0.118\n",
      "Sequence 2447028/3263200 (epoch 74), batch 249447, train_loss = 1.881, time/batch = 0.119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 completed, average train loss 1.180459, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 2447990/3263200 (epoch 75), batch 249547, train_loss = 1.503, time/batch = 0.121\n",
      "Sequence 2448950/3263200 (epoch 75), batch 249647, train_loss = 0.682, time/batch = 0.120\n",
      "Sequence 2449930/3263200 (epoch 75), batch 249747, train_loss = 1.371, time/batch = 0.117\n",
      "Sequence 2450910/3263200 (epoch 75), batch 249847, train_loss = 1.182, time/batch = 0.117\n",
      "Sequence 2451880/3263200 (epoch 75), batch 249947, train_loss = 0.820, time/batch = 0.120\n",
      "Sequence 2452870/3263200 (epoch 75), batch 250047, train_loss = 1.189, time/batch = 0.119\n",
      "Sequence 2453840/3263200 (epoch 75), batch 250147, train_loss = 1.317, time/batch = 0.118\n",
      "Sequence 2454800/3263200 (epoch 75), batch 250247, train_loss = 1.404, time/batch = 0.118\n",
      "Sequence 2455750/3263200 (epoch 75), batch 250347, train_loss = 0.821, time/batch = 0.118\n",
      "Sequence 2456720/3263200 (epoch 75), batch 250447, train_loss = 1.230, time/batch = 0.117\n",
      "Sequence 2457710/3263200 (epoch 75), batch 250547, train_loss = 1.711, time/batch = 0.118\n",
      "Sequence 2458710/3263200 (epoch 75), batch 250647, train_loss = 1.267, time/batch = 0.119\n",
      "Sequence 2459690/3263200 (epoch 75), batch 250747, train_loss = 1.181, time/batch = 0.117\n",
      "Sequence 2460690/3263200 (epoch 75), batch 250847, train_loss = 0.854, time/batch = 0.120\n",
      "Sequence 2461670/3263200 (epoch 75), batch 250947, train_loss = 1.145, time/batch = 0.119\n",
      "Sequence 2462650/3263200 (epoch 75), batch 251047, train_loss = 1.173, time/batch = 0.119\n",
      "Sequence 2463640/3263200 (epoch 75), batch 251147, train_loss = 1.262, time/batch = 0.116\n",
      "Sequence 2464630/3263200 (epoch 75), batch 251247, train_loss = 1.271, time/batch = 0.120\n",
      "Sequence 2465620/3263200 (epoch 75), batch 251347, train_loss = 1.587, time/batch = 0.117\n",
      "Sequence 2466580/3263200 (epoch 75), batch 251447, train_loss = 1.055, time/batch = 0.112\n",
      "Sequence 2467550/3263200 (epoch 75), batch 251547, train_loss = 1.080, time/batch = 0.117\n",
      "Sequence 2468530/3263200 (epoch 75), batch 251647, train_loss = 0.941, time/batch = 0.116\n",
      "Sequence 2469530/3263200 (epoch 75), batch 251747, train_loss = 1.198, time/batch = 0.117\n",
      "Sequence 2470490/3263200 (epoch 75), batch 251847, train_loss = 1.124, time/batch = 0.117\n",
      "Sequence 2471480/3263200 (epoch 75), batch 251947, train_loss = 1.426, time/batch = 0.119\n",
      "Sequence 2472430/3263200 (epoch 75), batch 252047, train_loss = 1.288, time/batch = 0.117\n",
      "Sequence 2473420/3263200 (epoch 75), batch 252147, train_loss = 0.853, time/batch = 0.119\n",
      "Sequence 2474410/3263200 (epoch 75), batch 252247, train_loss = 1.234, time/batch = 0.118\n",
      "Sequence 2475410/3263200 (epoch 75), batch 252347, train_loss = 1.203, time/batch = 0.117\n",
      "Sequence 2476400/3263200 (epoch 75), batch 252447, train_loss = 1.343, time/batch = 0.117\n",
      "Sequence 2477400/3263200 (epoch 75), batch 252547, train_loss = 1.122, time/batch = 0.119\n",
      "Sequence 2478380/3263200 (epoch 75), batch 252647, train_loss = 1.036, time/batch = 0.122\n",
      "Sequence 2479370/3263200 (epoch 75), batch 252747, train_loss = 1.201, time/batch = 0.118\n",
      "Epoch 75 completed, average train loss 1.179262, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2480352/3263200 (epoch 76), batch 252847, train_loss = 0.705, time/batch = 0.122\n",
      "Sequence 2481342/3263200 (epoch 76), batch 252947, train_loss = 1.345, time/batch = 0.120\n",
      "Sequence 2482332/3263200 (epoch 76), batch 253047, train_loss = 1.492, time/batch = 0.115\n",
      "Sequence 2483322/3263200 (epoch 76), batch 253147, train_loss = 0.702, time/batch = 0.114\n",
      "Sequence 2484282/3263200 (epoch 76), batch 253247, train_loss = 1.279, time/batch = 0.117\n",
      "Sequence 2485282/3263200 (epoch 76), batch 253347, train_loss = 0.703, time/batch = 0.117\n",
      "Sequence 2486272/3263200 (epoch 76), batch 253447, train_loss = 0.998, time/batch = 0.119\n",
      "Sequence 2487242/3263200 (epoch 76), batch 253547, train_loss = 0.829, time/batch = 0.120\n",
      "Sequence 2488222/3263200 (epoch 76), batch 253647, train_loss = 0.948, time/batch = 0.121\n",
      "Sequence 2489202/3263200 (epoch 76), batch 253747, train_loss = 1.211, time/batch = 0.122\n",
      "Sequence 2490162/3263200 (epoch 76), batch 253847, train_loss = 0.885, time/batch = 0.117\n",
      "Sequence 2491152/3263200 (epoch 76), batch 253947, train_loss = 0.853, time/batch = 0.113\n",
      "Sequence 2492122/3263200 (epoch 76), batch 254047, train_loss = 1.516, time/batch = 0.116\n",
      "Sequence 2493112/3263200 (epoch 76), batch 254147, train_loss = 0.820, time/batch = 0.117\n",
      "Sequence 2494112/3263200 (epoch 76), batch 254247, train_loss = 2.036, time/batch = 0.123\n",
      "Sequence 2495052/3263200 (epoch 76), batch 254347, train_loss = 0.947, time/batch = 0.120\n",
      "Sequence 2496042/3263200 (epoch 76), batch 254447, train_loss = 0.883, time/batch = 0.119\n",
      "Sequence 2496992/3263200 (epoch 76), batch 254547, train_loss = 1.807, time/batch = 0.119\n",
      "Sequence 2497962/3263200 (epoch 76), batch 254647, train_loss = 1.490, time/batch = 0.119\n",
      "Sequence 2498942/3263200 (epoch 76), batch 254747, train_loss = 1.170, time/batch = 0.119\n",
      "Sequence 2499932/3263200 (epoch 76), batch 254847, train_loss = 1.412, time/batch = 0.121\n",
      "Sequence 2500922/3263200 (epoch 76), batch 254947, train_loss = 0.823, time/batch = 0.118\n",
      "Sequence 2501912/3263200 (epoch 76), batch 255047, train_loss = 1.019, time/batch = 0.121\n",
      "Sequence 2502892/3263200 (epoch 76), batch 255147, train_loss = 1.077, time/batch = 0.121\n",
      "Sequence 2503872/3263200 (epoch 76), batch 255247, train_loss = 0.888, time/batch = 0.121\n",
      "Sequence 2504862/3263200 (epoch 76), batch 255347, train_loss = 0.820, time/batch = 0.118\n",
      "Sequence 2505852/3263200 (epoch 76), batch 255447, train_loss = 1.749, time/batch = 0.119\n",
      "Sequence 2506852/3263200 (epoch 76), batch 255547, train_loss = 1.640, time/batch = 0.118\n",
      "Sequence 2507842/3263200 (epoch 76), batch 255647, train_loss = 1.401, time/batch = 0.120\n",
      "Sequence 2508822/3263200 (epoch 76), batch 255747, train_loss = 0.998, time/batch = 0.118\n",
      "Sequence 2509782/3263200 (epoch 76), batch 255847, train_loss = 1.711, time/batch = 0.120\n",
      "Sequence 2510742/3263200 (epoch 76), batch 255947, train_loss = 0.788, time/batch = 0.117\n",
      "Sequence 2511732/3263200 (epoch 76), batch 256047, train_loss = 1.037, time/batch = 0.117\n",
      "Epoch 76 completed, average train loss 1.178165, learning rate 0.0010\n",
      "Sequence 2512714/3263200 (epoch 77), batch 256147, train_loss = 1.243, time/batch = 0.120\n",
      "Shuffling training data...\n",
      "Sequence 2513714/3263200 (epoch 77), batch 256247, train_loss = 1.335, time/batch = 0.117\n",
      "Sequence 2514694/3263200 (epoch 77), batch 256347, train_loss = 0.995, time/batch = 0.119\n",
      "Sequence 2515674/3263200 (epoch 77), batch 256447, train_loss = 0.787, time/batch = 0.118\n",
      "Sequence 2516654/3263200 (epoch 77), batch 256547, train_loss = 0.955, time/batch = 0.120\n",
      "Sequence 2517654/3263200 (epoch 77), batch 256647, train_loss = 1.613, time/batch = 0.119\n",
      "Sequence 2518634/3263200 (epoch 77), batch 256747, train_loss = 1.619, time/batch = 0.119\n",
      "Sequence 2519594/3263200 (epoch 77), batch 256847, train_loss = 1.515, time/batch = 0.113\n",
      "Sequence 2520594/3263200 (epoch 77), batch 256947, train_loss = 0.838, time/batch = 0.115\n",
      "Sequence 2521544/3263200 (epoch 77), batch 257047, train_loss = 1.334, time/batch = 0.115\n",
      "Sequence 2522534/3263200 (epoch 77), batch 257147, train_loss = 1.636, time/batch = 0.118\n",
      "Sequence 2523524/3263200 (epoch 77), batch 257247, train_loss = 1.442, time/batch = 0.119\n",
      "Sequence 2524474/3263200 (epoch 77), batch 257347, train_loss = 0.869, time/batch = 0.122\n",
      "Sequence 2525444/3263200 (epoch 77), batch 257447, train_loss = 1.138, time/batch = 0.119\n",
      "Sequence 2526424/3263200 (epoch 77), batch 257547, train_loss = 1.392, time/batch = 0.112\n",
      "Sequence 2527404/3263200 (epoch 77), batch 257647, train_loss = 1.389, time/batch = 0.114\n",
      "Sequence 2528364/3263200 (epoch 77), batch 257747, train_loss = 1.437, time/batch = 0.118\n",
      "Sequence 2529344/3263200 (epoch 77), batch 257848, train_loss = 0.637, time/batch = 0.117\n",
      "Sequence 2530334/3263200 (epoch 77), batch 257948, train_loss = 0.830, time/batch = 0.121\n",
      "Sequence 2531314/3263200 (epoch 77), batch 258048, train_loss = 1.145, time/batch = 0.120\n",
      "Sequence 2532314/3263200 (epoch 77), batch 258148, train_loss = 0.914, time/batch = 0.119\n",
      "Sequence 2533304/3263200 (epoch 77), batch 258248, train_loss = 0.879, time/batch = 0.118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2534304/3263200 (epoch 77), batch 258348, train_loss = 1.044, time/batch = 0.118\n",
      "Sequence 2535294/3263200 (epoch 77), batch 258448, train_loss = 1.066, time/batch = 0.118\n",
      "Sequence 2536264/3263200 (epoch 77), batch 258548, train_loss = 1.570, time/batch = 0.118\n",
      "Sequence 2537244/3263200 (epoch 77), batch 258648, train_loss = 0.988, time/batch = 0.119\n",
      "Sequence 2538204/3263200 (epoch 77), batch 258748, train_loss = 1.654, time/batch = 0.120\n",
      "Sequence 2539184/3263200 (epoch 77), batch 258848, train_loss = 0.931, time/batch = 0.119\n",
      "Sequence 2540154/3263200 (epoch 77), batch 258948, train_loss = 0.767, time/batch = 0.113\n",
      "Sequence 2541144/3263200 (epoch 77), batch 259048, train_loss = 0.838, time/batch = 0.111\n",
      "Sequence 2542134/3263200 (epoch 77), batch 259148, train_loss = 0.776, time/batch = 0.117\n",
      "Sequence 2543114/3263200 (epoch 77), batch 259248, train_loss = 0.826, time/batch = 0.118\n",
      "Sequence 2544104/3263200 (epoch 77), batch 259348, train_loss = 1.029, time/batch = 0.119\n",
      "Sequence 2545094/3263200 (epoch 77), batch 259448, train_loss = 0.975, time/batch = 0.118\n",
      "Epoch 77 completed, average train loss 1.174351, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2546086/3263200 (epoch 78), batch 259548, train_loss = 0.953, time/batch = 0.118\n",
      "Sequence 2547066/3263200 (epoch 78), batch 259648, train_loss = 1.620, time/batch = 0.119\n",
      "Sequence 2548046/3263200 (epoch 78), batch 259748, train_loss = 0.672, time/batch = 0.121\n",
      "Sequence 2549026/3263200 (epoch 78), batch 259848, train_loss = 1.039, time/batch = 0.117\n",
      "Sequence 2550006/3263200 (epoch 78), batch 259948, train_loss = 1.013, time/batch = 0.118\n",
      "Sequence 2550976/3263200 (epoch 78), batch 260048, train_loss = 0.834, time/batch = 0.117\n",
      "Sequence 2551946/3263200 (epoch 78), batch 260148, train_loss = 1.170, time/batch = 0.118\n",
      "Sequence 2552926/3263200 (epoch 78), batch 260248, train_loss = 1.005, time/batch = 0.118\n",
      "Sequence 2553916/3263200 (epoch 78), batch 260348, train_loss = 1.309, time/batch = 0.119\n",
      "Sequence 2554906/3263200 (epoch 78), batch 260448, train_loss = 1.274, time/batch = 0.120\n",
      "Sequence 2555886/3263200 (epoch 78), batch 260548, train_loss = 1.030, time/batch = 0.118\n",
      "Sequence 2556866/3263200 (epoch 78), batch 260648, train_loss = 0.984, time/batch = 0.117\n",
      "Sequence 2557856/3263200 (epoch 78), batch 260748, train_loss = 0.734, time/batch = 0.113\n",
      "Sequence 2558836/3263200 (epoch 78), batch 260848, train_loss = 1.015, time/batch = 0.119\n",
      "Sequence 2559806/3263200 (epoch 78), batch 260948, train_loss = 0.989, time/batch = 0.117\n",
      "Sequence 2560786/3263200 (epoch 78), batch 261048, train_loss = 0.792, time/batch = 0.116\n",
      "Sequence 2561766/3263200 (epoch 78), batch 261148, train_loss = 1.644, time/batch = 0.116\n",
      "Sequence 2562746/3263200 (epoch 78), batch 261248, train_loss = 1.027, time/batch = 0.119\n",
      "Sequence 2563726/3263200 (epoch 78), batch 261349, train_loss = 0.438, time/batch = 0.121\n",
      "Sequence 2564716/3263200 (epoch 78), batch 261449, train_loss = 0.895, time/batch = 0.118\n",
      "Sequence 2565696/3263200 (epoch 78), batch 261549, train_loss = 0.787, time/batch = 0.117\n",
      "Sequence 2566676/3263200 (epoch 78), batch 261649, train_loss = 0.898, time/batch = 0.117\n",
      "Sequence 2567626/3263200 (epoch 78), batch 261749, train_loss = 1.289, time/batch = 0.117\n",
      "Sequence 2568616/3263200 (epoch 78), batch 261849, train_loss = 1.700, time/batch = 0.118\n",
      "Sequence 2569596/3263200 (epoch 78), batch 261949, train_loss = 1.133, time/batch = 0.118\n",
      "Sequence 2570576/3263200 (epoch 78), batch 262049, train_loss = 1.893, time/batch = 0.117\n",
      "Sequence 2571556/3263200 (epoch 78), batch 262149, train_loss = 1.309, time/batch = 0.118\n",
      "Sequence 2572556/3263200 (epoch 78), batch 262249, train_loss = 0.987, time/batch = 0.117\n",
      "Sequence 2573546/3263200 (epoch 78), batch 262349, train_loss = 1.388, time/batch = 0.119\n",
      "Sequence 2574526/3263200 (epoch 78), batch 262449, train_loss = 1.562, time/batch = 0.118\n",
      "Sequence 2575516/3263200 (epoch 78), batch 262549, train_loss = 1.246, time/batch = 0.119\n",
      "Sequence 2576496/3263200 (epoch 78), batch 262649, train_loss = 1.204, time/batch = 0.117\n",
      "Sequence 2577476/3263200 (epoch 78), batch 262749, train_loss = 1.455, time/batch = 0.119\n",
      "Epoch 78 completed, average train loss 1.171782, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2578448/3263200 (epoch 79), batch 262849, train_loss = 1.408, time/batch = 0.120\n",
      "Sequence 2579428/3263200 (epoch 79), batch 262949, train_loss = 1.433, time/batch = 0.119\n",
      "Sequence 2580398/3263200 (epoch 79), batch 263049, train_loss = 0.930, time/batch = 0.116\n",
      "Sequence 2581378/3263200 (epoch 79), batch 263149, train_loss = 1.456, time/batch = 0.118\n",
      "Sequence 2582358/3263200 (epoch 79), batch 263249, train_loss = 1.089, time/batch = 0.118\n",
      "Sequence 2583348/3263200 (epoch 79), batch 263349, train_loss = 1.185, time/batch = 0.118\n",
      "Sequence 2584338/3263200 (epoch 79), batch 263449, train_loss = 0.887, time/batch = 0.119\n",
      "Sequence 2585328/3263200 (epoch 79), batch 263549, train_loss = 1.008, time/batch = 0.120\n",
      "Sequence 2586318/3263200 (epoch 79), batch 263649, train_loss = 1.081, time/batch = 0.118\n",
      "Sequence 2587318/3263200 (epoch 79), batch 263749, train_loss = 1.076, time/batch = 0.118\n",
      "Sequence 2588308/3263200 (epoch 79), batch 263849, train_loss = 1.152, time/batch = 0.114\n",
      "Sequence 2589278/3263200 (epoch 79), batch 263949, train_loss = 0.893, time/batch = 0.111\n",
      "Sequence 2590268/3263200 (epoch 79), batch 264049, train_loss = 1.053, time/batch = 0.111\n",
      "Sequence 2591228/3263200 (epoch 79), batch 264149, train_loss = 1.233, time/batch = 0.112\n",
      "Sequence 2592218/3263200 (epoch 79), batch 264249, train_loss = 1.334, time/batch = 0.119\n",
      "Sequence 2593188/3263200 (epoch 79), batch 264349, train_loss = 1.192, time/batch = 0.118\n",
      "Sequence 2594148/3263200 (epoch 79), batch 264449, train_loss = 0.943, time/batch = 0.118\n",
      "Sequence 2595128/3263200 (epoch 79), batch 264549, train_loss = 0.989, time/batch = 0.121\n",
      "Sequence 2596118/3263200 (epoch 79), batch 264649, train_loss = 1.297, time/batch = 0.118\n",
      "Sequence 2597118/3263200 (epoch 79), batch 264749, train_loss = 1.446, time/batch = 0.120\n",
      "Sequence 2598098/3263200 (epoch 79), batch 264849, train_loss = 1.326, time/batch = 0.118\n",
      "Sequence 2599088/3263200 (epoch 79), batch 264949, train_loss = 1.818, time/batch = 0.121\n",
      "Sequence 2600048/3263200 (epoch 79), batch 265049, train_loss = 1.493, time/batch = 0.112\n",
      "Sequence 2601028/3263200 (epoch 79), batch 265149, train_loss = 1.447, time/batch = 0.118\n",
      "Sequence 2602008/3263200 (epoch 79), batch 265249, train_loss = 0.852, time/batch = 0.117\n",
      "Sequence 2602988/3263200 (epoch 79), batch 265349, train_loss = 1.061, time/batch = 0.120\n",
      "Sequence 2603968/3263200 (epoch 79), batch 265449, train_loss = 0.979, time/batch = 0.118\n",
      "Sequence 2604948/3263200 (epoch 79), batch 265549, train_loss = 0.888, time/batch = 0.121\n",
      "Sequence 2605918/3263200 (epoch 79), batch 265649, train_loss = 1.142, time/batch = 0.119\n",
      "Sequence 2606908/3263200 (epoch 79), batch 265749, train_loss = 0.978, time/batch = 0.119\n",
      "Sequence 2607898/3263200 (epoch 79), batch 265849, train_loss = 1.108, time/batch = 0.119\n",
      "Sequence 2608868/3263200 (epoch 79), batch 265949, train_loss = 1.021, time/batch = 0.118\n",
      "Sequence 2609848/3263200 (epoch 79), batch 266049, train_loss = 0.904, time/batch = 0.120\n",
      "Epoch 79 completed, average train loss 1.170031, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 2610830/3263200 (epoch 80), batch 266149, train_loss = 1.573, time/batch = 0.116\n",
      "Sequence 2611800/3263200 (epoch 80), batch 266249, train_loss = 1.002, time/batch = 0.118\n",
      "Sequence 2612780/3263200 (epoch 80), batch 266349, train_loss = 1.249, time/batch = 0.120\n",
      "Sequence 2613760/3263200 (epoch 80), batch 266449, train_loss = 0.852, time/batch = 0.119\n",
      "Sequence 2614740/3263200 (epoch 80), batch 266549, train_loss = 0.742, time/batch = 0.119\n",
      "Sequence 2615720/3263200 (epoch 80), batch 266649, train_loss = 0.900, time/batch = 0.120\n",
      "Sequence 2616710/3263200 (epoch 80), batch 266749, train_loss = 1.604, time/batch = 0.119\n",
      "Sequence 2617700/3263200 (epoch 80), batch 266849, train_loss = 1.099, time/batch = 0.117\n",
      "Sequence 2618680/3263200 (epoch 80), batch 266949, train_loss = 1.205, time/batch = 0.119\n",
      "Sequence 2619670/3263200 (epoch 80), batch 267049, train_loss = 0.820, time/batch = 0.119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2620630/3263200 (epoch 80), batch 267149, train_loss = 1.039, time/batch = 0.118\n",
      "Sequence 2621610/3263200 (epoch 80), batch 267249, train_loss = 1.364, time/batch = 0.121\n",
      "Sequence 2622580/3263200 (epoch 80), batch 267349, train_loss = 0.682, time/batch = 0.118\n",
      "Sequence 2623550/3263200 (epoch 80), batch 267449, train_loss = 0.917, time/batch = 0.111\n",
      "Sequence 2624540/3263200 (epoch 80), batch 267549, train_loss = 1.728, time/batch = 0.111\n",
      "Sequence 2625530/3263200 (epoch 80), batch 267649, train_loss = 1.240, time/batch = 0.117\n",
      "Sequence 2626500/3263200 (epoch 80), batch 267749, train_loss = 1.120, time/batch = 0.117\n",
      "Sequence 2627500/3263200 (epoch 80), batch 267849, train_loss = 1.436, time/batch = 0.119\n",
      "Sequence 2628490/3263200 (epoch 80), batch 267949, train_loss = 1.001, time/batch = 0.119\n",
      "Sequence 2629470/3263200 (epoch 80), batch 268049, train_loss = 0.898, time/batch = 0.119\n",
      "Sequence 2630430/3263200 (epoch 80), batch 268149, train_loss = 1.174, time/batch = 0.120\n",
      "Sequence 2631420/3263200 (epoch 80), batch 268249, train_loss = 1.204, time/batch = 0.117\n",
      "Sequence 2632410/3263200 (epoch 80), batch 268349, train_loss = 1.032, time/batch = 0.116\n",
      "Sequence 2633400/3263200 (epoch 80), batch 268449, train_loss = 1.765, time/batch = 0.119\n",
      "Sequence 2634380/3263200 (epoch 80), batch 268549, train_loss = 1.488, time/batch = 0.118\n",
      "Sequence 2635340/3263200 (epoch 80), batch 268649, train_loss = 0.739, time/batch = 0.120\n",
      "Sequence 2636310/3263200 (epoch 80), batch 268749, train_loss = 0.713, time/batch = 0.119\n",
      "Sequence 2637300/3263200 (epoch 80), batch 268849, train_loss = 1.314, time/batch = 0.118\n",
      "Sequence 2638290/3263200 (epoch 80), batch 268949, train_loss = 1.449, time/batch = 0.122\n",
      "Sequence 2639280/3263200 (epoch 80), batch 269049, train_loss = 1.630, time/batch = 0.120\n",
      "Sequence 2640240/3263200 (epoch 80), batch 269149, train_loss = 0.870, time/batch = 0.116\n",
      "Sequence 2641220/3263200 (epoch 80), batch 269249, train_loss = 1.845, time/batch = 0.115\n",
      "Sequence 2642210/3263200 (epoch 80), batch 269349, train_loss = 0.993, time/batch = 0.116\n",
      "Epoch 80 completed, average train loss 1.168665, learning rate 0.0010\n",
      "Sequence 2643192/3263200 (epoch 81), batch 269449, train_loss = 1.008, time/batch = 0.119\n",
      "Shuffling training data...\n",
      "Sequence 2644172/3263200 (epoch 81), batch 269549, train_loss = 1.288, time/batch = 0.121\n",
      "Sequence 2645162/3263200 (epoch 81), batch 269649, train_loss = 0.991, time/batch = 0.120\n",
      "Sequence 2646142/3263200 (epoch 81), batch 269749, train_loss = 1.130, time/batch = 0.119\n",
      "Sequence 2647122/3263200 (epoch 81), batch 269849, train_loss = 1.326, time/batch = 0.122\n",
      "Sequence 2648102/3263200 (epoch 81), batch 269949, train_loss = 1.038, time/batch = 0.120\n",
      "Sequence 2649042/3263200 (epoch 81), batch 270049, train_loss = 2.578, time/batch = 0.119\n",
      "Sequence 2650032/3263200 (epoch 81), batch 270149, train_loss = 1.310, time/batch = 0.120\n",
      "Sequence 2651012/3263200 (epoch 81), batch 270249, train_loss = 1.334, time/batch = 0.118\n",
      "Sequence 2651982/3263200 (epoch 81), batch 270349, train_loss = 0.823, time/batch = 0.118\n",
      "Sequence 2652972/3263200 (epoch 81), batch 270449, train_loss = 1.045, time/batch = 0.118\n",
      "Sequence 2653952/3263200 (epoch 81), batch 270549, train_loss = 1.082, time/batch = 0.119\n",
      "Sequence 2654932/3263200 (epoch 81), batch 270649, train_loss = 0.957, time/batch = 0.117\n",
      "Sequence 2655922/3263200 (epoch 81), batch 270749, train_loss = 1.401, time/batch = 0.115\n",
      "Sequence 2656892/3263200 (epoch 81), batch 270849, train_loss = 1.420, time/batch = 0.114\n",
      "Sequence 2657872/3263200 (epoch 81), batch 270949, train_loss = 1.209, time/batch = 0.118\n",
      "Sequence 2658852/3263200 (epoch 81), batch 271049, train_loss = 0.888, time/batch = 0.118\n",
      "Sequence 2659822/3263200 (epoch 81), batch 271149, train_loss = 0.791, time/batch = 0.119\n",
      "Sequence 2660822/3263200 (epoch 81), batch 271249, train_loss = 1.236, time/batch = 0.120\n",
      "Sequence 2661812/3263200 (epoch 81), batch 271349, train_loss = 1.348, time/batch = 0.118\n",
      "Sequence 2662782/3263200 (epoch 81), batch 271449, train_loss = 0.760, time/batch = 0.117\n",
      "Sequence 2663772/3263200 (epoch 81), batch 271549, train_loss = 1.175, time/batch = 0.116\n",
      "Sequence 2664722/3263200 (epoch 81), batch 271649, train_loss = 0.923, time/batch = 0.118\n",
      "Sequence 2665712/3263200 (epoch 81), batch 271749, train_loss = 0.919, time/batch = 0.117\n",
      "Sequence 2666702/3263200 (epoch 81), batch 271849, train_loss = 0.780, time/batch = 0.117\n",
      "Sequence 2667702/3263200 (epoch 81), batch 271949, train_loss = 1.165, time/batch = 0.120\n",
      "Sequence 2668692/3263200 (epoch 81), batch 272049, train_loss = 1.207, time/batch = 0.119\n",
      "Sequence 2669692/3263200 (epoch 81), batch 272149, train_loss = 1.390, time/batch = 0.118\n",
      "Sequence 2670682/3263200 (epoch 81), batch 272249, train_loss = 1.665, time/batch = 0.116\n",
      "Sequence 2671672/3263200 (epoch 81), batch 272349, train_loss = 0.893, time/batch = 0.117\n",
      "Sequence 2672662/3263200 (epoch 81), batch 272449, train_loss = 1.379, time/batch = 0.120\n",
      "Sequence 2673652/3263200 (epoch 81), batch 272549, train_loss = 1.352, time/batch = 0.121\n",
      "Sequence 2674592/3263200 (epoch 81), batch 272649, train_loss = 1.237, time/batch = 0.118\n",
      "Sequence 2675562/3263200 (epoch 81), batch 272749, train_loss = 0.937, time/batch = 0.118\n",
      "Epoch 81 completed, average train loss 1.167458, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2676534/3263200 (epoch 82), batch 272849, train_loss = 0.991, time/batch = 0.119\n",
      "Sequence 2677504/3263200 (epoch 82), batch 272949, train_loss = 1.298, time/batch = 0.119\n",
      "Sequence 2678494/3263200 (epoch 82), batch 273049, train_loss = 1.257, time/batch = 0.120\n",
      "Sequence 2679464/3263200 (epoch 82), batch 273149, train_loss = 1.407, time/batch = 0.119\n",
      "Sequence 2680444/3263200 (epoch 82), batch 273249, train_loss = 1.153, time/batch = 0.119\n",
      "Sequence 2681404/3263200 (epoch 82), batch 273349, train_loss = 1.027, time/batch = 0.117\n",
      "Sequence 2682384/3263200 (epoch 82), batch 273449, train_loss = 1.866, time/batch = 0.120\n",
      "Sequence 2683374/3263200 (epoch 82), batch 273549, train_loss = 1.146, time/batch = 0.119\n",
      "Sequence 2684374/3263200 (epoch 82), batch 273649, train_loss = 1.491, time/batch = 0.115\n",
      "Sequence 2685374/3263200 (epoch 82), batch 273749, train_loss = 1.079, time/batch = 0.110\n",
      "Sequence 2686344/3263200 (epoch 82), batch 273849, train_loss = 1.404, time/batch = 0.113\n",
      "Sequence 2687344/3263200 (epoch 82), batch 273949, train_loss = 1.358, time/batch = 0.111\n",
      "Sequence 2688314/3263200 (epoch 82), batch 274049, train_loss = 1.305, time/batch = 0.120\n",
      "Sequence 2689284/3263200 (epoch 82), batch 274149, train_loss = 1.121, time/batch = 0.120\n",
      "Sequence 2690264/3263200 (epoch 82), batch 274249, train_loss = 0.773, time/batch = 0.120\n",
      "Sequence 2691234/3263200 (epoch 82), batch 274349, train_loss = 1.038, time/batch = 0.120\n",
      "Sequence 2692204/3263200 (epoch 82), batch 274449, train_loss = 1.521, time/batch = 0.118\n",
      "Sequence 2693184/3263200 (epoch 82), batch 274549, train_loss = 1.506, time/batch = 0.117\n",
      "Sequence 2694174/3263200 (epoch 82), batch 274649, train_loss = 1.037, time/batch = 0.120\n",
      "Sequence 2695164/3263200 (epoch 82), batch 274749, train_loss = 0.989, time/batch = 0.118\n",
      "Sequence 2696154/3263200 (epoch 82), batch 274849, train_loss = 1.440, time/batch = 0.117\n",
      "Sequence 2697134/3263200 (epoch 82), batch 274949, train_loss = 1.488, time/batch = 0.117\n",
      "Sequence 2698124/3263200 (epoch 82), batch 275049, train_loss = 0.847, time/batch = 0.119\n",
      "Sequence 2699114/3263200 (epoch 82), batch 275149, train_loss = 1.088, time/batch = 0.116\n",
      "Sequence 2700104/3263200 (epoch 82), batch 275249, train_loss = 1.201, time/batch = 0.117\n",
      "Sequence 2701064/3263200 (epoch 82), batch 275349, train_loss = 1.230, time/batch = 0.118\n",
      "Sequence 2702064/3263200 (epoch 82), batch 275449, train_loss = 1.150, time/batch = 0.120\n",
      "Sequence 2703054/3263200 (epoch 82), batch 275549, train_loss = 1.192, time/batch = 0.119\n",
      "Sequence 2704044/3263200 (epoch 82), batch 275649, train_loss = 1.469, time/batch = 0.118\n",
      "Sequence 2705034/3263200 (epoch 82), batch 275749, train_loss = 0.964, time/batch = 0.120\n",
      "Sequence 2706014/3263200 (epoch 82), batch 275849, train_loss = 1.248, time/batch = 0.118\n",
      "Sequence 2707004/3263200 (epoch 82), batch 275949, train_loss = 0.820, time/batch = 0.123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2707954/3263200 (epoch 82), batch 276049, train_loss = 0.886, time/batch = 0.122\n",
      "Epoch 82 completed, average train loss 1.164223, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2708936/3263200 (epoch 83), batch 276149, train_loss = 1.145, time/batch = 0.117\n",
      "Sequence 2709926/3263200 (epoch 83), batch 276249, train_loss = 1.189, time/batch = 0.119\n",
      "Sequence 2710916/3263200 (epoch 83), batch 276349, train_loss = 0.874, time/batch = 0.118\n",
      "Sequence 2711886/3263200 (epoch 83), batch 276449, train_loss = 0.964, time/batch = 0.120\n",
      "Sequence 2712856/3263200 (epoch 83), batch 276549, train_loss = 1.266, time/batch = 0.120\n",
      "Sequence 2713836/3263200 (epoch 83), batch 276649, train_loss = 1.102, time/batch = 0.118\n",
      "Sequence 2714816/3263200 (epoch 83), batch 276749, train_loss = 0.971, time/batch = 0.120\n",
      "Sequence 2715796/3263200 (epoch 83), batch 276849, train_loss = 1.128, time/batch = 0.119\n",
      "Sequence 2716786/3263200 (epoch 83), batch 276949, train_loss = 0.853, time/batch = 0.118\n",
      "Sequence 2717766/3263200 (epoch 83), batch 277049, train_loss = 1.294, time/batch = 0.118\n",
      "Sequence 2718736/3263200 (epoch 83), batch 277149, train_loss = 1.186, time/batch = 0.120\n",
      "Sequence 2719726/3263200 (epoch 83), batch 277249, train_loss = 2.100, time/batch = 0.117\n",
      "Sequence 2720716/3263200 (epoch 83), batch 277349, train_loss = 0.926, time/batch = 0.115\n",
      "Sequence 2721686/3263200 (epoch 83), batch 277449, train_loss = 1.220, time/batch = 0.114\n",
      "Sequence 2722606/3263200 (epoch 83), batch 277549, train_loss = 1.132, time/batch = 0.116\n",
      "Sequence 2723606/3263200 (epoch 83), batch 277649, train_loss = 0.842, time/batch = 0.120\n",
      "Sequence 2724596/3263200 (epoch 83), batch 277749, train_loss = 1.027, time/batch = 0.121\n",
      "Sequence 2725576/3263200 (epoch 83), batch 277849, train_loss = 1.339, time/batch = 0.119\n",
      "Sequence 2726576/3263200 (epoch 83), batch 277949, train_loss = 2.369, time/batch = 0.118\n",
      "Sequence 2727556/3263200 (epoch 83), batch 278049, train_loss = 0.859, time/batch = 0.119\n",
      "Sequence 2728516/3263200 (epoch 83), batch 278149, train_loss = 1.271, time/batch = 0.118\n",
      "Sequence 2729486/3263200 (epoch 83), batch 278249, train_loss = 1.607, time/batch = 0.119\n",
      "Sequence 2730486/3263200 (epoch 83), batch 278349, train_loss = 1.245, time/batch = 0.109\n",
      "Sequence 2731466/3263200 (epoch 83), batch 278449, train_loss = 0.966, time/batch = 0.111\n",
      "Sequence 2732466/3263200 (epoch 83), batch 278549, train_loss = 0.797, time/batch = 0.114\n",
      "Sequence 2733456/3263200 (epoch 83), batch 278649, train_loss = 1.216, time/batch = 0.118\n",
      "Sequence 2734456/3263200 (epoch 83), batch 278749, train_loss = 0.958, time/batch = 0.117\n",
      "Sequence 2735436/3263200 (epoch 83), batch 278849, train_loss = 1.064, time/batch = 0.118\n",
      "Sequence 2736406/3263200 (epoch 83), batch 278949, train_loss = 0.662, time/batch = 0.118\n",
      "Sequence 2737386/3263200 (epoch 83), batch 279049, train_loss = 1.134, time/batch = 0.119\n",
      "Sequence 2738366/3263200 (epoch 83), batch 279149, train_loss = 1.016, time/batch = 0.119\n",
      "Sequence 2739336/3263200 (epoch 83), batch 279249, train_loss = 1.036, time/batch = 0.119\n",
      "Sequence 2740326/3263200 (epoch 83), batch 279349, train_loss = 0.715, time/batch = 0.112\n",
      "Epoch 83 completed, average train loss 1.161868, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2741288/3263200 (epoch 84), batch 279449, train_loss = 1.051, time/batch = 0.118\n",
      "Sequence 2742278/3263200 (epoch 84), batch 279549, train_loss = 1.838, time/batch = 0.119\n",
      "Sequence 2743228/3263200 (epoch 84), batch 279649, train_loss = 1.139, time/batch = 0.112\n",
      "Sequence 2744198/3263200 (epoch 84), batch 279749, train_loss = 1.371, time/batch = 0.117\n",
      "Sequence 2745188/3263200 (epoch 84), batch 279849, train_loss = 1.129, time/batch = 0.115\n",
      "Sequence 2746158/3263200 (epoch 84), batch 279949, train_loss = 1.040, time/batch = 0.120\n",
      "Sequence 2747148/3263200 (epoch 84), batch 280049, train_loss = 2.063, time/batch = 0.118\n",
      "Sequence 2748148/3263200 (epoch 84), batch 280149, train_loss = 1.109, time/batch = 0.119\n",
      "Sequence 2749128/3263200 (epoch 84), batch 280249, train_loss = 1.022, time/batch = 0.119\n",
      "Sequence 2750108/3263200 (epoch 84), batch 280349, train_loss = 1.068, time/batch = 0.118\n",
      "Sequence 2751108/3263200 (epoch 84), batch 280449, train_loss = 1.305, time/batch = 0.116\n",
      "Sequence 2752088/3263200 (epoch 84), batch 280549, train_loss = 1.396, time/batch = 0.118\n",
      "Sequence 2753068/3263200 (epoch 84), batch 280649, train_loss = 1.064, time/batch = 0.116\n",
      "Sequence 2754038/3263200 (epoch 84), batch 280749, train_loss = 1.062, time/batch = 0.120\n",
      "Sequence 2755018/3263200 (epoch 84), batch 280849, train_loss = 0.842, time/batch = 0.120\n",
      "Sequence 2756008/3263200 (epoch 84), batch 280949, train_loss = 1.081, time/batch = 0.118\n",
      "Sequence 2756968/3263200 (epoch 84), batch 281050, train_loss = 0.506, time/batch = 0.120\n",
      "Sequence 2757958/3263200 (epoch 84), batch 281150, train_loss = 0.936, time/batch = 0.118\n",
      "Sequence 2758948/3263200 (epoch 84), batch 281250, train_loss = 0.928, time/batch = 0.118\n",
      "Sequence 2759938/3263200 (epoch 84), batch 281350, train_loss = 1.171, time/batch = 0.119\n",
      "Sequence 2760928/3263200 (epoch 84), batch 281450, train_loss = 0.869, time/batch = 0.114\n",
      "Sequence 2761908/3263200 (epoch 84), batch 281550, train_loss = 1.250, time/batch = 0.110\n",
      "Sequence 2762898/3263200 (epoch 84), batch 281650, train_loss = 1.299, time/batch = 0.110\n",
      "Sequence 2763898/3263200 (epoch 84), batch 281750, train_loss = 1.309, time/batch = 0.111\n",
      "Sequence 2764858/3263200 (epoch 84), batch 281850, train_loss = 1.081, time/batch = 0.119\n",
      "Sequence 2765848/3263200 (epoch 84), batch 281951, train_loss = 1.023, time/batch = 0.120\n",
      "Sequence 2766848/3263200 (epoch 84), batch 282051, train_loss = 1.019, time/batch = 0.117\n",
      "Sequence 2767838/3263200 (epoch 84), batch 282151, train_loss = 1.350, time/batch = 0.117\n",
      "Sequence 2768808/3263200 (epoch 84), batch 282251, train_loss = 1.016, time/batch = 0.119\n",
      "Sequence 2769758/3263200 (epoch 84), batch 282351, train_loss = 1.273, time/batch = 0.120\n",
      "Sequence 2770748/3263200 (epoch 84), batch 282451, train_loss = 0.845, time/batch = 0.117\n",
      "Sequence 2771718/3263200 (epoch 84), batch 282551, train_loss = 1.378, time/batch = 0.119\n",
      "Sequence 2772718/3263200 (epoch 84), batch 282651, train_loss = 1.254, time/batch = 0.119\n",
      "Sequence 2773688/3263200 (epoch 84), batch 282751, train_loss = 0.968, time/batch = 0.118\n",
      "Epoch 84 completed, average train loss 1.159161, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 2774670/3263200 (epoch 85), batch 282851, train_loss = 1.272, time/batch = 0.114\n",
      "Sequence 2775660/3263200 (epoch 85), batch 282951, train_loss = 1.100, time/batch = 0.113\n",
      "Sequence 2776650/3263200 (epoch 85), batch 283051, train_loss = 0.953, time/batch = 0.118\n",
      "Sequence 2777600/3263200 (epoch 85), batch 283151, train_loss = 1.374, time/batch = 0.118\n",
      "Sequence 2778570/3263200 (epoch 85), batch 283251, train_loss = 1.902, time/batch = 0.121\n",
      "Sequence 2779570/3263200 (epoch 85), batch 283351, train_loss = 1.419, time/batch = 0.119\n",
      "Sequence 2780540/3263200 (epoch 85), batch 283451, train_loss = 0.949, time/batch = 0.118\n",
      "Sequence 2781520/3263200 (epoch 85), batch 283551, train_loss = 0.850, time/batch = 0.117\n",
      "Sequence 2782510/3263200 (epoch 85), batch 283651, train_loss = 0.996, time/batch = 0.119\n",
      "Sequence 2783490/3263200 (epoch 85), batch 283751, train_loss = 1.952, time/batch = 0.117\n",
      "Sequence 2784490/3263200 (epoch 85), batch 283851, train_loss = 1.032, time/batch = 0.118\n",
      "Sequence 2785480/3263200 (epoch 85), batch 283951, train_loss = 0.881, time/batch = 0.118\n",
      "Sequence 2786460/3263200 (epoch 85), batch 284052, train_loss = 0.849, time/batch = 0.117\n",
      "Sequence 2787440/3263200 (epoch 85), batch 284152, train_loss = 1.249, time/batch = 0.120\n",
      "Sequence 2788440/3263200 (epoch 85), batch 284252, train_loss = 1.405, time/batch = 0.118\n",
      "Sequence 2789440/3263200 (epoch 85), batch 284352, train_loss = 0.755, time/batch = 0.118\n",
      "Sequence 2790410/3263200 (epoch 85), batch 284452, train_loss = 1.254, time/batch = 0.114\n",
      "Sequence 2791380/3263200 (epoch 85), batch 284552, train_loss = 1.557, time/batch = 0.117\n",
      "Sequence 2792360/3263200 (epoch 85), batch 284652, train_loss = 1.065, time/batch = 0.118\n",
      "Sequence 2793340/3263200 (epoch 85), batch 284752, train_loss = 1.312, time/batch = 0.120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2794300/3263200 (epoch 85), batch 284852, train_loss = 1.257, time/batch = 0.118\n",
      "Sequence 2795290/3263200 (epoch 85), batch 284952, train_loss = 1.003, time/batch = 0.120\n",
      "Sequence 2796280/3263200 (epoch 85), batch 285052, train_loss = 1.066, time/batch = 0.118\n",
      "Sequence 2797260/3263200 (epoch 85), batch 285152, train_loss = 0.971, time/batch = 0.122\n",
      "Sequence 2798250/3263200 (epoch 85), batch 285252, train_loss = 1.134, time/batch = 0.120\n",
      "Sequence 2799220/3263200 (epoch 85), batch 285352, train_loss = 1.207, time/batch = 0.120\n",
      "Sequence 2800180/3263200 (epoch 85), batch 285453, train_loss = 1.079, time/batch = 0.117\n",
      "Sequence 2801170/3263200 (epoch 85), batch 285553, train_loss = 1.183, time/batch = 0.113\n",
      "Sequence 2802150/3263200 (epoch 85), batch 285653, train_loss = 0.744, time/batch = 0.110\n",
      "Sequence 2803120/3263200 (epoch 85), batch 285753, train_loss = 1.229, time/batch = 0.116\n",
      "Sequence 2804120/3263200 (epoch 85), batch 285853, train_loss = 1.418, time/batch = 0.120\n",
      "Sequence 2805100/3263200 (epoch 85), batch 285953, train_loss = 1.019, time/batch = 0.120\n",
      "Sequence 2806080/3263200 (epoch 85), batch 286053, train_loss = 1.378, time/batch = 0.118\n",
      "Epoch 85 completed, average train loss 1.157621, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2807062/3263200 (epoch 86), batch 286153, train_loss = 1.086, time/batch = 0.119\n",
      "Sequence 2808062/3263200 (epoch 86), batch 286253, train_loss = 0.889, time/batch = 0.120\n",
      "Sequence 2809052/3263200 (epoch 86), batch 286353, train_loss = 1.117, time/batch = 0.119\n",
      "Sequence 2810032/3263200 (epoch 86), batch 286453, train_loss = 0.938, time/batch = 0.120\n",
      "Sequence 2811032/3263200 (epoch 86), batch 286553, train_loss = 0.743, time/batch = 0.115\n",
      "Sequence 2812002/3263200 (epoch 86), batch 286653, train_loss = 0.988, time/batch = 0.120\n",
      "Sequence 2812992/3263200 (epoch 86), batch 286753, train_loss = 1.383, time/batch = 0.116\n",
      "Sequence 2813972/3263200 (epoch 86), batch 286853, train_loss = 0.892, time/batch = 0.118\n",
      "Sequence 2814952/3263200 (epoch 86), batch 286953, train_loss = 0.907, time/batch = 0.121\n",
      "Sequence 2815922/3263200 (epoch 86), batch 287053, train_loss = 1.290, time/batch = 0.121\n",
      "Sequence 2816892/3263200 (epoch 86), batch 287153, train_loss = 0.799, time/batch = 0.119\n",
      "Sequence 2817852/3263200 (epoch 86), batch 287253, train_loss = 1.333, time/batch = 0.120\n",
      "Sequence 2818842/3263200 (epoch 86), batch 287353, train_loss = 0.990, time/batch = 0.118\n",
      "Sequence 2819832/3263200 (epoch 86), batch 287453, train_loss = 0.731, time/batch = 0.117\n",
      "Sequence 2820812/3263200 (epoch 86), batch 287553, train_loss = 1.179, time/batch = 0.121\n",
      "Sequence 2821802/3263200 (epoch 86), batch 287653, train_loss = 1.070, time/batch = 0.119\n",
      "Sequence 2822792/3263200 (epoch 86), batch 287753, train_loss = 0.987, time/batch = 0.120\n",
      "Sequence 2823772/3263200 (epoch 86), batch 287853, train_loss = 1.315, time/batch = 0.118\n",
      "Sequence 2824762/3263200 (epoch 86), batch 287953, train_loss = 0.840, time/batch = 0.118\n",
      "Sequence 2825732/3263200 (epoch 86), batch 288053, train_loss = 1.034, time/batch = 0.117\n",
      "Sequence 2826712/3263200 (epoch 86), batch 288153, train_loss = 1.167, time/batch = 0.112\n",
      "Sequence 2827692/3263200 (epoch 86), batch 288253, train_loss = 1.243, time/batch = 0.111\n",
      "Sequence 2828662/3263200 (epoch 86), batch 288353, train_loss = 1.016, time/batch = 0.111\n",
      "Sequence 2829642/3263200 (epoch 86), batch 288453, train_loss = 1.099, time/batch = 0.116\n",
      "Sequence 2830622/3263200 (epoch 86), batch 288554, train_loss = 0.778, time/batch = 0.122\n",
      "Sequence 2831582/3263200 (epoch 86), batch 288654, train_loss = 1.283, time/batch = 0.119\n",
      "Sequence 2832562/3263200 (epoch 86), batch 288754, train_loss = 0.896, time/batch = 0.119\n",
      "Sequence 2833552/3263200 (epoch 86), batch 288854, train_loss = 1.238, time/batch = 0.120\n",
      "Sequence 2834532/3263200 (epoch 86), batch 288954, train_loss = 1.029, time/batch = 0.120\n",
      "Sequence 2835522/3263200 (epoch 86), batch 289054, train_loss = 0.899, time/batch = 0.115\n",
      "Sequence 2836512/3263200 (epoch 86), batch 289154, train_loss = 1.482, time/batch = 0.118\n",
      "Sequence 2837492/3263200 (epoch 86), batch 289254, train_loss = 0.964, time/batch = 0.120\n",
      "Sequence 2838432/3263200 (epoch 86), batch 289354, train_loss = 1.472, time/batch = 0.119\n",
      "Epoch 86 completed, average train loss 1.156902, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2839434/3263200 (epoch 87), batch 289454, train_loss = 1.467, time/batch = 0.119\n",
      "Sequence 2840414/3263200 (epoch 87), batch 289554, train_loss = 0.958, time/batch = 0.119\n",
      "Sequence 2841394/3263200 (epoch 87), batch 289654, train_loss = 0.826, time/batch = 0.121\n",
      "Sequence 2842394/3263200 (epoch 87), batch 289754, train_loss = 1.124, time/batch = 0.118\n",
      "Sequence 2843384/3263200 (epoch 87), batch 289854, train_loss = 1.132, time/batch = 0.118\n",
      "Sequence 2844374/3263200 (epoch 87), batch 289954, train_loss = 0.848, time/batch = 0.117\n",
      "Sequence 2845354/3263200 (epoch 87), batch 290054, train_loss = 1.058, time/batch = 0.116\n",
      "Sequence 2846344/3263200 (epoch 87), batch 290154, train_loss = 0.910, time/batch = 0.118\n",
      "Sequence 2847334/3263200 (epoch 87), batch 290254, train_loss = 1.307, time/batch = 0.120\n",
      "Sequence 2848294/3263200 (epoch 87), batch 290354, train_loss = 1.424, time/batch = 0.119\n",
      "Sequence 2849254/3263200 (epoch 87), batch 290454, train_loss = 1.060, time/batch = 0.117\n",
      "Sequence 2850234/3263200 (epoch 87), batch 290554, train_loss = 1.882, time/batch = 0.118\n",
      "Sequence 2851204/3263200 (epoch 87), batch 290654, train_loss = 1.044, time/batch = 0.120\n",
      "Sequence 2852174/3263200 (epoch 87), batch 290754, train_loss = 1.251, time/batch = 0.118\n",
      "Sequence 2853154/3263200 (epoch 87), batch 290854, train_loss = 1.580, time/batch = 0.117\n",
      "Sequence 2854144/3263200 (epoch 87), batch 290954, train_loss = 0.940, time/batch = 0.119\n",
      "Sequence 2855124/3263200 (epoch 87), batch 291054, train_loss = 1.578, time/batch = 0.117\n",
      "Sequence 2856094/3263200 (epoch 87), batch 291154, train_loss = 0.894, time/batch = 0.118\n",
      "Sequence 2857044/3263200 (epoch 87), batch 291254, train_loss = 1.239, time/batch = 0.116\n",
      "Sequence 2858014/3263200 (epoch 87), batch 291354, train_loss = 0.728, time/batch = 0.119\n",
      "Sequence 2858994/3263200 (epoch 87), batch 291454, train_loss = 1.238, time/batch = 0.117\n",
      "Sequence 2859984/3263200 (epoch 87), batch 291554, train_loss = 0.988, time/batch = 0.118\n",
      "Sequence 2860964/3263200 (epoch 87), batch 291654, train_loss = 1.443, time/batch = 0.118\n",
      "Sequence 2861954/3263200 (epoch 87), batch 291754, train_loss = 1.592, time/batch = 0.122\n",
      "Sequence 2862934/3263200 (epoch 87), batch 291854, train_loss = 1.355, time/batch = 0.117\n",
      "Sequence 2863914/3263200 (epoch 87), batch 291954, train_loss = 2.164, time/batch = 0.120\n",
      "Sequence 2864894/3263200 (epoch 87), batch 292054, train_loss = 0.984, time/batch = 0.119\n",
      "Sequence 2865854/3263200 (epoch 87), batch 292154, train_loss = 1.234, time/batch = 0.118\n",
      "Sequence 2866854/3263200 (epoch 87), batch 292254, train_loss = 1.182, time/batch = 0.119\n",
      "Sequence 2867844/3263200 (epoch 87), batch 292354, train_loss = 1.396, time/batch = 0.118\n",
      "Sequence 2868834/3263200 (epoch 87), batch 292454, train_loss = 2.067, time/batch = 0.119\n",
      "Sequence 2869804/3263200 (epoch 87), batch 292554, train_loss = 0.650, time/batch = 0.117\n",
      "Sequence 2870794/3263200 (epoch 87), batch 292654, train_loss = 0.867, time/batch = 0.118\n",
      "Epoch 87 completed, average train loss 1.153884, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2871796/3263200 (epoch 88), batch 292754, train_loss = 1.069, time/batch = 0.116\n",
      "Sequence 2872766/3263200 (epoch 88), batch 292854, train_loss = 1.055, time/batch = 0.116\n",
      "Sequence 2873756/3263200 (epoch 88), batch 292954, train_loss = 0.972, time/batch = 0.117\n",
      "Sequence 2874746/3263200 (epoch 88), batch 293054, train_loss = 1.416, time/batch = 0.120\n",
      "Sequence 2875726/3263200 (epoch 88), batch 293154, train_loss = 0.970, time/batch = 0.121\n",
      "Sequence 2876716/3263200 (epoch 88), batch 293254, train_loss = 1.074, time/batch = 0.118\n",
      "Sequence 2877696/3263200 (epoch 88), batch 293354, train_loss = 1.135, time/batch = 0.119\n",
      "Sequence 2878696/3263200 (epoch 88), batch 293454, train_loss = 0.985, time/batch = 0.118\n",
      "Sequence 2879676/3263200 (epoch 88), batch 293554, train_loss = 1.688, time/batch = 0.118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2880666/3263200 (epoch 88), batch 293654, train_loss = 0.823, time/batch = 0.117\n",
      "Sequence 2881656/3263200 (epoch 88), batch 293754, train_loss = 0.774, time/batch = 0.119\n",
      "Sequence 2882646/3263200 (epoch 88), batch 293854, train_loss = 1.170, time/batch = 0.120\n",
      "Sequence 2883646/3263200 (epoch 88), batch 293954, train_loss = 1.365, time/batch = 0.119\n",
      "Sequence 2884636/3263200 (epoch 88), batch 294054, train_loss = 1.106, time/batch = 0.118\n",
      "Sequence 2885626/3263200 (epoch 88), batch 294154, train_loss = 1.044, time/batch = 0.122\n",
      "Sequence 2886616/3263200 (epoch 88), batch 294254, train_loss = 2.150, time/batch = 0.120\n",
      "Sequence 2887606/3263200 (epoch 88), batch 294354, train_loss = 0.897, time/batch = 0.119\n",
      "Sequence 2888596/3263200 (epoch 88), batch 294454, train_loss = 1.293, time/batch = 0.117\n",
      "Sequence 2889576/3263200 (epoch 88), batch 294554, train_loss = 1.001, time/batch = 0.120\n",
      "Sequence 2890536/3263200 (epoch 88), batch 294654, train_loss = 1.433, time/batch = 0.120\n",
      "Sequence 2891526/3263200 (epoch 88), batch 294754, train_loss = 1.313, time/batch = 0.120\n",
      "Sequence 2892516/3263200 (epoch 88), batch 294854, train_loss = 1.051, time/batch = 0.119\n",
      "Sequence 2893496/3263200 (epoch 88), batch 294954, train_loss = 1.214, time/batch = 0.119\n",
      "Sequence 2894486/3263200 (epoch 88), batch 295054, train_loss = 1.600, time/batch = 0.117\n",
      "Sequence 2895456/3263200 (epoch 88), batch 295154, train_loss = 1.127, time/batch = 0.120\n",
      "Sequence 2896396/3263200 (epoch 88), batch 295254, train_loss = 0.756, time/batch = 0.119\n",
      "Sequence 2897386/3263200 (epoch 88), batch 295354, train_loss = 0.817, time/batch = 0.120\n",
      "Sequence 2898376/3263200 (epoch 88), batch 295454, train_loss = 0.941, time/batch = 0.120\n",
      "Sequence 2899326/3263200 (epoch 88), batch 295554, train_loss = 1.062, time/batch = 0.118\n",
      "Sequence 2900286/3263200 (epoch 88), batch 295654, train_loss = 1.413, time/batch = 0.120\n",
      "Sequence 2901236/3263200 (epoch 88), batch 295754, train_loss = 1.093, time/batch = 0.121\n",
      "Sequence 2902216/3263200 (epoch 88), batch 295854, train_loss = 1.117, time/batch = 0.120\n",
      "Sequence 2903196/3263200 (epoch 88), batch 295954, train_loss = 0.926, time/batch = 0.118\n",
      "Sequence 2904156/3263200 (epoch 88), batch 296054, train_loss = 0.796, time/batch = 0.119\n",
      "Epoch 88 completed, average train loss 1.151664, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2905158/3263200 (epoch 89), batch 296154, train_loss = 0.753, time/batch = 0.120\n",
      "Sequence 2906138/3263200 (epoch 89), batch 296254, train_loss = 0.666, time/batch = 0.118\n",
      "Sequence 2907118/3263200 (epoch 89), batch 296354, train_loss = 0.826, time/batch = 0.121\n",
      "Sequence 2908088/3263200 (epoch 89), batch 296454, train_loss = 1.366, time/batch = 0.120\n",
      "Sequence 2909018/3263200 (epoch 89), batch 296554, train_loss = 0.902, time/batch = 0.118\n",
      "Sequence 2910008/3263200 (epoch 89), batch 296654, train_loss = 1.148, time/batch = 0.112\n",
      "Sequence 2910988/3263200 (epoch 89), batch 296754, train_loss = 1.024, time/batch = 0.111\n",
      "Sequence 2911978/3263200 (epoch 89), batch 296854, train_loss = 1.134, time/batch = 0.111\n",
      "Sequence 2912968/3263200 (epoch 89), batch 296954, train_loss = 1.285, time/batch = 0.111\n",
      "Sequence 2913958/3263200 (epoch 89), batch 297054, train_loss = 1.426, time/batch = 0.115\n",
      "Sequence 2914948/3263200 (epoch 89), batch 297154, train_loss = 1.593, time/batch = 0.121\n",
      "Sequence 2915948/3263200 (epoch 89), batch 297254, train_loss = 0.911, time/batch = 0.118\n",
      "Sequence 2916938/3263200 (epoch 89), batch 297354, train_loss = 0.776, time/batch = 0.119\n",
      "Sequence 2917898/3263200 (epoch 89), batch 297454, train_loss = 1.062, time/batch = 0.119\n",
      "Sequence 2918878/3263200 (epoch 89), batch 297554, train_loss = 1.308, time/batch = 0.115\n",
      "Sequence 2919848/3263200 (epoch 89), batch 297654, train_loss = 0.953, time/batch = 0.116\n",
      "Sequence 2920838/3263200 (epoch 89), batch 297754, train_loss = 1.205, time/batch = 0.122\n",
      "Sequence 2921818/3263200 (epoch 89), batch 297854, train_loss = 1.423, time/batch = 0.120\n",
      "Sequence 2922778/3263200 (epoch 89), batch 297954, train_loss = 0.709, time/batch = 0.118\n",
      "Sequence 2923768/3263200 (epoch 89), batch 298054, train_loss = 1.509, time/batch = 0.122\n",
      "Sequence 2924748/3263200 (epoch 89), batch 298154, train_loss = 1.583, time/batch = 0.118\n",
      "Sequence 2925738/3263200 (epoch 89), batch 298254, train_loss = 1.582, time/batch = 0.120\n",
      "Sequence 2926728/3263200 (epoch 89), batch 298354, train_loss = 1.545, time/batch = 0.120\n",
      "Sequence 2927728/3263200 (epoch 89), batch 298454, train_loss = 1.064, time/batch = 0.118\n",
      "Sequence 2928718/3263200 (epoch 89), batch 298554, train_loss = 1.584, time/batch = 0.114\n",
      "Sequence 2929718/3263200 (epoch 89), batch 298654, train_loss = 0.856, time/batch = 0.112\n",
      "Sequence 2930688/3263200 (epoch 89), batch 298754, train_loss = 0.864, time/batch = 0.116\n",
      "Sequence 2931658/3263200 (epoch 89), batch 298854, train_loss = 1.332, time/batch = 0.119\n",
      "Sequence 2932648/3263200 (epoch 89), batch 298954, train_loss = 1.373, time/batch = 0.120\n",
      "Sequence 2933628/3263200 (epoch 89), batch 299054, train_loss = 0.917, time/batch = 0.117\n",
      "Sequence 2934588/3263200 (epoch 89), batch 299154, train_loss = 0.929, time/batch = 0.120\n",
      "Sequence 2935568/3263200 (epoch 89), batch 299254, train_loss = 1.073, time/batch = 0.119\n",
      "Sequence 2936568/3263200 (epoch 89), batch 299354, train_loss = 1.054, time/batch = 0.121\n",
      "Epoch 89 completed, average train loss 1.152637, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 2937530/3263200 (epoch 90), batch 299454, train_loss = 1.509, time/batch = 0.119\n",
      "Sequence 2938520/3263200 (epoch 90), batch 299554, train_loss = 0.905, time/batch = 0.116\n",
      "Sequence 2939510/3263200 (epoch 90), batch 299654, train_loss = 0.776, time/batch = 0.119\n",
      "Sequence 2940500/3263200 (epoch 90), batch 299754, train_loss = 0.957, time/batch = 0.118\n",
      "Sequence 2941460/3263200 (epoch 90), batch 299854, train_loss = 0.923, time/batch = 0.120\n",
      "Sequence 2942430/3263200 (epoch 90), batch 299954, train_loss = 0.924, time/batch = 0.122\n",
      "Sequence 2943420/3263200 (epoch 90), batch 300054, train_loss = 1.356, time/batch = 0.118\n",
      "Sequence 2944390/3263200 (epoch 90), batch 300154, train_loss = 0.847, time/batch = 0.117\n",
      "Sequence 2945380/3263200 (epoch 90), batch 300254, train_loss = 1.088, time/batch = 0.120\n",
      "Sequence 2946360/3263200 (epoch 90), batch 300354, train_loss = 1.081, time/batch = 0.120\n",
      "Sequence 2947330/3263200 (epoch 90), batch 300454, train_loss = 0.993, time/batch = 0.117\n",
      "Sequence 2948310/3263200 (epoch 90), batch 300554, train_loss = 1.525, time/batch = 0.119\n",
      "Sequence 2949280/3263200 (epoch 90), batch 300654, train_loss = 1.360, time/batch = 0.117\n",
      "Sequence 2950270/3263200 (epoch 90), batch 300754, train_loss = 0.977, time/batch = 0.118\n",
      "Sequence 2951260/3263200 (epoch 90), batch 300854, train_loss = 1.121, time/batch = 0.122\n",
      "Sequence 2952240/3263200 (epoch 90), batch 300954, train_loss = 1.110, time/batch = 0.120\n",
      "Sequence 2953240/3263200 (epoch 90), batch 301054, train_loss = 1.619, time/batch = 0.119\n",
      "Sequence 2954220/3263200 (epoch 90), batch 301154, train_loss = 0.973, time/batch = 0.120\n",
      "Sequence 2955190/3263200 (epoch 90), batch 301254, train_loss = 0.803, time/batch = 0.118\n",
      "Sequence 2956170/3263200 (epoch 90), batch 301354, train_loss = 1.580, time/batch = 0.118\n",
      "Sequence 2957160/3263200 (epoch 90), batch 301454, train_loss = 0.897, time/batch = 0.117\n",
      "Sequence 2958140/3263200 (epoch 90), batch 301554, train_loss = 1.211, time/batch = 0.120\n",
      "Sequence 2959130/3263200 (epoch 90), batch 301654, train_loss = 1.139, time/batch = 0.119\n",
      "Sequence 2960120/3263200 (epoch 90), batch 301754, train_loss = 0.811, time/batch = 0.117\n",
      "Sequence 2961110/3263200 (epoch 90), batch 301854, train_loss = 1.045, time/batch = 0.119\n",
      "Sequence 2962100/3263200 (epoch 90), batch 301954, train_loss = 1.020, time/batch = 0.119\n",
      "Sequence 2963070/3263200 (epoch 90), batch 302054, train_loss = 1.771, time/batch = 0.120\n",
      "Sequence 2964060/3263200 (epoch 90), batch 302154, train_loss = 1.193, time/batch = 0.118\n",
      "Sequence 2965020/3263200 (epoch 90), batch 302255, train_loss = 0.676, time/batch = 0.119\n",
      "Sequence 2966000/3263200 (epoch 90), batch 302355, train_loss = 0.758, time/batch = 0.118\n",
      "Sequence 2966970/3263200 (epoch 90), batch 302455, train_loss = 1.065, time/batch = 0.120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2967930/3263200 (epoch 90), batch 302555, train_loss = 1.195, time/batch = 0.117\n",
      "Sequence 2968930/3263200 (epoch 90), batch 302655, train_loss = 0.902, time/batch = 0.118\n",
      "Epoch 90 completed, average train loss 1.150477, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2969912/3263200 (epoch 91), batch 302755, train_loss = 0.852, time/batch = 0.111\n",
      "Sequence 2970882/3263200 (epoch 91), batch 302855, train_loss = 0.951, time/batch = 0.119\n",
      "Sequence 2971852/3263200 (epoch 91), batch 302955, train_loss = 1.016, time/batch = 0.122\n",
      "Sequence 2972852/3263200 (epoch 91), batch 303055, train_loss = 1.175, time/batch = 0.118\n",
      "Sequence 2973802/3263200 (epoch 91), batch 303155, train_loss = 1.258, time/batch = 0.120\n",
      "Sequence 2974782/3263200 (epoch 91), batch 303255, train_loss = 1.230, time/batch = 0.116\n",
      "Sequence 2975772/3263200 (epoch 91), batch 303355, train_loss = 1.263, time/batch = 0.120\n",
      "Sequence 2976772/3263200 (epoch 91), batch 303455, train_loss = 1.645, time/batch = 0.114\n",
      "Sequence 2977762/3263200 (epoch 91), batch 303555, train_loss = 0.953, time/batch = 0.111\n",
      "Sequence 2978722/3263200 (epoch 91), batch 303655, train_loss = 1.121, time/batch = 0.113\n",
      "Sequence 2979712/3263200 (epoch 91), batch 303755, train_loss = 1.706, time/batch = 0.115\n",
      "Sequence 2980692/3263200 (epoch 91), batch 303855, train_loss = 1.142, time/batch = 0.117\n",
      "Sequence 2981672/3263200 (epoch 91), batch 303955, train_loss = 0.877, time/batch = 0.120\n",
      "Sequence 2982622/3263200 (epoch 91), batch 304055, train_loss = 0.922, time/batch = 0.121\n",
      "Sequence 2983612/3263200 (epoch 91), batch 304155, train_loss = 1.123, time/batch = 0.119\n",
      "Sequence 2984612/3263200 (epoch 91), batch 304255, train_loss = 1.531, time/batch = 0.119\n",
      "Sequence 2985582/3263200 (epoch 91), batch 304355, train_loss = 0.946, time/batch = 0.116\n",
      "Sequence 2986582/3263200 (epoch 91), batch 304455, train_loss = 1.271, time/batch = 0.118\n",
      "Sequence 2987522/3263200 (epoch 91), batch 304555, train_loss = 1.180, time/batch = 0.121\n",
      "Sequence 2988512/3263200 (epoch 91), batch 304655, train_loss = 1.284, time/batch = 0.121\n",
      "Sequence 2989502/3263200 (epoch 91), batch 304755, train_loss = 0.882, time/batch = 0.113\n",
      "Sequence 2990472/3263200 (epoch 91), batch 304855, train_loss = 0.835, time/batch = 0.111\n",
      "Sequence 2991442/3263200 (epoch 91), batch 304955, train_loss = 2.059, time/batch = 0.117\n",
      "Sequence 2992412/3263200 (epoch 91), batch 305055, train_loss = 1.367, time/batch = 0.119\n",
      "Sequence 2993412/3263200 (epoch 91), batch 305155, train_loss = 0.833, time/batch = 0.119\n",
      "Sequence 2994412/3263200 (epoch 91), batch 305255, train_loss = 0.908, time/batch = 0.120\n",
      "Sequence 2995382/3263200 (epoch 91), batch 305355, train_loss = 1.532, time/batch = 0.119\n",
      "Sequence 2996382/3263200 (epoch 91), batch 305455, train_loss = 0.913, time/batch = 0.121\n",
      "Sequence 2997382/3263200 (epoch 91), batch 305555, train_loss = 1.367, time/batch = 0.121\n",
      "Sequence 2998362/3263200 (epoch 91), batch 305655, train_loss = 1.196, time/batch = 0.118\n",
      "Sequence 2999352/3263200 (epoch 91), batch 305756, train_loss = 0.552, time/batch = 0.117\n",
      "Sequence 3000312/3263200 (epoch 91), batch 305856, train_loss = 1.183, time/batch = 0.118\n",
      "Sequence 3001292/3263200 (epoch 91), batch 305956, train_loss = 0.879, time/batch = 0.123\n",
      "Epoch 91 completed, average train loss 1.146957, learning rate 0.0010\n",
      "Sequence 3002294/3263200 (epoch 92), batch 306056, train_loss = 0.690, time/batch = 0.118\n",
      "Shuffling training data...\n",
      "Sequence 3003274/3263200 (epoch 92), batch 306156, train_loss = 0.814, time/batch = 0.118\n",
      "Sequence 3004264/3263200 (epoch 92), batch 306256, train_loss = 1.164, time/batch = 0.120\n",
      "Sequence 3005204/3263200 (epoch 92), batch 306356, train_loss = 1.013, time/batch = 0.118\n",
      "Sequence 3006194/3263200 (epoch 92), batch 306456, train_loss = 1.505, time/batch = 0.118\n",
      "Sequence 3007174/3263200 (epoch 92), batch 306556, train_loss = 1.259, time/batch = 0.121\n",
      "Sequence 3008164/3263200 (epoch 92), batch 306656, train_loss = 1.073, time/batch = 0.116\n",
      "Sequence 3009144/3263200 (epoch 92), batch 306757, train_loss = 0.785, time/batch = 0.118\n",
      "Sequence 3010144/3263200 (epoch 92), batch 306857, train_loss = 1.337, time/batch = 0.118\n",
      "Sequence 3011104/3263200 (epoch 92), batch 306957, train_loss = 0.844, time/batch = 0.121\n",
      "Sequence 3012104/3263200 (epoch 92), batch 307057, train_loss = 1.594, time/batch = 0.120\n",
      "Sequence 3013074/3263200 (epoch 92), batch 307158, train_loss = 0.347, time/batch = 0.120\n",
      "Sequence 3014054/3263200 (epoch 92), batch 307258, train_loss = 0.832, time/batch = 0.119\n",
      "Sequence 3015034/3263200 (epoch 92), batch 307358, train_loss = 1.144, time/batch = 0.117\n",
      "Sequence 3016034/3263200 (epoch 92), batch 307458, train_loss = 0.969, time/batch = 0.118\n",
      "Sequence 3017024/3263200 (epoch 92), batch 307558, train_loss = 1.234, time/batch = 0.121\n",
      "Sequence 3018004/3263200 (epoch 92), batch 307658, train_loss = 0.950, time/batch = 0.122\n",
      "Sequence 3018984/3263200 (epoch 92), batch 307758, train_loss = 1.373, time/batch = 0.120\n",
      "Sequence 3019964/3263200 (epoch 92), batch 307858, train_loss = 1.368, time/batch = 0.122\n",
      "Sequence 3020944/3263200 (epoch 92), batch 307958, train_loss = 1.102, time/batch = 0.116\n",
      "Sequence 3021924/3263200 (epoch 92), batch 308058, train_loss = 1.167, time/batch = 0.118\n",
      "Sequence 3022894/3263200 (epoch 92), batch 308158, train_loss = 0.878, time/batch = 0.117\n",
      "Sequence 3023884/3263200 (epoch 92), batch 308258, train_loss = 0.956, time/batch = 0.121\n",
      "Sequence 3024864/3263200 (epoch 92), batch 308358, train_loss = 1.854, time/batch = 0.119\n",
      "Sequence 3025834/3263200 (epoch 92), batch 308458, train_loss = 1.380, time/batch = 0.122\n",
      "Sequence 3026824/3263200 (epoch 92), batch 308558, train_loss = 1.202, time/batch = 0.120\n",
      "Sequence 3027794/3263200 (epoch 92), batch 308658, train_loss = 0.785, time/batch = 0.120\n",
      "Sequence 3028784/3263200 (epoch 92), batch 308758, train_loss = 0.928, time/batch = 0.119\n",
      "Sequence 3029764/3263200 (epoch 92), batch 308858, train_loss = 1.027, time/batch = 0.119\n",
      "Sequence 3030734/3263200 (epoch 92), batch 308958, train_loss = 0.863, time/batch = 0.122\n",
      "Sequence 3031724/3263200 (epoch 92), batch 309058, train_loss = 0.620, time/batch = 0.123\n",
      "Sequence 3032714/3263200 (epoch 92), batch 309158, train_loss = 0.910, time/batch = 0.122\n",
      "Sequence 3033704/3263200 (epoch 92), batch 309258, train_loss = 1.540, time/batch = 0.117\n",
      "Sequence 3034684/3263200 (epoch 92), batch 309358, train_loss = 1.583, time/batch = 0.117\n",
      "Epoch 92 completed, average train loss 1.147677, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 3035666/3263200 (epoch 93), batch 309458, train_loss = 0.984, time/batch = 0.117\n",
      "Sequence 3036666/3263200 (epoch 93), batch 309558, train_loss = 0.914, time/batch = 0.115\n",
      "Sequence 3037646/3263200 (epoch 93), batch 309658, train_loss = 1.780, time/batch = 0.118\n",
      "Sequence 3038636/3263200 (epoch 93), batch 309758, train_loss = 1.162, time/batch = 0.117\n",
      "Sequence 3039626/3263200 (epoch 93), batch 309858, train_loss = 1.237, time/batch = 0.116\n",
      "Sequence 3040556/3263200 (epoch 93), batch 309958, train_loss = 1.446, time/batch = 0.118\n",
      "Sequence 3041506/3263200 (epoch 93), batch 310058, train_loss = 1.297, time/batch = 0.119\n",
      "Sequence 3042496/3263200 (epoch 93), batch 310158, train_loss = 1.099, time/batch = 0.118\n",
      "Sequence 3043486/3263200 (epoch 93), batch 310258, train_loss = 1.090, time/batch = 0.112\n",
      "Sequence 3044486/3263200 (epoch 93), batch 310358, train_loss = 1.241, time/batch = 0.117\n",
      "Sequence 3045466/3263200 (epoch 93), batch 310458, train_loss = 1.076, time/batch = 0.118\n",
      "Sequence 3046466/3263200 (epoch 93), batch 310558, train_loss = 1.070, time/batch = 0.120\n",
      "Sequence 3047426/3263200 (epoch 93), batch 310658, train_loss = 1.454, time/batch = 0.116\n",
      "Sequence 3048406/3263200 (epoch 93), batch 310758, train_loss = 1.483, time/batch = 0.120\n",
      "Sequence 3049396/3263200 (epoch 93), batch 310858, train_loss = 1.256, time/batch = 0.120\n",
      "Sequence 3050386/3263200 (epoch 93), batch 310958, train_loss = 0.818, time/batch = 0.113\n",
      "Sequence 3051366/3263200 (epoch 93), batch 311058, train_loss = 1.209, time/batch = 0.120\n",
      "Sequence 3052336/3263200 (epoch 93), batch 311159, train_loss = 0.864, time/batch = 0.120\n",
      "Sequence 3053326/3263200 (epoch 93), batch 311259, train_loss = 0.869, time/batch = 0.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 3054316/3263200 (epoch 93), batch 311359, train_loss = 1.206, time/batch = 0.116\n",
      "Sequence 3055296/3263200 (epoch 93), batch 311459, train_loss = 1.766, time/batch = 0.122\n",
      "Sequence 3056276/3263200 (epoch 93), batch 311559, train_loss = 1.175, time/batch = 0.121\n",
      "Sequence 3057256/3263200 (epoch 93), batch 311659, train_loss = 1.312, time/batch = 0.122\n",
      "Sequence 3058236/3263200 (epoch 93), batch 311759, train_loss = 1.000, time/batch = 0.119\n",
      "Sequence 3059206/3263200 (epoch 93), batch 311859, train_loss = 0.948, time/batch = 0.119\n",
      "Sequence 3060196/3263200 (epoch 93), batch 311959, train_loss = 1.104, time/batch = 0.124\n",
      "Sequence 3061176/3263200 (epoch 93), batch 312059, train_loss = 1.632, time/batch = 0.121\n",
      "Sequence 3062166/3263200 (epoch 93), batch 312159, train_loss = 0.802, time/batch = 0.120\n",
      "Sequence 3063156/3263200 (epoch 93), batch 312259, train_loss = 1.175, time/batch = 0.119\n",
      "Sequence 3064126/3263200 (epoch 93), batch 312359, train_loss = 1.153, time/batch = 0.119\n",
      "Sequence 3065126/3263200 (epoch 93), batch 312459, train_loss = 2.217, time/batch = 0.119\n",
      "Sequence 3066086/3263200 (epoch 93), batch 312559, train_loss = 0.995, time/batch = 0.119\n",
      "Sequence 3067086/3263200 (epoch 93), batch 312659, train_loss = 1.706, time/batch = 0.117\n",
      "Epoch 93 completed, average train loss 1.144270, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 3068068/3263200 (epoch 94), batch 312759, train_loss = 1.528, time/batch = 0.120\n",
      "Sequence 3069038/3263200 (epoch 94), batch 312859, train_loss = 0.828, time/batch = 0.124\n",
      "Sequence 3070038/3263200 (epoch 94), batch 312959, train_loss = 0.814, time/batch = 0.121\n",
      "Sequence 3071018/3263200 (epoch 94), batch 313059, train_loss = 1.166, time/batch = 0.118\n",
      "Sequence 3071988/3263200 (epoch 94), batch 313159, train_loss = 1.437, time/batch = 0.121\n",
      "Sequence 3072958/3263200 (epoch 94), batch 313259, train_loss = 1.177, time/batch = 0.118\n",
      "Sequence 3073918/3263200 (epoch 94), batch 313359, train_loss = 1.080, time/batch = 0.119\n",
      "Sequence 3074898/3263200 (epoch 94), batch 313459, train_loss = 1.799, time/batch = 0.120\n",
      "Sequence 3075888/3263200 (epoch 94), batch 313559, train_loss = 1.431, time/batch = 0.120\n",
      "Sequence 3076878/3263200 (epoch 94), batch 313659, train_loss = 1.594, time/batch = 0.118\n",
      "Sequence 3077848/3263200 (epoch 94), batch 313759, train_loss = 0.985, time/batch = 0.118\n",
      "Sequence 3078818/3263200 (epoch 94), batch 313859, train_loss = 1.129, time/batch = 0.123\n",
      "Sequence 3079788/3263200 (epoch 94), batch 313959, train_loss = 1.691, time/batch = 0.121\n",
      "Sequence 3080778/3263200 (epoch 94), batch 314059, train_loss = 0.907, time/batch = 0.120\n",
      "Sequence 3081748/3263200 (epoch 94), batch 314159, train_loss = 1.077, time/batch = 0.121\n",
      "Sequence 3082718/3263200 (epoch 94), batch 314259, train_loss = 1.880, time/batch = 0.117\n",
      "Sequence 3083708/3263200 (epoch 94), batch 314359, train_loss = 0.791, time/batch = 0.119\n",
      "Sequence 3084678/3263200 (epoch 94), batch 314459, train_loss = 1.302, time/batch = 0.122\n",
      "Sequence 3085678/3263200 (epoch 94), batch 314559, train_loss = 1.127, time/batch = 0.119\n",
      "Sequence 3086648/3263200 (epoch 94), batch 314659, train_loss = 0.907, time/batch = 0.123\n",
      "Sequence 3087628/3263200 (epoch 94), batch 314759, train_loss = 0.853, time/batch = 0.122\n",
      "Sequence 3088568/3263200 (epoch 94), batch 314859, train_loss = 0.933, time/batch = 0.121\n",
      "Sequence 3089558/3263200 (epoch 94), batch 314959, train_loss = 1.452, time/batch = 0.121\n",
      "Sequence 3090548/3263200 (epoch 94), batch 315059, train_loss = 1.113, time/batch = 0.120\n",
      "Sequence 3091538/3263200 (epoch 94), batch 315159, train_loss = 1.047, time/batch = 0.116\n",
      "Sequence 3092528/3263200 (epoch 94), batch 315259, train_loss = 1.479, time/batch = 0.118\n",
      "Sequence 3093478/3263200 (epoch 94), batch 315359, train_loss = 0.737, time/batch = 0.122\n",
      "Sequence 3094478/3263200 (epoch 94), batch 315459, train_loss = 1.628, time/batch = 0.120\n",
      "Sequence 3095468/3263200 (epoch 94), batch 315559, train_loss = 1.186, time/batch = 0.121\n",
      "Sequence 3096458/3263200 (epoch 94), batch 315659, train_loss = 0.912, time/batch = 0.118\n",
      "Sequence 3097458/3263200 (epoch 94), batch 315759, train_loss = 1.144, time/batch = 0.117\n",
      "Sequence 3098448/3263200 (epoch 94), batch 315859, train_loss = 1.003, time/batch = 0.119\n",
      "Sequence 3099428/3263200 (epoch 94), batch 315959, train_loss = 1.115, time/batch = 0.122\n",
      "Epoch 94 completed, average train loss 1.142149, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 3100430/3263200 (epoch 95), batch 316059, train_loss = 1.016, time/batch = 0.121\n",
      "Sequence 3101410/3263200 (epoch 95), batch 316159, train_loss = 1.600, time/batch = 0.120\n",
      "Sequence 3102410/3263200 (epoch 95), batch 316259, train_loss = 0.909, time/batch = 0.121\n",
      "Sequence 3103380/3263200 (epoch 95), batch 316359, train_loss = 1.212, time/batch = 0.121\n",
      "Sequence 3104340/3263200 (epoch 95), batch 316459, train_loss = 0.500, time/batch = 0.119\n",
      "Sequence 3105280/3263200 (epoch 95), batch 316559, train_loss = 1.185, time/batch = 0.119\n",
      "Sequence 3106270/3263200 (epoch 95), batch 316659, train_loss = 1.105, time/batch = 0.119\n",
      "Sequence 3107250/3263200 (epoch 95), batch 316759, train_loss = 1.425, time/batch = 0.119\n",
      "Sequence 3108220/3263200 (epoch 95), batch 316859, train_loss = 1.106, time/batch = 0.120\n",
      "Sequence 3109220/3263200 (epoch 95), batch 316959, train_loss = 1.352, time/batch = 0.120\n",
      "Sequence 3110220/3263200 (epoch 95), batch 317059, train_loss = 1.117, time/batch = 0.115\n",
      "Sequence 3111200/3263200 (epoch 95), batch 317159, train_loss = 1.054, time/batch = 0.118\n",
      "Sequence 3112190/3263200 (epoch 95), batch 317259, train_loss = 1.034, time/batch = 0.116\n",
      "Sequence 3113170/3263200 (epoch 95), batch 317359, train_loss = 1.499, time/batch = 0.121\n",
      "Sequence 3114140/3263200 (epoch 95), batch 317459, train_loss = 0.948, time/batch = 0.120\n",
      "Sequence 3115080/3263200 (epoch 95), batch 317559, train_loss = 0.795, time/batch = 0.122\n",
      "Sequence 3116080/3263200 (epoch 95), batch 317659, train_loss = 1.359, time/batch = 0.121\n",
      "Sequence 3117070/3263200 (epoch 95), batch 317759, train_loss = 0.918, time/batch = 0.122\n",
      "Sequence 3118050/3263200 (epoch 95), batch 317859, train_loss = 1.321, time/batch = 0.122\n",
      "Sequence 3119030/3263200 (epoch 95), batch 317959, train_loss = 0.977, time/batch = 0.120\n",
      "Sequence 3120030/3263200 (epoch 95), batch 318059, train_loss = 1.304, time/batch = 0.121\n",
      "Sequence 3121020/3263200 (epoch 95), batch 318159, train_loss = 1.064, time/batch = 0.119\n",
      "Sequence 3122010/3263200 (epoch 95), batch 318259, train_loss = 1.243, time/batch = 0.121\n",
      "Sequence 3123010/3263200 (epoch 95), batch 318359, train_loss = 1.215, time/batch = 0.120\n",
      "Sequence 3123980/3263200 (epoch 95), batch 318459, train_loss = 1.969, time/batch = 0.119\n",
      "Sequence 3124940/3263200 (epoch 95), batch 318559, train_loss = 0.802, time/batch = 0.118\n",
      "Sequence 3125930/3263200 (epoch 95), batch 318659, train_loss = 0.869, time/batch = 0.120\n",
      "Sequence 3126900/3263200 (epoch 95), batch 318759, train_loss = 0.864, time/batch = 0.120\n",
      "Sequence 3127890/3263200 (epoch 95), batch 318859, train_loss = 0.939, time/batch = 0.122\n",
      "Sequence 3128870/3263200 (epoch 95), batch 318959, train_loss = 1.376, time/batch = 0.122\n",
      "Sequence 3129860/3263200 (epoch 95), batch 319059, train_loss = 1.057, time/batch = 0.119\n",
      "Sequence 3130840/3263200 (epoch 95), batch 319159, train_loss = 1.106, time/batch = 0.122\n",
      "Sequence 3131830/3263200 (epoch 95), batch 319259, train_loss = 1.065, time/batch = 0.122\n",
      "Epoch 95 completed, average train loss 1.140931, learning rate 0.0010\n",
      "Sequence 3132812/3263200 (epoch 96), batch 319359, train_loss = 0.975, time/batch = 0.121\n",
      "Shuffling training data...\n",
      "Sequence 3133782/3263200 (epoch 96), batch 319459, train_loss = 0.966, time/batch = 0.119\n",
      "Sequence 3134752/3263200 (epoch 96), batch 319559, train_loss = 1.419, time/batch = 0.119\n",
      "Sequence 3135732/3263200 (epoch 96), batch 319659, train_loss = 1.996, time/batch = 0.121\n",
      "Sequence 3136702/3263200 (epoch 96), batch 319759, train_loss = 1.104, time/batch = 0.120\n",
      "Sequence 3137692/3263200 (epoch 96), batch 319859, train_loss = 1.037, time/batch = 0.116\n",
      "Sequence 3138682/3263200 (epoch 96), batch 319959, train_loss = 1.187, time/batch = 0.120\n",
      "Sequence 3139642/3263200 (epoch 96), batch 320059, train_loss = 0.794, time/batch = 0.122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 3140642/3263200 (epoch 96), batch 320159, train_loss = 1.102, time/batch = 0.119\n",
      "Sequence 3141612/3263200 (epoch 96), batch 320259, train_loss = 1.155, time/batch = 0.122\n",
      "Sequence 3142582/3263200 (epoch 96), batch 320359, train_loss = 1.372, time/batch = 0.121\n",
      "Sequence 3143572/3263200 (epoch 96), batch 320459, train_loss = 1.303, time/batch = 0.119\n",
      "Sequence 3144522/3263200 (epoch 96), batch 320559, train_loss = 0.987, time/batch = 0.122\n",
      "Sequence 3145492/3263200 (epoch 96), batch 320659, train_loss = 1.150, time/batch = 0.121\n",
      "Sequence 3146482/3263200 (epoch 96), batch 320759, train_loss = 1.345, time/batch = 0.120\n",
      "Sequence 3147472/3263200 (epoch 96), batch 320859, train_loss = 1.228, time/batch = 0.122\n",
      "Sequence 3148462/3263200 (epoch 96), batch 320959, train_loss = 1.672, time/batch = 0.122\n",
      "Sequence 3149432/3263200 (epoch 96), batch 321059, train_loss = 0.917, time/batch = 0.121\n",
      "Sequence 3150412/3263200 (epoch 96), batch 321159, train_loss = 1.150, time/batch = 0.120\n",
      "Sequence 3151372/3263200 (epoch 96), batch 321259, train_loss = 1.103, time/batch = 0.122\n",
      "Sequence 3152362/3263200 (epoch 96), batch 321359, train_loss = 1.146, time/batch = 0.119\n",
      "Sequence 3153332/3263200 (epoch 96), batch 321459, train_loss = 2.139, time/batch = 0.117\n",
      "Sequence 3154312/3263200 (epoch 96), batch 321559, train_loss = 1.159, time/batch = 0.116\n",
      "Sequence 3155312/3263200 (epoch 96), batch 321659, train_loss = 0.829, time/batch = 0.120\n",
      "Sequence 3156292/3263200 (epoch 96), batch 321759, train_loss = 1.262, time/batch = 0.119\n",
      "Sequence 3157282/3263200 (epoch 96), batch 321859, train_loss = 1.585, time/batch = 0.119\n",
      "Sequence 3158262/3263200 (epoch 96), batch 321959, train_loss = 1.655, time/batch = 0.118\n",
      "Sequence 3159242/3263200 (epoch 96), batch 322059, train_loss = 1.082, time/batch = 0.122\n",
      "Sequence 3160232/3263200 (epoch 96), batch 322159, train_loss = 1.182, time/batch = 0.118\n",
      "Sequence 3161212/3263200 (epoch 96), batch 322259, train_loss = 1.114, time/batch = 0.118\n",
      "Sequence 3162202/3263200 (epoch 96), batch 322359, train_loss = 0.863, time/batch = 0.119\n",
      "Sequence 3163202/3263200 (epoch 96), batch 322459, train_loss = 1.455, time/batch = 0.117\n",
      "Sequence 3164182/3263200 (epoch 96), batch 322559, train_loss = 0.692, time/batch = 0.122\n",
      "Sequence 3165172/3263200 (epoch 96), batch 322659, train_loss = 1.045, time/batch = 0.118\n",
      "Epoch 96 completed, average train loss 1.139156, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 3166154/3263200 (epoch 97), batch 322759, train_loss = 0.750, time/batch = 0.120\n",
      "Sequence 3167124/3263200 (epoch 97), batch 322859, train_loss = 1.158, time/batch = 0.116\n",
      "Sequence 3168114/3263200 (epoch 97), batch 322959, train_loss = 0.931, time/batch = 0.117\n",
      "Sequence 3169054/3263200 (epoch 97), batch 323059, train_loss = 1.282, time/batch = 0.123\n",
      "Sequence 3170044/3263200 (epoch 97), batch 323159, train_loss = 1.150, time/batch = 0.123\n",
      "Sequence 3171024/3263200 (epoch 97), batch 323259, train_loss = 1.212, time/batch = 0.118\n",
      "Sequence 3172014/3263200 (epoch 97), batch 323359, train_loss = 0.927, time/batch = 0.119\n",
      "Sequence 3172984/3263200 (epoch 97), batch 323459, train_loss = 1.132, time/batch = 0.121\n",
      "Sequence 3173954/3263200 (epoch 97), batch 323559, train_loss = 0.977, time/batch = 0.118\n",
      "Sequence 3174954/3263200 (epoch 97), batch 323659, train_loss = 1.659, time/batch = 0.117\n",
      "Sequence 3175944/3263200 (epoch 97), batch 323759, train_loss = 1.038, time/batch = 0.119\n",
      "Sequence 3176934/3263200 (epoch 97), batch 323859, train_loss = 1.423, time/batch = 0.118\n",
      "Sequence 3177934/3263200 (epoch 97), batch 323959, train_loss = 0.984, time/batch = 0.119\n",
      "Sequence 3178904/3263200 (epoch 97), batch 324059, train_loss = 1.082, time/batch = 0.121\n",
      "Sequence 3179884/3263200 (epoch 97), batch 324159, train_loss = 0.911, time/batch = 0.120\n",
      "Sequence 3180854/3263200 (epoch 97), batch 324259, train_loss = 1.055, time/batch = 0.120\n",
      "Sequence 3181834/3263200 (epoch 97), batch 324359, train_loss = 1.357, time/batch = 0.117\n",
      "Sequence 3182804/3263200 (epoch 97), batch 324459, train_loss = 1.208, time/batch = 0.119\n",
      "Sequence 3183794/3263200 (epoch 97), batch 324559, train_loss = 0.981, time/batch = 0.117\n",
      "Sequence 3184764/3263200 (epoch 97), batch 324659, train_loss = 1.839, time/batch = 0.120\n",
      "Sequence 3185734/3263200 (epoch 97), batch 324759, train_loss = 0.894, time/batch = 0.118\n",
      "Sequence 3186734/3263200 (epoch 97), batch 324859, train_loss = 1.110, time/batch = 0.121\n",
      "Sequence 3187724/3263200 (epoch 97), batch 324959, train_loss = 1.136, time/batch = 0.121\n",
      "Sequence 3188704/3263200 (epoch 97), batch 325059, train_loss = 1.152, time/batch = 0.118\n",
      "Sequence 3189684/3263200 (epoch 97), batch 325159, train_loss = 1.067, time/batch = 0.123\n",
      "Sequence 3190654/3263200 (epoch 97), batch 325259, train_loss = 0.906, time/batch = 0.122\n",
      "Sequence 3191654/3263200 (epoch 97), batch 325359, train_loss = 1.449, time/batch = 0.119\n",
      "Sequence 3192644/3263200 (epoch 97), batch 325459, train_loss = 1.078, time/batch = 0.120\n",
      "Sequence 3193624/3263200 (epoch 97), batch 325559, train_loss = 1.302, time/batch = 0.121\n",
      "Sequence 3194594/3263200 (epoch 97), batch 325659, train_loss = 0.943, time/batch = 0.119\n",
      "Sequence 3195564/3263200 (epoch 97), batch 325759, train_loss = 1.430, time/batch = 0.117\n",
      "Sequence 3196534/3263200 (epoch 97), batch 325859, train_loss = 1.280, time/batch = 0.119\n",
      "Sequence 3197534/3263200 (epoch 97), batch 325959, train_loss = 1.003, time/batch = 0.121\n",
      "Epoch 97 completed, average train loss 1.136600, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 3198516/3263200 (epoch 98), batch 326059, train_loss = 1.171, time/batch = 0.121\n",
      "Sequence 3199516/3263200 (epoch 98), batch 326159, train_loss = 0.806, time/batch = 0.119\n",
      "Sequence 3200516/3263200 (epoch 98), batch 326259, train_loss = 1.380, time/batch = 0.121\n",
      "Sequence 3201496/3263200 (epoch 98), batch 326359, train_loss = 1.054, time/batch = 0.120\n",
      "Sequence 3202486/3263200 (epoch 98), batch 326459, train_loss = 1.161, time/batch = 0.122\n",
      "Sequence 3203446/3263200 (epoch 98), batch 326559, train_loss = 1.050, time/batch = 0.121\n",
      "Sequence 3204426/3263200 (epoch 98), batch 326659, train_loss = 1.138, time/batch = 0.120\n",
      "Sequence 3205406/3263200 (epoch 98), batch 326759, train_loss = 1.226, time/batch = 0.119\n",
      "Sequence 3206366/3263200 (epoch 98), batch 326859, train_loss = 1.467, time/batch = 0.121\n",
      "Sequence 3207356/3263200 (epoch 98), batch 326959, train_loss = 0.832, time/batch = 0.120\n",
      "Sequence 3208346/3263200 (epoch 98), batch 327059, train_loss = 1.288, time/batch = 0.120\n",
      "Sequence 3209346/3263200 (epoch 98), batch 327159, train_loss = 0.821, time/batch = 0.119\n",
      "Sequence 3210336/3263200 (epoch 98), batch 327259, train_loss = 0.854, time/batch = 0.118\n",
      "Sequence 3211326/3263200 (epoch 98), batch 327359, train_loss = 1.163, time/batch = 0.120\n",
      "Sequence 3212306/3263200 (epoch 98), batch 327459, train_loss = 1.015, time/batch = 0.119\n",
      "Sequence 3213266/3263200 (epoch 98), batch 327560, train_loss = 0.896, time/batch = 0.123\n",
      "Sequence 3214246/3263200 (epoch 98), batch 327660, train_loss = 1.086, time/batch = 0.119\n",
      "Sequence 3215216/3263200 (epoch 98), batch 327760, train_loss = 1.200, time/batch = 0.119\n",
      "Sequence 3216186/3263200 (epoch 98), batch 327860, train_loss = 0.847, time/batch = 0.117\n",
      "Sequence 3217156/3263200 (epoch 98), batch 327960, train_loss = 0.894, time/batch = 0.121\n",
      "Sequence 3218126/3263200 (epoch 98), batch 328060, train_loss = 1.080, time/batch = 0.118\n",
      "Sequence 3219126/3263200 (epoch 98), batch 328160, train_loss = 1.447, time/batch = 0.118\n",
      "Sequence 3220116/3263200 (epoch 98), batch 328260, train_loss = 1.237, time/batch = 0.121\n",
      "Sequence 3221096/3263200 (epoch 98), batch 328360, train_loss = 0.855, time/batch = 0.119\n",
      "Sequence 3222066/3263200 (epoch 98), batch 328460, train_loss = 0.819, time/batch = 0.120\n",
      "Sequence 3223046/3263200 (epoch 98), batch 328560, train_loss = 1.192, time/batch = 0.122\n",
      "Sequence 3224026/3263200 (epoch 98), batch 328660, train_loss = 0.745, time/batch = 0.121\n",
      "Sequence 3224996/3263200 (epoch 98), batch 328760, train_loss = 1.103, time/batch = 0.123\n",
      "Sequence 3225996/3263200 (epoch 98), batch 328860, train_loss = 0.987, time/batch = 0.119\n",
      "Sequence 3226986/3263200 (epoch 98), batch 328960, train_loss = 1.359, time/batch = 0.121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 3227966/3263200 (epoch 98), batch 329060, train_loss = 1.119, time/batch = 0.121\n",
      "Sequence 3228936/3263200 (epoch 98), batch 329160, train_loss = 1.022, time/batch = 0.121\n",
      "Sequence 3229916/3263200 (epoch 98), batch 329260, train_loss = 1.000, time/batch = 0.116\n",
      "Epoch 98 completed, average train loss 1.137398, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 3230908/3263200 (epoch 99), batch 329360, train_loss = 0.836, time/batch = 0.116\n",
      "Sequence 3231898/3263200 (epoch 99), batch 329460, train_loss = 1.159, time/batch = 0.117\n",
      "Sequence 3232878/3263200 (epoch 99), batch 329560, train_loss = 1.052, time/batch = 0.118\n",
      "Sequence 3233818/3263200 (epoch 99), batch 329660, train_loss = 1.320, time/batch = 0.116\n",
      "Sequence 3234788/3263200 (epoch 99), batch 329760, train_loss = 0.895, time/batch = 0.121\n",
      "Sequence 3235778/3263200 (epoch 99), batch 329860, train_loss = 0.943, time/batch = 0.120\n",
      "Sequence 3236778/3263200 (epoch 99), batch 329960, train_loss = 1.120, time/batch = 0.122\n",
      "Sequence 3237758/3263200 (epoch 99), batch 330060, train_loss = 0.881, time/batch = 0.121\n",
      "Sequence 3238728/3263200 (epoch 99), batch 330160, train_loss = 1.501, time/batch = 0.123\n",
      "Sequence 3239728/3263200 (epoch 99), batch 330260, train_loss = 1.627, time/batch = 0.122\n",
      "Sequence 3240718/3263200 (epoch 99), batch 330360, train_loss = 2.033, time/batch = 0.122\n",
      "Sequence 3241698/3263200 (epoch 99), batch 330460, train_loss = 1.457, time/batch = 0.121\n",
      "Sequence 3242688/3263200 (epoch 99), batch 330560, train_loss = 1.006, time/batch = 0.121\n",
      "Sequence 3243668/3263200 (epoch 99), batch 330660, train_loss = 0.899, time/batch = 0.121\n",
      "Sequence 3244638/3263200 (epoch 99), batch 330760, train_loss = 1.175, time/batch = 0.119\n",
      "Sequence 3245638/3263200 (epoch 99), batch 330860, train_loss = 1.318, time/batch = 0.121\n",
      "Sequence 3246588/3263200 (epoch 99), batch 330960, train_loss = 1.008, time/batch = 0.118\n",
      "Sequence 3247558/3263200 (epoch 99), batch 331060, train_loss = 1.279, time/batch = 0.120\n",
      "Sequence 3248558/3263200 (epoch 99), batch 331160, train_loss = 0.871, time/batch = 0.121\n",
      "Sequence 3249538/3263200 (epoch 99), batch 331260, train_loss = 0.787, time/batch = 0.118\n",
      "Sequence 3250528/3263200 (epoch 99), batch 331360, train_loss = 0.861, time/batch = 0.117\n",
      "Sequence 3251528/3263200 (epoch 99), batch 331461, train_loss = 0.496, time/batch = 0.122\n",
      "Sequence 3252518/3263200 (epoch 99), batch 331561, train_loss = 1.213, time/batch = 0.120\n",
      "Sequence 3253498/3263200 (epoch 99), batch 331661, train_loss = 0.691, time/batch = 0.125\n",
      "Sequence 3254498/3263200 (epoch 99), batch 331762, train_loss = 0.955, time/batch = 0.122\n",
      "Sequence 3255478/3263200 (epoch 99), batch 331862, train_loss = 0.858, time/batch = 0.120\n",
      "Sequence 3256458/3263200 (epoch 99), batch 331962, train_loss = 0.838, time/batch = 0.120\n",
      "Sequence 3257448/3263200 (epoch 99), batch 332062, train_loss = 1.192, time/batch = 0.120\n",
      "Sequence 3258428/3263200 (epoch 99), batch 332162, train_loss = 0.881, time/batch = 0.120\n",
      "Sequence 3259408/3263200 (epoch 99), batch 332262, train_loss = 1.160, time/batch = 0.126\n",
      "Sequence 3260408/3263200 (epoch 99), batch 332362, train_loss = 0.800, time/batch = 0.120\n",
      "Sequence 3261358/3263200 (epoch 99), batch 332462, train_loss = 0.569, time/batch = 0.123\n",
      "Sequence 3262328/3263200 (epoch 99), batch 332562, train_loss = 1.233, time/batch = 0.119\n",
      "Epoch 99 completed, average train loss 1.134524, learning rate 0.0010\n",
      "model saved.\n"
     ]
    }
   ],
   "source": [
    "#args.num_epochs = 4\n",
    "#args.save_every = 2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  #with tf.device('/device:GPU:2'):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. train method2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dim_rec', type=int, default=128,\n",
    "                     help='size of RNN hidden state')\n",
    "parser.add_argument('--num_layers', type=int, default=2,\n",
    "                     help='number of layers in the RNN. ')\n",
    "parser.add_argument('--batch_size', type=int, default=10,\n",
    "                     help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200,\n",
    "                     help='number of epochs')\n",
    "parser.add_argument('--save_every', type=int, default=10,\n",
    "                     help='save frequency by epoches')\n",
    "parser.add_argument('--model_dir', type=str, default='checkpoints',\n",
    "                     help='directory to save model to')\n",
    "parser.add_argument('--summary_dir', type=str, default='summary',\n",
    "                     help='directory to save tensorboard info')\n",
    "parser.add_argument('--max_grad_norm', type=float, default=1.,\n",
    "                     help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                     help='learning rate')\n",
    "parser.add_argument('--decay_rate', type=float, default=1.0,\n",
    "                     help='decay rate for the optimizer')\n",
    "parser.add_argument('--num_mixture', type=int, default=2,\n",
    "                     help='number of gaussian mixtures')\n",
    "parser.add_argument('--data_scale', type=float, default=1000,\n",
    "                     help='factor to scale raw data down by')\n",
    "parser.add_argument('--load_model', type=str, default=None,\n",
    "                     help='Reload a model checkpoint and restore training.' )\n",
    "parser.add_argument('--bptt_length', type=int, default=120,\n",
    "                     help='How many steps should the gradients pass back.' )\n",
    "parser.add_argument('--loss_form', type=str, default='mse',\n",
    "                     help='mse / gmm' )\n",
    "parser.add_argument('--constraint_factor', type=float, default=0.,\n",
    "                     help='the weight for constraint term in the cost function.' )\n",
    "  \n",
    "args = parser.parse_args(['--num_epochs','200'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python36.zip', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/lib-dynload', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/Sphinx-1.5.6-py3.6.egg', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/svgwrite-1.1.6-py3.6.egg', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/IPython/extensions', '/home/mye/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#reload(sys)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal length of the training data is 169\n",
      "Training data case distribution: [5348, 5108, 4904, 1208, 1260, 1224, 916, 1180, 1216, 1044, 876, 456, 1264, 992, 1080, 932, 1008, 420, 1160, 1036]\n",
      "Validation data case distribution: [70, 67, 65, 15, 17, 16, 12, 15, 16, 14, 12, 6, 17, 13, 14, 13, 14, 5, 15, 13]\n",
      "Shuffling training data...\n",
      "Shuffling training data...\n",
      "(10, 120, 6)\n",
      "(10, 120, 3)\n",
      "(10, 120, 1)\n",
      "(10, 12)\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(args.batch_size, args.data_scale, args.bptt_length) # batch_size=10, bptt_length=120\n",
    "data_loader.reset_batch_pointer()\n",
    "x, y, w, c, lens = data_loader.next_batch()\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(w.shape)\n",
    "print(c.shape)\n",
    "print(args.bptt_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
