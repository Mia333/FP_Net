{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The main script for training\n",
    "\n",
    "requirement: python 2, tensorflow r1.4 or r1.2 \n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib.framework import nest\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--dim_rec', type=int, default=128,\n",
    "                     help='size of RNN hidden state')\n",
    "  parser.add_argument('--num_layers', type=int, default=2,\n",
    "                     help='number of layers in the RNN. ')\n",
    "  parser.add_argument('--batch_size', type=int, default=10,\n",
    "                     help='minibatch size')\n",
    "  parser.add_argument('--num_epochs', type=int, default=200,\n",
    "                     help='number of epochs')\n",
    "  parser.add_argument('--save_every', type=int, default=10,\n",
    "                     help='save frequency by epoches')\n",
    "  parser.add_argument('--model_dir', type=str, default='checkpoints',\n",
    "                     help='directory to save model to')\n",
    "  parser.add_argument('--summary_dir', type=str, default='summary',\n",
    "                     help='directory to save tensorboard info')\n",
    "  parser.add_argument('--max_grad_norm', type=float, default=1.,\n",
    "                     help='clip gradients at this value')\n",
    "  parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                     help='learning rate')\n",
    "  parser.add_argument('--decay_rate', type=float, default=1.0,\n",
    "                     help='decay rate for the optimizer')\n",
    "  parser.add_argument('--num_mixture', type=int, default=2,\n",
    "                     help='number of gaussian mixtures')\n",
    "  parser.add_argument('--data_scale', type=float, default=1000,\n",
    "                     help='factor to scale raw data down by')\n",
    "  parser.add_argument('--load_model', type=str, default=None,\n",
    "                     help='Reload a model checkpoint and restore training.' )\n",
    "  parser.add_argument('--bptt_length', type=int, default=120,\n",
    "                     help='How many steps should the gradients pass back.' )\n",
    "  parser.add_argument('--loss_form', type=str, default='mse',\n",
    "                     help='mse / gmm' )\n",
    "  parser.add_argument('--constraint_factor', type=float, default=0.,\n",
    "                     help='the weight for constraint term in the cost function.' )\n",
    "  \n",
    "  args = parser.parse_args(['--num_epochs','200'])\n",
    "\n",
    "  args.num_epochs = 100\n",
    "  args.save_every = 5\n",
    "\n",
    "\n",
    "  train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "  data_loader = DataLoader(args.batch_size, args.data_scale, args.bptt_length)\n",
    "  data_loader.reset_batch_pointer()\n",
    "\n",
    "  if args.model_dir != '' and not os.path.exists(args.model_dir):\n",
    "    os.makedirs(args.model_dir)\n",
    "\n",
    "  with open(os.path.join(args.model_dir, 'config.pkl'), 'wb') as f:\n",
    "    pickle.dump(args, f)\n",
    "  print(\"hyperparam. saved.\")\n",
    "\n",
    "  model = Model(args)\n",
    "\n",
    "  # training\n",
    "  with tf.Session(config=config) as sess:\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    if args.load_model is not None:\n",
    "        saver.restore(sess, args.load_model)\n",
    "        _, ep_start = args.load_model.rsplit(\"-\", 1)\n",
    "        ep_start = int(ep_start)\n",
    "        model_steps = int(ep_start * data_loader.num_batches)\n",
    "    else:\n",
    "        ep_start = 0\n",
    "        model_steps = last_model_steps = 0\n",
    "\n",
    "    last_time = time.time()\n",
    "\n",
    "    for ep in range(ep_start, args.num_epochs):\n",
    "      ep_loss = []\n",
    "      sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** ep))) ###????????\n",
    "\n",
    "      for i in range(int(data_loader.num_sequences / args.batch_size)):\n",
    "        idx = ep * data_loader.num_sequences + i * args.batch_size\n",
    "        start = time.time()\n",
    "        x, y, w, c, lens = data_loader.next_batch()\n",
    "\n",
    "        loss_list, model_steps = model.train(\n",
    "          sess=sess, \n",
    "          sequence=x, \n",
    "          targets=y, \n",
    "          weights=w, \n",
    "          conditions=c, \n",
    "          subseq_length=args.bptt_length, \n",
    "          step_count=model_steps\n",
    "          )\n",
    "\n",
    "        ep_loss += loss_list\n",
    "        ########?????????????????????\n",
    "        if model_steps - last_model_steps >= 100:\n",
    "          new_time = time.time()\n",
    "          print(\n",
    "            \"Sequence %d/%d (epoch %d), batch %d, train_loss = %.3f, time/batch = %.3f\" \n",
    "            % (\n",
    "                idx,\n",
    "                args.num_epochs * data_loader.num_sequences,\n",
    "                ep,\n",
    "                model_steps,\n",
    "                np.mean(loss_list),\n",
    "                (new_time - last_time) / (model_steps - last_model_steps)\n",
    "              )\n",
    "            )\n",
    "          sys.stdout.flush()\n",
    "          last_model_steps = model_steps\n",
    "          last_time = new_time\n",
    "      print(\"Epoch %d completed, average train loss %.6f, learning rate %.4f\" % (ep, np.mean(ep_loss), args.learning_rate * (args.decay_rate ** ep)))\n",
    "      sys.stdout.flush()\n",
    "      if not os.path.isdir(args.model_dir):\n",
    "        os.makedirs(args.model_dir)\n",
    "      if (ep+1) % args.save_every == 0:\n",
    "        checkpoint_path = os.path.join(args.model_dir, 'model.ckpt')\n",
    "        saver.save(sess, save_path=checkpoint_path, global_step = (ep+1))\n",
    "        print(\"model saved.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal length of the training data is 169\n",
      "Training data case distribution: [5348, 5108, 4904, 1208, 1260, 1224, 916, 1180, 1216, 1044, 876, 456, 1264, 992, 1080, 932, 1008, 420, 1160, 1036]\n",
      "Validation data case distribution: [70, 67, 65, 15, 17, 16, 12, 15, 16, 14, 12, 6, 17, 13, 14, 13, 14, 5, 15, 13]\n",
      "Shuffling training data...\n",
      "Shuffling training data...\n",
      "hyperparam. saved.\n",
      "Number of trainable variables 6\n",
      "[<tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(134, 512) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'out_W:0' shape=(128, 3) dtype=float32_ref>, <tf.Variable 'out_b:0' shape=(3,) dtype=float32_ref>]\n",
      "Sequence 970/3263200 (epoch 0), batch 100, train_loss = 4.186, time/batch = 0.108\n",
      "Sequence 1950/3263200 (epoch 0), batch 200, train_loss = 2.286, time/batch = 0.095\n",
      "Sequence 2930/3263200 (epoch 0), batch 300, train_loss = 3.615, time/batch = 0.094\n",
      "Sequence 3930/3263200 (epoch 0), batch 400, train_loss = 4.042, time/batch = 0.095\n",
      "Sequence 4930/3263200 (epoch 0), batch 500, train_loss = 4.330, time/batch = 0.094\n",
      "Sequence 5880/3263200 (epoch 0), batch 600, train_loss = 4.268, time/batch = 0.095\n",
      "Sequence 6880/3263200 (epoch 0), batch 700, train_loss = 2.973, time/batch = 0.095\n",
      "Sequence 7870/3263200 (epoch 0), batch 800, train_loss = 2.411, time/batch = 0.093\n",
      "Sequence 8850/3263200 (epoch 0), batch 900, train_loss = 3.407, time/batch = 0.093\n",
      "Sequence 9840/3263200 (epoch 0), batch 1000, train_loss = 4.088, time/batch = 0.094\n",
      "Sequence 10820/3263200 (epoch 0), batch 1100, train_loss = 1.720, time/batch = 0.093\n",
      "Sequence 11800/3263200 (epoch 0), batch 1200, train_loss = 2.454, time/batch = 0.094\n",
      "Sequence 12780/3263200 (epoch 0), batch 1300, train_loss = 3.065, time/batch = 0.094\n",
      "Sequence 13740/3263200 (epoch 0), batch 1400, train_loss = 4.247, time/batch = 0.095\n",
      "Sequence 14730/3263200 (epoch 0), batch 1500, train_loss = 2.695, time/batch = 0.094\n",
      "Sequence 15690/3263200 (epoch 0), batch 1600, train_loss = 2.586, time/batch = 0.094\n",
      "Sequence 16690/3263200 (epoch 0), batch 1700, train_loss = 2.341, time/batch = 0.095\n",
      "Sequence 17670/3263200 (epoch 0), batch 1800, train_loss = 3.955, time/batch = 0.096\n",
      "Sequence 18660/3263200 (epoch 0), batch 1900, train_loss = 1.354, time/batch = 0.093\n",
      "Sequence 19620/3263200 (epoch 0), batch 2000, train_loss = 1.549, time/batch = 0.095\n",
      "Sequence 20600/3263200 (epoch 0), batch 2101, train_loss = 2.529, time/batch = 0.093\n",
      "Sequence 21590/3263200 (epoch 0), batch 2201, train_loss = 2.873, time/batch = 0.094\n",
      "Sequence 22540/3263200 (epoch 0), batch 2301, train_loss = 1.240, time/batch = 0.096\n",
      "Sequence 23520/3263200 (epoch 0), batch 2401, train_loss = 4.256, time/batch = 0.094\n",
      "Sequence 24520/3263200 (epoch 0), batch 2501, train_loss = 1.645, time/batch = 0.094\n",
      "Sequence 25490/3263200 (epoch 0), batch 2601, train_loss = 2.163, time/batch = 0.094\n",
      "Sequence 26490/3263200 (epoch 0), batch 2701, train_loss = 3.712, time/batch = 0.093\n",
      "Sequence 27470/3263200 (epoch 0), batch 2801, train_loss = 3.314, time/batch = 0.095\n",
      "Sequence 28440/3263200 (epoch 0), batch 2901, train_loss = 3.120, time/batch = 0.094\n",
      "Sequence 29420/3263200 (epoch 0), batch 3001, train_loss = 2.386, time/batch = 0.093\n",
      "Sequence 30400/3263200 (epoch 0), batch 3101, train_loss = 2.373, time/batch = 0.094\n",
      "Sequence 31390/3263200 (epoch 0), batch 3201, train_loss = 3.026, time/batch = 0.094\n",
      "Sequence 32370/3263200 (epoch 0), batch 3301, train_loss = 1.189, time/batch = 0.093\n",
      "Epoch 0 completed, average train loss 3.261842, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 33352/3263200 (epoch 1), batch 3401, train_loss = 2.704, time/batch = 0.095\n",
      "Sequence 34312/3263200 (epoch 1), batch 3501, train_loss = 1.700, time/batch = 0.095\n",
      "Sequence 35292/3263200 (epoch 1), batch 3601, train_loss = 2.025, time/batch = 0.094\n",
      "Sequence 36252/3263200 (epoch 1), batch 3701, train_loss = 1.818, time/batch = 0.093\n",
      "Sequence 37252/3263200 (epoch 1), batch 3801, train_loss = 2.115, time/batch = 0.094\n",
      "Sequence 38232/3263200 (epoch 1), batch 3901, train_loss = 2.673, time/batch = 0.095\n",
      "Sequence 39222/3263200 (epoch 1), batch 4001, train_loss = 1.202, time/batch = 0.094\n",
      "Sequence 40192/3263200 (epoch 1), batch 4101, train_loss = 3.218, time/batch = 0.093\n",
      "Sequence 41182/3263200 (epoch 1), batch 4201, train_loss = 1.330, time/batch = 0.095\n",
      "Sequence 42162/3263200 (epoch 1), batch 4301, train_loss = 3.344, time/batch = 0.095\n",
      "Sequence 43112/3263200 (epoch 1), batch 4401, train_loss = 1.945, time/batch = 0.094\n",
      "Sequence 44102/3263200 (epoch 1), batch 4501, train_loss = 2.153, time/batch = 0.094\n",
      "Sequence 45072/3263200 (epoch 1), batch 4601, train_loss = 2.720, time/batch = 0.093\n",
      "Sequence 46052/3263200 (epoch 1), batch 4701, train_loss = 1.301, time/batch = 0.093\n",
      "Sequence 47032/3263200 (epoch 1), batch 4801, train_loss = 1.357, time/batch = 0.093\n",
      "Sequence 48012/3263200 (epoch 1), batch 4901, train_loss = 2.122, time/batch = 0.094\n",
      "Sequence 48992/3263200 (epoch 1), batch 5001, train_loss = 1.186, time/batch = 0.094\n",
      "Sequence 49972/3263200 (epoch 1), batch 5101, train_loss = 2.766, time/batch = 0.094\n",
      "Sequence 50962/3263200 (epoch 1), batch 5201, train_loss = 3.356, time/batch = 0.093\n",
      "Sequence 51952/3263200 (epoch 1), batch 5301, train_loss = 3.777, time/batch = 0.095\n",
      "Sequence 52932/3263200 (epoch 1), batch 5401, train_loss = 1.817, time/batch = 0.095\n",
      "Sequence 53932/3263200 (epoch 1), batch 5501, train_loss = 1.919, time/batch = 0.095\n",
      "Sequence 54912/3263200 (epoch 1), batch 5601, train_loss = 1.937, time/batch = 0.095\n",
      "Sequence 55902/3263200 (epoch 1), batch 5701, train_loss = 1.348, time/batch = 0.094\n",
      "Sequence 56902/3263200 (epoch 1), batch 5801, train_loss = 1.250, time/batch = 0.094\n",
      "Sequence 57882/3263200 (epoch 1), batch 5901, train_loss = 3.212, time/batch = 0.095\n",
      "Sequence 58852/3263200 (epoch 1), batch 6001, train_loss = 2.102, time/batch = 0.094\n",
      "Sequence 59842/3263200 (epoch 1), batch 6101, train_loss = 1.824, time/batch = 0.093\n",
      "Sequence 60832/3263200 (epoch 1), batch 6201, train_loss = 1.564, time/batch = 0.093\n",
      "Sequence 61812/3263200 (epoch 1), batch 6301, train_loss = 2.056, time/batch = 0.095\n",
      "Sequence 62792/3263200 (epoch 1), batch 6401, train_loss = 3.067, time/batch = 0.094\n",
      "Sequence 63742/3263200 (epoch 1), batch 6501, train_loss = 2.704, time/batch = 0.094\n",
      "Sequence 64732/3263200 (epoch 1), batch 6601, train_loss = 2.053, time/batch = 0.093\n",
      "Epoch 1 completed, average train loss 2.390208, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 65714/3263200 (epoch 2), batch 6701, train_loss = 1.197, time/batch = 0.096\n",
      "Sequence 66704/3263200 (epoch 2), batch 6801, train_loss = 5.578, time/batch = 0.095\n",
      "Sequence 67694/3263200 (epoch 2), batch 6901, train_loss = 2.820, time/batch = 0.094\n",
      "Sequence 68674/3263200 (epoch 2), batch 7001, train_loss = 1.337, time/batch = 0.093\n",
      "Sequence 69654/3263200 (epoch 2), batch 7101, train_loss = 1.473, time/batch = 0.094\n",
      "Sequence 70634/3263200 (epoch 2), batch 7201, train_loss = 3.403, time/batch = 0.094\n",
      "Sequence 71594/3263200 (epoch 2), batch 7301, train_loss = 2.302, time/batch = 0.093\n",
      "Sequence 72564/3263200 (epoch 2), batch 7401, train_loss = 2.537, time/batch = 0.096\n",
      "Sequence 73554/3263200 (epoch 2), batch 7501, train_loss = 2.267, time/batch = 0.094\n",
      "Sequence 74534/3263200 (epoch 2), batch 7601, train_loss = 1.809, time/batch = 0.094\n",
      "Sequence 75504/3263200 (epoch 2), batch 7701, train_loss = 2.398, time/batch = 0.094\n",
      "Sequence 76484/3263200 (epoch 2), batch 7801, train_loss = 1.800, time/batch = 0.094\n",
      "Sequence 77474/3263200 (epoch 2), batch 7901, train_loss = 2.255, time/batch = 0.095\n",
      "Sequence 78464/3263200 (epoch 2), batch 8002, train_loss = 1.486, time/batch = 0.094\n",
      "Sequence 79464/3263200 (epoch 2), batch 8102, train_loss = 1.966, time/batch = 0.094\n",
      "Sequence 80434/3263200 (epoch 2), batch 8202, train_loss = 1.648, time/batch = 0.093\n",
      "Sequence 81394/3263200 (epoch 2), batch 8302, train_loss = 2.410, time/batch = 0.094\n",
      "Sequence 82384/3263200 (epoch 2), batch 8402, train_loss = 2.048, time/batch = 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 83384/3263200 (epoch 2), batch 8502, train_loss = 2.331, time/batch = 0.094\n",
      "Sequence 84354/3263200 (epoch 2), batch 8602, train_loss = 1.765, time/batch = 0.093\n",
      "Sequence 85304/3263200 (epoch 2), batch 8702, train_loss = 2.527, time/batch = 0.095\n",
      "Sequence 86294/3263200 (epoch 2), batch 8802, train_loss = 2.301, time/batch = 0.096\n",
      "Sequence 87284/3263200 (epoch 2), batch 8902, train_loss = 1.647, time/batch = 0.095\n",
      "Sequence 88264/3263200 (epoch 2), batch 9002, train_loss = 2.033, time/batch = 0.093\n",
      "Sequence 89254/3263200 (epoch 2), batch 9102, train_loss = 1.749, time/batch = 0.094\n",
      "Sequence 90254/3263200 (epoch 2), batch 9202, train_loss = 1.141, time/batch = 0.095\n",
      "Sequence 91224/3263200 (epoch 2), batch 9302, train_loss = 1.364, time/batch = 0.094\n",
      "Sequence 92194/3263200 (epoch 2), batch 9402, train_loss = 1.182, time/batch = 0.095\n",
      "Sequence 93194/3263200 (epoch 2), batch 9502, train_loss = 2.777, time/batch = 0.095\n",
      "Sequence 94174/3263200 (epoch 2), batch 9602, train_loss = 1.545, time/batch = 0.094\n",
      "Sequence 95144/3263200 (epoch 2), batch 9702, train_loss = 2.027, time/batch = 0.095\n",
      "Sequence 96134/3263200 (epoch 2), batch 9802, train_loss = 1.728, time/batch = 0.095\n",
      "Sequence 97114/3263200 (epoch 2), batch 9902, train_loss = 2.456, time/batch = 0.095\n",
      "Epoch 2 completed, average train loss 2.160461, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 98106/3263200 (epoch 3), batch 10002, train_loss = 2.957, time/batch = 0.094\n",
      "Sequence 99086/3263200 (epoch 3), batch 10102, train_loss = 1.786, time/batch = 0.093\n",
      "Sequence 100056/3263200 (epoch 3), batch 10202, train_loss = 2.103, time/batch = 0.094\n",
      "Sequence 101026/3263200 (epoch 3), batch 10302, train_loss = 1.946, time/batch = 0.093\n",
      "Sequence 102016/3263200 (epoch 3), batch 10402, train_loss = 1.762, time/batch = 0.094\n",
      "Sequence 102996/3263200 (epoch 3), batch 10502, train_loss = 2.371, time/batch = 0.095\n",
      "Sequence 103986/3263200 (epoch 3), batch 10602, train_loss = 3.388, time/batch = 0.093\n",
      "Sequence 104976/3263200 (epoch 3), batch 10702, train_loss = 2.174, time/batch = 0.093\n",
      "Sequence 105956/3263200 (epoch 3), batch 10802, train_loss = 1.839, time/batch = 0.095\n",
      "Sequence 106936/3263200 (epoch 3), batch 10902, train_loss = 2.815, time/batch = 0.094\n",
      "Sequence 107916/3263200 (epoch 3), batch 11002, train_loss = 2.240, time/batch = 0.094\n",
      "Sequence 108886/3263200 (epoch 3), batch 11102, train_loss = 1.819, time/batch = 0.093\n",
      "Sequence 109866/3263200 (epoch 3), batch 11202, train_loss = 2.211, time/batch = 0.094\n",
      "Sequence 110866/3263200 (epoch 3), batch 11302, train_loss = 2.539, time/batch = 0.095\n",
      "Sequence 111846/3263200 (epoch 3), batch 11402, train_loss = 2.520, time/batch = 0.094\n",
      "Sequence 112826/3263200 (epoch 3), batch 11502, train_loss = 1.391, time/batch = 0.094\n",
      "Sequence 113786/3263200 (epoch 3), batch 11602, train_loss = 2.725, time/batch = 0.093\n",
      "Sequence 114766/3263200 (epoch 3), batch 11702, train_loss = 1.818, time/batch = 0.095\n",
      "Sequence 115746/3263200 (epoch 3), batch 11802, train_loss = 1.696, time/batch = 0.095\n",
      "Sequence 116746/3263200 (epoch 3), batch 11902, train_loss = 2.032, time/batch = 0.093\n",
      "Sequence 117726/3263200 (epoch 3), batch 12002, train_loss = 2.030, time/batch = 0.092\n",
      "Sequence 118706/3263200 (epoch 3), batch 12102, train_loss = 1.860, time/batch = 0.093\n",
      "Sequence 119686/3263200 (epoch 3), batch 12202, train_loss = 2.596, time/batch = 0.094\n",
      "Sequence 120676/3263200 (epoch 3), batch 12302, train_loss = 2.061, time/batch = 0.093\n",
      "Sequence 121656/3263200 (epoch 3), batch 12403, train_loss = 1.157, time/batch = 0.094\n",
      "Sequence 122636/3263200 (epoch 3), batch 12503, train_loss = 1.913, time/batch = 0.095\n",
      "Sequence 123626/3263200 (epoch 3), batch 12603, train_loss = 1.630, time/batch = 0.095\n",
      "Sequence 124616/3263200 (epoch 3), batch 12703, train_loss = 1.433, time/batch = 0.093\n",
      "Sequence 125596/3263200 (epoch 3), batch 12803, train_loss = 1.852, time/batch = 0.093\n",
      "Sequence 126576/3263200 (epoch 3), batch 12903, train_loss = 2.012, time/batch = 0.094\n",
      "Sequence 127576/3263200 (epoch 3), batch 13003, train_loss = 1.553, time/batch = 0.095\n",
      "Sequence 128546/3263200 (epoch 3), batch 13103, train_loss = 2.084, time/batch = 0.093\n",
      "Sequence 129526/3263200 (epoch 3), batch 13203, train_loss = 1.416, time/batch = 0.094\n",
      "Sequence 130476/3263200 (epoch 3), batch 13303, train_loss = 1.329, time/batch = 0.095\n",
      "Epoch 3 completed, average train loss 2.011759, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 131478/3263200 (epoch 4), batch 13403, train_loss = 1.787, time/batch = 0.095\n",
      "Sequence 132438/3263200 (epoch 4), batch 13503, train_loss = 1.973, time/batch = 0.094\n",
      "Sequence 133428/3263200 (epoch 4), batch 13603, train_loss = 2.319, time/batch = 0.094\n",
      "Sequence 134408/3263200 (epoch 4), batch 13703, train_loss = 2.195, time/batch = 0.095\n",
      "Sequence 135378/3263200 (epoch 4), batch 13803, train_loss = 1.669, time/batch = 0.095\n",
      "Sequence 136348/3263200 (epoch 4), batch 13903, train_loss = 1.328, time/batch = 0.095\n",
      "Sequence 137318/3263200 (epoch 4), batch 14003, train_loss = 2.221, time/batch = 0.094\n",
      "Sequence 138278/3263200 (epoch 4), batch 14103, train_loss = 1.627, time/batch = 0.094\n",
      "Sequence 139268/3263200 (epoch 4), batch 14203, train_loss = 1.685, time/batch = 0.095\n",
      "Sequence 140268/3263200 (epoch 4), batch 14303, train_loss = 1.260, time/batch = 0.095\n",
      "Sequence 141258/3263200 (epoch 4), batch 14403, train_loss = 1.985, time/batch = 0.095\n",
      "Sequence 142238/3263200 (epoch 4), batch 14504, train_loss = 1.295, time/batch = 0.096\n",
      "Sequence 143218/3263200 (epoch 4), batch 14604, train_loss = 1.893, time/batch = 0.095\n",
      "Sequence 144208/3263200 (epoch 4), batch 14704, train_loss = 3.444, time/batch = 0.094\n",
      "Sequence 145198/3263200 (epoch 4), batch 14804, train_loss = 2.298, time/batch = 0.095\n",
      "Sequence 146198/3263200 (epoch 4), batch 14904, train_loss = 2.945, time/batch = 0.093\n",
      "Sequence 147148/3263200 (epoch 4), batch 15004, train_loss = 3.363, time/batch = 0.094\n",
      "Sequence 148108/3263200 (epoch 4), batch 15104, train_loss = 3.834, time/batch = 0.095\n",
      "Sequence 149108/3263200 (epoch 4), batch 15204, train_loss = 1.419, time/batch = 0.094\n",
      "Sequence 150088/3263200 (epoch 4), batch 15304, train_loss = 1.669, time/batch = 0.093\n",
      "Sequence 151068/3263200 (epoch 4), batch 15404, train_loss = 1.685, time/batch = 0.095\n",
      "Sequence 152048/3263200 (epoch 4), batch 15504, train_loss = 2.385, time/batch = 0.093\n",
      "Sequence 153028/3263200 (epoch 4), batch 15604, train_loss = 2.030, time/batch = 0.093\n",
      "Sequence 154008/3263200 (epoch 4), batch 15704, train_loss = 2.132, time/batch = 0.094\n",
      "Sequence 155008/3263200 (epoch 4), batch 15804, train_loss = 1.899, time/batch = 0.095\n",
      "Sequence 155998/3263200 (epoch 4), batch 15904, train_loss = 1.579, time/batch = 0.094\n",
      "Sequence 156998/3263200 (epoch 4), batch 16004, train_loss = 1.662, time/batch = 0.095\n",
      "Sequence 157998/3263200 (epoch 4), batch 16104, train_loss = 1.337, time/batch = 0.094\n",
      "Sequence 158978/3263200 (epoch 4), batch 16204, train_loss = 1.636, time/batch = 0.094\n",
      "Sequence 159948/3263200 (epoch 4), batch 16304, train_loss = 1.843, time/batch = 0.094\n",
      "Sequence 160918/3263200 (epoch 4), batch 16404, train_loss = 1.261, time/batch = 0.093\n",
      "Sequence 161888/3263200 (epoch 4), batch 16504, train_loss = 1.493, time/batch = 0.094\n",
      "Sequence 162848/3263200 (epoch 4), batch 16604, train_loss = 3.978, time/batch = 0.094\n",
      "Epoch 4 completed, average train loss 1.878934, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 163850/3263200 (epoch 5), batch 16704, train_loss = 1.642, time/batch = 0.097\n",
      "Sequence 164830/3263200 (epoch 5), batch 16804, train_loss = 2.778, time/batch = 0.092\n",
      "Sequence 165810/3263200 (epoch 5), batch 16904, train_loss = 2.541, time/batch = 0.091\n",
      "Sequence 166790/3263200 (epoch 5), batch 17004, train_loss = 1.799, time/batch = 0.094\n",
      "Sequence 167760/3263200 (epoch 5), batch 17104, train_loss = 1.886, time/batch = 0.094\n",
      "Sequence 168760/3263200 (epoch 5), batch 17204, train_loss = 1.701, time/batch = 0.094\n",
      "Sequence 169720/3263200 (epoch 5), batch 17304, train_loss = 3.796, time/batch = 0.095\n",
      "Sequence 170700/3263200 (epoch 5), batch 17404, train_loss = 2.041, time/batch = 0.093\n",
      "Sequence 171690/3263200 (epoch 5), batch 17504, train_loss = 1.648, time/batch = 0.095\n",
      "Sequence 172690/3263200 (epoch 5), batch 17604, train_loss = 1.507, time/batch = 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 173660/3263200 (epoch 5), batch 17704, train_loss = 1.255, time/batch = 0.095\n",
      "Sequence 174630/3263200 (epoch 5), batch 17804, train_loss = 1.523, time/batch = 0.094\n",
      "Sequence 175620/3263200 (epoch 5), batch 17904, train_loss = 3.230, time/batch = 0.094\n",
      "Sequence 176610/3263200 (epoch 5), batch 18004, train_loss = 1.991, time/batch = 0.095\n",
      "Sequence 177580/3263200 (epoch 5), batch 18104, train_loss = 0.933, time/batch = 0.094\n",
      "Sequence 178570/3263200 (epoch 5), batch 18204, train_loss = 1.281, time/batch = 0.094\n",
      "Sequence 179540/3263200 (epoch 5), batch 18304, train_loss = 0.804, time/batch = 0.094\n",
      "Sequence 180510/3263200 (epoch 5), batch 18404, train_loss = 1.353, time/batch = 0.093\n",
      "Sequence 181490/3263200 (epoch 5), batch 18504, train_loss = 1.549, time/batch = 0.094\n",
      "Sequence 182450/3263200 (epoch 5), batch 18604, train_loss = 1.190, time/batch = 0.094\n",
      "Sequence 183440/3263200 (epoch 5), batch 18704, train_loss = 2.450, time/batch = 0.095\n",
      "Sequence 184410/3263200 (epoch 5), batch 18804, train_loss = 1.503, time/batch = 0.093\n",
      "Sequence 185400/3263200 (epoch 5), batch 18904, train_loss = 1.638, time/batch = 0.095\n",
      "Sequence 186390/3263200 (epoch 5), batch 19004, train_loss = 1.686, time/batch = 0.093\n",
      "Sequence 187370/3263200 (epoch 5), batch 19104, train_loss = 1.438, time/batch = 0.093\n",
      "Sequence 188350/3263200 (epoch 5), batch 19204, train_loss = 1.539, time/batch = 0.096\n",
      "Sequence 189320/3263200 (epoch 5), batch 19304, train_loss = 1.547, time/batch = 0.094\n",
      "Sequence 190310/3263200 (epoch 5), batch 19404, train_loss = 2.175, time/batch = 0.093\n",
      "Sequence 191280/3263200 (epoch 5), batch 19504, train_loss = 1.038, time/batch = 0.095\n",
      "Sequence 192280/3263200 (epoch 5), batch 19604, train_loss = 1.513, time/batch = 0.093\n",
      "Sequence 193260/3263200 (epoch 5), batch 19704, train_loss = 1.764, time/batch = 0.094\n",
      "Sequence 194240/3263200 (epoch 5), batch 19804, train_loss = 1.342, time/batch = 0.094\n",
      "Sequence 195230/3263200 (epoch 5), batch 19904, train_loss = 2.271, time/batch = 0.096\n",
      "Epoch 5 completed, average train loss 1.775755, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 196202/3263200 (epoch 6), batch 20004, train_loss = 1.381, time/batch = 0.095\n",
      "Sequence 197182/3263200 (epoch 6), batch 20104, train_loss = 1.620, time/batch = 0.094\n",
      "Sequence 198152/3263200 (epoch 6), batch 20204, train_loss = 1.215, time/batch = 0.095\n",
      "Sequence 199132/3263200 (epoch 6), batch 20304, train_loss = 1.923, time/batch = 0.095\n",
      "Sequence 200112/3263200 (epoch 6), batch 20404, train_loss = 2.223, time/batch = 0.095\n",
      "Sequence 201112/3263200 (epoch 6), batch 20504, train_loss = 2.578, time/batch = 0.096\n",
      "Sequence 202112/3263200 (epoch 6), batch 20604, train_loss = 0.902, time/batch = 0.095\n",
      "Sequence 203112/3263200 (epoch 6), batch 20704, train_loss = 2.091, time/batch = 0.094\n",
      "Sequence 204042/3263200 (epoch 6), batch 20804, train_loss = 1.299, time/batch = 0.095\n",
      "Sequence 205002/3263200 (epoch 6), batch 20904, train_loss = 1.161, time/batch = 0.093\n",
      "Sequence 205982/3263200 (epoch 6), batch 21004, train_loss = 1.422, time/batch = 0.094\n",
      "Sequence 206972/3263200 (epoch 6), batch 21104, train_loss = 0.938, time/batch = 0.093\n",
      "Sequence 207952/3263200 (epoch 6), batch 21204, train_loss = 1.604, time/batch = 0.095\n",
      "Sequence 208942/3263200 (epoch 6), batch 21304, train_loss = 1.027, time/batch = 0.094\n",
      "Sequence 209922/3263200 (epoch 6), batch 21404, train_loss = 1.206, time/batch = 0.094\n",
      "Sequence 210902/3263200 (epoch 6), batch 21504, train_loss = 1.068, time/batch = 0.095\n",
      "Sequence 211902/3263200 (epoch 6), batch 21604, train_loss = 2.185, time/batch = 0.094\n",
      "Sequence 212862/3263200 (epoch 6), batch 21704, train_loss = 1.130, time/batch = 0.094\n",
      "Sequence 213842/3263200 (epoch 6), batch 21804, train_loss = 1.528, time/batch = 0.094\n",
      "Sequence 214822/3263200 (epoch 6), batch 21904, train_loss = 1.524, time/batch = 0.093\n",
      "Sequence 215812/3263200 (epoch 6), batch 22004, train_loss = 1.458, time/batch = 0.095\n",
      "Sequence 216782/3263200 (epoch 6), batch 22104, train_loss = 2.161, time/batch = 0.095\n",
      "Sequence 217782/3263200 (epoch 6), batch 22204, train_loss = 1.462, time/batch = 0.095\n",
      "Sequence 218752/3263200 (epoch 6), batch 22304, train_loss = 2.514, time/batch = 0.095\n",
      "Sequence 219722/3263200 (epoch 6), batch 22404, train_loss = 2.611, time/batch = 0.095\n",
      "Sequence 220702/3263200 (epoch 6), batch 22504, train_loss = 1.381, time/batch = 0.096\n",
      "Sequence 221702/3263200 (epoch 6), batch 22604, train_loss = 2.322, time/batch = 0.095\n",
      "Sequence 222692/3263200 (epoch 6), batch 22704, train_loss = 1.280, time/batch = 0.094\n",
      "Sequence 223672/3263200 (epoch 6), batch 22804, train_loss = 1.176, time/batch = 0.094\n",
      "Sequence 224662/3263200 (epoch 6), batch 22904, train_loss = 1.166, time/batch = 0.095\n",
      "Sequence 225642/3263200 (epoch 6), batch 23004, train_loss = 1.529, time/batch = 0.093\n",
      "Sequence 226622/3263200 (epoch 6), batch 23104, train_loss = 4.579, time/batch = 0.095\n",
      "Sequence 227582/3263200 (epoch 6), batch 23204, train_loss = 1.505, time/batch = 0.094\n",
      "Epoch 6 completed, average train loss 1.708166, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 228574/3263200 (epoch 7), batch 23304, train_loss = 1.860, time/batch = 0.095\n",
      "Sequence 229574/3263200 (epoch 7), batch 23404, train_loss = 1.913, time/batch = 0.093\n",
      "Sequence 230564/3263200 (epoch 7), batch 23504, train_loss = 2.493, time/batch = 0.094\n",
      "Sequence 231544/3263200 (epoch 7), batch 23604, train_loss = 1.378, time/batch = 0.095\n",
      "Sequence 232504/3263200 (epoch 7), batch 23704, train_loss = 1.141, time/batch = 0.094\n",
      "Sequence 233504/3263200 (epoch 7), batch 23804, train_loss = 2.015, time/batch = 0.094\n",
      "Sequence 234504/3263200 (epoch 7), batch 23904, train_loss = 2.167, time/batch = 0.094\n",
      "Sequence 235484/3263200 (epoch 7), batch 24004, train_loss = 1.097, time/batch = 0.094\n",
      "Sequence 236464/3263200 (epoch 7), batch 24104, train_loss = 1.758, time/batch = 0.095\n",
      "Sequence 237434/3263200 (epoch 7), batch 24204, train_loss = 1.972, time/batch = 0.095\n",
      "Sequence 238414/3263200 (epoch 7), batch 24304, train_loss = 1.428, time/batch = 0.094\n",
      "Sequence 239374/3263200 (epoch 7), batch 24404, train_loss = 1.390, time/batch = 0.094\n",
      "Sequence 240364/3263200 (epoch 7), batch 24504, train_loss = 1.297, time/batch = 0.093\n",
      "Sequence 241334/3263200 (epoch 7), batch 24604, train_loss = 1.687, time/batch = 0.095\n",
      "Sequence 242324/3263200 (epoch 7), batch 24704, train_loss = 0.826, time/batch = 0.095\n",
      "Sequence 243314/3263200 (epoch 7), batch 24804, train_loss = 1.579, time/batch = 0.094\n",
      "Sequence 244284/3263200 (epoch 7), batch 24904, train_loss = 2.155, time/batch = 0.094\n",
      "Sequence 245244/3263200 (epoch 7), batch 25004, train_loss = 2.023, time/batch = 0.095\n",
      "Sequence 246234/3263200 (epoch 7), batch 25104, train_loss = 1.782, time/batch = 0.095\n",
      "Sequence 247204/3263200 (epoch 7), batch 25204, train_loss = 0.989, time/batch = 0.094\n",
      "Sequence 248194/3263200 (epoch 7), batch 25304, train_loss = 1.970, time/batch = 0.094\n",
      "Sequence 249184/3263200 (epoch 7), batch 25404, train_loss = 1.755, time/batch = 0.094\n",
      "Sequence 250174/3263200 (epoch 7), batch 25504, train_loss = 1.368, time/batch = 0.095\n",
      "Sequence 251174/3263200 (epoch 7), batch 25604, train_loss = 2.178, time/batch = 0.094\n",
      "Sequence 252144/3263200 (epoch 7), batch 25704, train_loss = 1.479, time/batch = 0.094\n",
      "Sequence 253114/3263200 (epoch 7), batch 25804, train_loss = 1.631, time/batch = 0.094\n",
      "Sequence 254104/3263200 (epoch 7), batch 25904, train_loss = 1.377, time/batch = 0.095\n",
      "Sequence 255084/3263200 (epoch 7), batch 26004, train_loss = 1.203, time/batch = 0.094\n",
      "Sequence 256054/3263200 (epoch 7), batch 26104, train_loss = 1.494, time/batch = 0.094\n",
      "Sequence 257024/3263200 (epoch 7), batch 26204, train_loss = 2.440, time/batch = 0.093\n",
      "Sequence 258024/3263200 (epoch 7), batch 26304, train_loss = 1.204, time/batch = 0.094\n",
      "Sequence 259004/3263200 (epoch 7), batch 26404, train_loss = 2.236, time/batch = 0.093\n",
      "Sequence 259974/3263200 (epoch 7), batch 26504, train_loss = 1.032, time/batch = 0.093\n",
      "Sequence 260944/3263200 (epoch 7), batch 26604, train_loss = 1.164, time/batch = 0.094\n",
      "Epoch 7 completed, average train loss 1.662538, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 261906/3263200 (epoch 8), batch 26704, train_loss = 1.133, time/batch = 0.096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 262906/3263200 (epoch 8), batch 26804, train_loss = 2.461, time/batch = 0.094\n",
      "Sequence 263846/3263200 (epoch 8), batch 26904, train_loss = 1.945, time/batch = 0.094\n",
      "Sequence 264826/3263200 (epoch 8), batch 27004, train_loss = 1.693, time/batch = 0.095\n",
      "Sequence 265806/3263200 (epoch 8), batch 27104, train_loss = 2.122, time/batch = 0.096\n",
      "Sequence 266776/3263200 (epoch 8), batch 27204, train_loss = 1.324, time/batch = 0.095\n",
      "Sequence 267746/3263200 (epoch 8), batch 27304, train_loss = 1.973, time/batch = 0.095\n",
      "Sequence 268726/3263200 (epoch 8), batch 27404, train_loss = 1.596, time/batch = 0.096\n",
      "Sequence 269726/3263200 (epoch 8), batch 27504, train_loss = 3.005, time/batch = 0.095\n",
      "Sequence 270706/3263200 (epoch 8), batch 27604, train_loss = 1.334, time/batch = 0.095\n",
      "Sequence 271706/3263200 (epoch 8), batch 27704, train_loss = 3.468, time/batch = 0.096\n",
      "Sequence 272686/3263200 (epoch 8), batch 27804, train_loss = 2.609, time/batch = 0.093\n",
      "Sequence 273666/3263200 (epoch 8), batch 27904, train_loss = 1.870, time/batch = 0.093\n",
      "Sequence 274666/3263200 (epoch 8), batch 28004, train_loss = 1.362, time/batch = 0.095\n",
      "Sequence 275636/3263200 (epoch 8), batch 28104, train_loss = 0.862, time/batch = 0.095\n",
      "Sequence 276596/3263200 (epoch 8), batch 28204, train_loss = 1.663, time/batch = 0.094\n",
      "Sequence 277576/3263200 (epoch 8), batch 28304, train_loss = 1.145, time/batch = 0.093\n",
      "Sequence 278556/3263200 (epoch 8), batch 28404, train_loss = 1.685, time/batch = 0.095\n",
      "Sequence 279516/3263200 (epoch 8), batch 28504, train_loss = 1.555, time/batch = 0.094\n",
      "Sequence 280486/3263200 (epoch 8), batch 28604, train_loss = 1.029, time/batch = 0.094\n",
      "Sequence 281476/3263200 (epoch 8), batch 28704, train_loss = 1.875, time/batch = 0.095\n",
      "Sequence 282456/3263200 (epoch 8), batch 28804, train_loss = 1.531, time/batch = 0.096\n",
      "Sequence 283436/3263200 (epoch 8), batch 28904, train_loss = 1.422, time/batch = 0.094\n",
      "Sequence 284406/3263200 (epoch 8), batch 29004, train_loss = 1.622, time/batch = 0.093\n",
      "Sequence 285406/3263200 (epoch 8), batch 29104, train_loss = 1.154, time/batch = 0.094\n",
      "Sequence 286406/3263200 (epoch 8), batch 29204, train_loss = 1.075, time/batch = 0.095\n",
      "Sequence 287376/3263200 (epoch 8), batch 29304, train_loss = 1.545, time/batch = 0.094\n",
      "Sequence 288366/3263200 (epoch 8), batch 29404, train_loss = 1.890, time/batch = 0.094\n",
      "Sequence 289356/3263200 (epoch 8), batch 29504, train_loss = 1.069, time/batch = 0.094\n",
      "Sequence 290346/3263200 (epoch 8), batch 29604, train_loss = 1.341, time/batch = 0.096\n",
      "Sequence 291336/3263200 (epoch 8), batch 29704, train_loss = 0.945, time/batch = 0.096\n",
      "Sequence 292316/3263200 (epoch 8), batch 29804, train_loss = 2.013, time/batch = 0.094\n",
      "Sequence 293306/3263200 (epoch 8), batch 29904, train_loss = 1.254, time/batch = 0.094\n",
      "Epoch 8 completed, average train loss 1.627936, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 294298/3263200 (epoch 9), batch 30004, train_loss = 1.159, time/batch = 0.096\n",
      "Sequence 295258/3263200 (epoch 9), batch 30104, train_loss = 1.042, time/batch = 0.094\n",
      "Sequence 296238/3263200 (epoch 9), batch 30204, train_loss = 1.536, time/batch = 0.093\n",
      "Sequence 297228/3263200 (epoch 9), batch 30304, train_loss = 1.189, time/batch = 0.094\n",
      "Sequence 298208/3263200 (epoch 9), batch 30404, train_loss = 1.310, time/batch = 0.094\n",
      "Sequence 299188/3263200 (epoch 9), batch 30504, train_loss = 1.519, time/batch = 0.094\n",
      "Sequence 300188/3263200 (epoch 9), batch 30604, train_loss = 1.809, time/batch = 0.094\n",
      "Sequence 301188/3263200 (epoch 9), batch 30704, train_loss = 2.096, time/batch = 0.094\n",
      "Sequence 302148/3263200 (epoch 9), batch 30804, train_loss = 1.665, time/batch = 0.094\n",
      "Sequence 303138/3263200 (epoch 9), batch 30904, train_loss = 0.995, time/batch = 0.095\n",
      "Sequence 304138/3263200 (epoch 9), batch 31004, train_loss = 1.367, time/batch = 0.093\n",
      "Sequence 305118/3263200 (epoch 9), batch 31104, train_loss = 2.283, time/batch = 0.094\n",
      "Sequence 306098/3263200 (epoch 9), batch 31204, train_loss = 2.364, time/batch = 0.093\n",
      "Sequence 307078/3263200 (epoch 9), batch 31304, train_loss = 1.617, time/batch = 0.094\n",
      "Sequence 308078/3263200 (epoch 9), batch 31404, train_loss = 1.326, time/batch = 0.095\n",
      "Sequence 309058/3263200 (epoch 9), batch 31504, train_loss = 2.087, time/batch = 0.093\n",
      "Sequence 310048/3263200 (epoch 9), batch 31604, train_loss = 0.869, time/batch = 0.095\n",
      "Sequence 311048/3263200 (epoch 9), batch 31704, train_loss = 1.490, time/batch = 0.093\n",
      "Sequence 312028/3263200 (epoch 9), batch 31804, train_loss = 1.723, time/batch = 0.094\n",
      "Sequence 312988/3263200 (epoch 9), batch 31904, train_loss = 1.207, time/batch = 0.094\n",
      "Sequence 313948/3263200 (epoch 9), batch 32004, train_loss = 1.966, time/batch = 0.095\n",
      "Sequence 314938/3263200 (epoch 9), batch 32104, train_loss = 2.151, time/batch = 0.093\n",
      "Sequence 315898/3263200 (epoch 9), batch 32204, train_loss = 1.735, time/batch = 0.092\n",
      "Sequence 316838/3263200 (epoch 9), batch 32304, train_loss = 2.084, time/batch = 0.093\n",
      "Sequence 317828/3263200 (epoch 9), batch 32404, train_loss = 1.173, time/batch = 0.095\n",
      "Sequence 318808/3263200 (epoch 9), batch 32504, train_loss = 1.098, time/batch = 0.096\n",
      "Sequence 319788/3263200 (epoch 9), batch 32604, train_loss = 1.791, time/batch = 0.094\n",
      "Sequence 320758/3263200 (epoch 9), batch 32704, train_loss = 1.369, time/batch = 0.094\n",
      "Sequence 321748/3263200 (epoch 9), batch 32804, train_loss = 0.868, time/batch = 0.094\n",
      "Sequence 322728/3263200 (epoch 9), batch 32904, train_loss = 1.829, time/batch = 0.096\n",
      "Sequence 323718/3263200 (epoch 9), batch 33004, train_loss = 2.256, time/batch = 0.094\n",
      "Sequence 324688/3263200 (epoch 9), batch 33104, train_loss = 1.298, time/batch = 0.094\n",
      "Sequence 325668/3263200 (epoch 9), batch 33204, train_loss = 1.908, time/batch = 0.095\n",
      "Epoch 9 completed, average train loss 1.598107, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 326670/3263200 (epoch 10), batch 33304, train_loss = 1.983, time/batch = 0.096\n",
      "Sequence 327670/3263200 (epoch 10), batch 33404, train_loss = 2.569, time/batch = 0.093\n",
      "Sequence 328630/3263200 (epoch 10), batch 33504, train_loss = 1.961, time/batch = 0.097\n",
      "Sequence 329600/3263200 (epoch 10), batch 33604, train_loss = 1.407, time/batch = 0.094\n",
      "Sequence 330580/3263200 (epoch 10), batch 33704, train_loss = 1.485, time/batch = 0.095\n",
      "Sequence 331570/3263200 (epoch 10), batch 33804, train_loss = 1.613, time/batch = 0.095\n",
      "Sequence 332540/3263200 (epoch 10), batch 33904, train_loss = 1.065, time/batch = 0.096\n",
      "Sequence 333520/3263200 (epoch 10), batch 34004, train_loss = 1.946, time/batch = 0.094\n",
      "Sequence 334510/3263200 (epoch 10), batch 34104, train_loss = 1.580, time/batch = 0.094\n",
      "Sequence 335490/3263200 (epoch 10), batch 34204, train_loss = 1.068, time/batch = 0.095\n",
      "Sequence 336480/3263200 (epoch 10), batch 34304, train_loss = 0.919, time/batch = 0.094\n",
      "Sequence 337450/3263200 (epoch 10), batch 34404, train_loss = 1.543, time/batch = 0.094\n",
      "Sequence 338440/3263200 (epoch 10), batch 34504, train_loss = 1.211, time/batch = 0.094\n",
      "Sequence 339420/3263200 (epoch 10), batch 34604, train_loss = 1.398, time/batch = 0.093\n",
      "Sequence 340410/3263200 (epoch 10), batch 34704, train_loss = 1.205, time/batch = 0.094\n",
      "Sequence 341400/3263200 (epoch 10), batch 34804, train_loss = 1.160, time/batch = 0.094\n",
      "Sequence 342370/3263200 (epoch 10), batch 34904, train_loss = 1.255, time/batch = 0.093\n",
      "Sequence 343370/3263200 (epoch 10), batch 35004, train_loss = 1.283, time/batch = 0.094\n",
      "Sequence 344320/3263200 (epoch 10), batch 35104, train_loss = 1.436, time/batch = 0.095\n",
      "Sequence 345300/3263200 (epoch 10), batch 35204, train_loss = 2.276, time/batch = 0.094\n",
      "Sequence 346300/3263200 (epoch 10), batch 35304, train_loss = 1.972, time/batch = 0.095\n",
      "Sequence 347290/3263200 (epoch 10), batch 35404, train_loss = 0.962, time/batch = 0.095\n",
      "Sequence 348290/3263200 (epoch 10), batch 35504, train_loss = 1.568, time/batch = 0.094\n",
      "Sequence 349270/3263200 (epoch 10), batch 35604, train_loss = 1.112, time/batch = 0.094\n",
      "Sequence 350260/3263200 (epoch 10), batch 35704, train_loss = 1.060, time/batch = 0.094\n",
      "Sequence 351250/3263200 (epoch 10), batch 35804, train_loss = 1.215, time/batch = 0.094\n",
      "Sequence 352250/3263200 (epoch 10), batch 35904, train_loss = 2.313, time/batch = 0.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 353230/3263200 (epoch 10), batch 36004, train_loss = 1.687, time/batch = 0.094\n",
      "Sequence 354200/3263200 (epoch 10), batch 36104, train_loss = 1.675, time/batch = 0.094\n",
      "Sequence 355180/3263200 (epoch 10), batch 36204, train_loss = 1.826, time/batch = 0.089\n",
      "Sequence 356140/3263200 (epoch 10), batch 36304, train_loss = 1.350, time/batch = 0.094\n",
      "Sequence 357120/3263200 (epoch 10), batch 36404, train_loss = 1.363, time/batch = 0.095\n",
      "Sequence 358090/3263200 (epoch 10), batch 36504, train_loss = 2.262, time/batch = 0.094\n",
      "Epoch 10 completed, average train loss 1.572067, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 359062/3263200 (epoch 11), batch 36604, train_loss = 1.757, time/batch = 0.095\n",
      "Sequence 360032/3263200 (epoch 11), batch 36704, train_loss = 1.901, time/batch = 0.094\n",
      "Sequence 360992/3263200 (epoch 11), batch 36804, train_loss = 1.203, time/batch = 0.095\n",
      "Sequence 361972/3263200 (epoch 11), batch 36904, train_loss = 1.321, time/batch = 0.094\n",
      "Sequence 362952/3263200 (epoch 11), batch 37004, train_loss = 1.238, time/batch = 0.094\n",
      "Sequence 363952/3263200 (epoch 11), batch 37104, train_loss = 0.829, time/batch = 0.093\n",
      "Sequence 364942/3263200 (epoch 11), batch 37204, train_loss = 2.047, time/batch = 0.093\n",
      "Sequence 365922/3263200 (epoch 11), batch 37304, train_loss = 1.586, time/batch = 0.093\n",
      "Sequence 366912/3263200 (epoch 11), batch 37404, train_loss = 1.922, time/batch = 0.094\n",
      "Sequence 367882/3263200 (epoch 11), batch 37504, train_loss = 2.173, time/batch = 0.094\n",
      "Sequence 368872/3263200 (epoch 11), batch 37604, train_loss = 1.621, time/batch = 0.094\n",
      "Sequence 369872/3263200 (epoch 11), batch 37704, train_loss = 1.917, time/batch = 0.093\n",
      "Sequence 370852/3263200 (epoch 11), batch 37804, train_loss = 2.493, time/batch = 0.094\n",
      "Sequence 371842/3263200 (epoch 11), batch 37904, train_loss = 1.121, time/batch = 0.094\n",
      "Sequence 372812/3263200 (epoch 11), batch 38004, train_loss = 1.584, time/batch = 0.094\n",
      "Sequence 373792/3263200 (epoch 11), batch 38104, train_loss = 1.221, time/batch = 0.096\n",
      "Sequence 374792/3263200 (epoch 11), batch 38204, train_loss = 1.885, time/batch = 0.094\n",
      "Sequence 375762/3263200 (epoch 11), batch 38304, train_loss = 2.353, time/batch = 0.095\n",
      "Sequence 376762/3263200 (epoch 11), batch 38404, train_loss = 1.058, time/batch = 0.094\n",
      "Sequence 377722/3263200 (epoch 11), batch 38504, train_loss = 1.574, time/batch = 0.094\n",
      "Sequence 378692/3263200 (epoch 11), batch 38604, train_loss = 1.600, time/batch = 0.094\n",
      "Sequence 379682/3263200 (epoch 11), batch 38705, train_loss = 0.863, time/batch = 0.094\n",
      "Sequence 380682/3263200 (epoch 11), batch 38805, train_loss = 2.253, time/batch = 0.094\n",
      "Sequence 381682/3263200 (epoch 11), batch 38905, train_loss = 0.911, time/batch = 0.094\n",
      "Sequence 382662/3263200 (epoch 11), batch 39005, train_loss = 1.487, time/batch = 0.093\n",
      "Sequence 383632/3263200 (epoch 11), batch 39105, train_loss = 1.081, time/batch = 0.094\n",
      "Sequence 384622/3263200 (epoch 11), batch 39205, train_loss = 1.734, time/batch = 0.094\n",
      "Sequence 385612/3263200 (epoch 11), batch 39305, train_loss = 1.443, time/batch = 0.093\n",
      "Sequence 386592/3263200 (epoch 11), batch 39405, train_loss = 1.198, time/batch = 0.094\n",
      "Sequence 387552/3263200 (epoch 11), batch 39505, train_loss = 1.477, time/batch = 0.093\n",
      "Sequence 388522/3263200 (epoch 11), batch 39605, train_loss = 2.048, time/batch = 0.094\n",
      "Sequence 389492/3263200 (epoch 11), batch 39705, train_loss = 1.323, time/batch = 0.094\n",
      "Sequence 390442/3263200 (epoch 11), batch 39805, train_loss = 2.677, time/batch = 0.094\n",
      "Sequence 391442/3263200 (epoch 11), batch 39905, train_loss = 1.360, time/batch = 0.088\n",
      "Epoch 11 completed, average train loss 1.551998, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 392444/3263200 (epoch 12), batch 40005, train_loss = 1.843, time/batch = 0.095\n",
      "Sequence 393424/3263200 (epoch 12), batch 40105, train_loss = 1.673, time/batch = 0.095\n",
      "Sequence 394414/3263200 (epoch 12), batch 40205, train_loss = 1.263, time/batch = 0.094\n",
      "Sequence 395404/3263200 (epoch 12), batch 40305, train_loss = 2.068, time/batch = 0.097\n",
      "Sequence 396364/3263200 (epoch 12), batch 40405, train_loss = 1.206, time/batch = 0.094\n",
      "Sequence 397354/3263200 (epoch 12), batch 40505, train_loss = 1.113, time/batch = 0.094\n",
      "Sequence 398324/3263200 (epoch 12), batch 40605, train_loss = 2.406, time/batch = 0.094\n",
      "Sequence 399294/3263200 (epoch 12), batch 40705, train_loss = 0.754, time/batch = 0.094\n",
      "Sequence 400284/3263200 (epoch 12), batch 40805, train_loss = 1.435, time/batch = 0.096\n",
      "Sequence 401264/3263200 (epoch 12), batch 40905, train_loss = 1.090, time/batch = 0.094\n",
      "Sequence 402254/3263200 (epoch 12), batch 41005, train_loss = 1.649, time/batch = 0.094\n",
      "Sequence 403234/3263200 (epoch 12), batch 41105, train_loss = 1.593, time/batch = 0.093\n",
      "Sequence 404224/3263200 (epoch 12), batch 41205, train_loss = 1.359, time/batch = 0.094\n",
      "Sequence 405184/3263200 (epoch 12), batch 41305, train_loss = 3.085, time/batch = 0.094\n",
      "Sequence 406164/3263200 (epoch 12), batch 41405, train_loss = 2.282, time/batch = 0.093\n",
      "Sequence 407154/3263200 (epoch 12), batch 41505, train_loss = 1.350, time/batch = 0.095\n",
      "Sequence 408144/3263200 (epoch 12), batch 41606, train_loss = 3.166, time/batch = 0.094\n",
      "Sequence 409134/3263200 (epoch 12), batch 41706, train_loss = 2.188, time/batch = 0.094\n",
      "Sequence 410114/3263200 (epoch 12), batch 41806, train_loss = 1.879, time/batch = 0.093\n",
      "Sequence 411104/3263200 (epoch 12), batch 41906, train_loss = 1.672, time/batch = 0.092\n",
      "Sequence 412064/3263200 (epoch 12), batch 42006, train_loss = 1.179, time/batch = 0.095\n",
      "Sequence 413014/3263200 (epoch 12), batch 42106, train_loss = 0.819, time/batch = 0.093\n",
      "Sequence 413964/3263200 (epoch 12), batch 42206, train_loss = 1.154, time/batch = 0.095\n",
      "Sequence 414964/3263200 (epoch 12), batch 42306, train_loss = 2.183, time/batch = 0.094\n",
      "Sequence 415904/3263200 (epoch 12), batch 42406, train_loss = 1.603, time/batch = 0.094\n",
      "Sequence 416894/3263200 (epoch 12), batch 42506, train_loss = 2.007, time/batch = 0.093\n",
      "Sequence 417874/3263200 (epoch 12), batch 42606, train_loss = 1.666, time/batch = 0.093\n",
      "Sequence 418844/3263200 (epoch 12), batch 42706, train_loss = 0.831, time/batch = 0.094\n",
      "Sequence 419844/3263200 (epoch 12), batch 42806, train_loss = 1.048, time/batch = 0.093\n",
      "Sequence 420844/3263200 (epoch 12), batch 42906, train_loss = 1.581, time/batch = 0.094\n",
      "Sequence 421834/3263200 (epoch 12), batch 43006, train_loss = 1.685, time/batch = 0.094\n",
      "Sequence 422834/3263200 (epoch 12), batch 43106, train_loss = 1.249, time/batch = 0.095\n",
      "Sequence 423824/3263200 (epoch 12), batch 43206, train_loss = 1.783, time/batch = 0.093\n",
      "Epoch 12 completed, average train loss 1.533823, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 424816/3263200 (epoch 13), batch 43306, train_loss = 2.449, time/batch = 0.096\n",
      "Sequence 425806/3263200 (epoch 13), batch 43406, train_loss = 1.440, time/batch = 0.094\n",
      "Sequence 426786/3263200 (epoch 13), batch 43506, train_loss = 1.346, time/batch = 0.094\n",
      "Sequence 427756/3263200 (epoch 13), batch 43606, train_loss = 2.138, time/batch = 0.094\n",
      "Sequence 428716/3263200 (epoch 13), batch 43706, train_loss = 1.751, time/batch = 0.094\n",
      "Sequence 429696/3263200 (epoch 13), batch 43806, train_loss = 1.142, time/batch = 0.095\n",
      "Sequence 430656/3263200 (epoch 13), batch 43907, train_loss = 1.722, time/batch = 0.095\n",
      "Sequence 431646/3263200 (epoch 13), batch 44007, train_loss = 2.358, time/batch = 0.095\n",
      "Sequence 432616/3263200 (epoch 13), batch 44107, train_loss = 1.147, time/batch = 0.094\n",
      "Sequence 433596/3263200 (epoch 13), batch 44207, train_loss = 1.164, time/batch = 0.096\n",
      "Sequence 434566/3263200 (epoch 13), batch 44307, train_loss = 1.176, time/batch = 0.094\n",
      "Sequence 435566/3263200 (epoch 13), batch 44407, train_loss = 1.297, time/batch = 0.095\n",
      "Sequence 436546/3263200 (epoch 13), batch 44507, train_loss = 1.314, time/batch = 0.094\n",
      "Sequence 437536/3263200 (epoch 13), batch 44607, train_loss = 1.231, time/batch = 0.094\n",
      "Sequence 438526/3263200 (epoch 13), batch 44707, train_loss = 1.070, time/batch = 0.094\n",
      "Sequence 439516/3263200 (epoch 13), batch 44807, train_loss = 1.330, time/batch = 0.094\n",
      "Sequence 440476/3263200 (epoch 13), batch 44907, train_loss = 1.981, time/batch = 0.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 441446/3263200 (epoch 13), batch 45007, train_loss = 1.330, time/batch = 0.092\n",
      "Sequence 442436/3263200 (epoch 13), batch 45107, train_loss = 0.999, time/batch = 0.093\n",
      "Sequence 443396/3263200 (epoch 13), batch 45207, train_loss = 1.313, time/batch = 0.095\n",
      "Sequence 444396/3263200 (epoch 13), batch 45307, train_loss = 0.941, time/batch = 0.095\n",
      "Sequence 445376/3263200 (epoch 13), batch 45407, train_loss = 1.420, time/batch = 0.094\n",
      "Sequence 446356/3263200 (epoch 13), batch 45507, train_loss = 1.116, time/batch = 0.095\n",
      "Sequence 447346/3263200 (epoch 13), batch 45607, train_loss = 2.006, time/batch = 0.093\n",
      "Sequence 448336/3263200 (epoch 13), batch 45707, train_loss = 1.735, time/batch = 0.094\n",
      "Sequence 449316/3263200 (epoch 13), batch 45807, train_loss = 1.846, time/batch = 0.094\n",
      "Sequence 450306/3263200 (epoch 13), batch 45907, train_loss = 2.303, time/batch = 0.096\n",
      "Sequence 451286/3263200 (epoch 13), batch 46007, train_loss = 1.136, time/batch = 0.094\n",
      "Sequence 452276/3263200 (epoch 13), batch 46107, train_loss = 1.529, time/batch = 0.093\n",
      "Sequence 453256/3263200 (epoch 13), batch 46207, train_loss = 1.042, time/batch = 0.094\n",
      "Sequence 454236/3263200 (epoch 13), batch 46307, train_loss = 0.945, time/batch = 0.087\n",
      "Sequence 455226/3263200 (epoch 13), batch 46407, train_loss = 1.002, time/batch = 0.092\n",
      "Sequence 456196/3263200 (epoch 13), batch 46507, train_loss = 1.774, time/batch = 0.093\n",
      "Epoch 13 completed, average train loss 1.515162, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 457178/3263200 (epoch 14), batch 46607, train_loss = 1.029, time/batch = 0.089\n",
      "Sequence 458158/3263200 (epoch 14), batch 46707, train_loss = 2.774, time/batch = 0.094\n",
      "Sequence 459148/3263200 (epoch 14), batch 46807, train_loss = 1.346, time/batch = 0.099\n",
      "Sequence 460118/3263200 (epoch 14), batch 46907, train_loss = 1.393, time/batch = 0.100\n",
      "Sequence 461108/3263200 (epoch 14), batch 47007, train_loss = 1.686, time/batch = 0.095\n",
      "Sequence 462088/3263200 (epoch 14), batch 47107, train_loss = 1.730, time/batch = 0.096\n",
      "Sequence 463068/3263200 (epoch 14), batch 47207, train_loss = 1.653, time/batch = 0.094\n",
      "Sequence 464068/3263200 (epoch 14), batch 47307, train_loss = 0.842, time/batch = 0.093\n",
      "Sequence 465048/3263200 (epoch 14), batch 47407, train_loss = 2.264, time/batch = 0.094\n",
      "Sequence 466028/3263200 (epoch 14), batch 47507, train_loss = 1.696, time/batch = 0.094\n",
      "Sequence 467008/3263200 (epoch 14), batch 47607, train_loss = 0.756, time/batch = 0.094\n",
      "Sequence 467988/3263200 (epoch 14), batch 47707, train_loss = 1.698, time/batch = 0.093\n",
      "Sequence 468958/3263200 (epoch 14), batch 47807, train_loss = 1.441, time/batch = 0.093\n",
      "Sequence 469928/3263200 (epoch 14), batch 47907, train_loss = 1.690, time/batch = 0.095\n",
      "Sequence 470928/3263200 (epoch 14), batch 48007, train_loss = 1.463, time/batch = 0.094\n",
      "Sequence 471898/3263200 (epoch 14), batch 48107, train_loss = 1.185, time/batch = 0.094\n",
      "Sequence 472878/3263200 (epoch 14), batch 48207, train_loss = 1.657, time/batch = 0.094\n",
      "Sequence 473878/3263200 (epoch 14), batch 48307, train_loss = 1.337, time/batch = 0.094\n",
      "Sequence 474868/3263200 (epoch 14), batch 48407, train_loss = 1.201, time/batch = 0.092\n",
      "Sequence 475858/3263200 (epoch 14), batch 48507, train_loss = 2.376, time/batch = 0.095\n",
      "Sequence 476848/3263200 (epoch 14), batch 48607, train_loss = 1.477, time/batch = 0.096\n",
      "Sequence 477818/3263200 (epoch 14), batch 48707, train_loss = 1.160, time/batch = 0.096\n",
      "Sequence 478808/3263200 (epoch 14), batch 48807, train_loss = 1.699, time/batch = 0.094\n",
      "Sequence 479768/3263200 (epoch 14), batch 48907, train_loss = 1.374, time/batch = 0.095\n",
      "Sequence 480718/3263200 (epoch 14), batch 49007, train_loss = 1.131, time/batch = 0.096\n",
      "Sequence 481708/3263200 (epoch 14), batch 49107, train_loss = 1.464, time/batch = 0.094\n",
      "Sequence 482678/3263200 (epoch 14), batch 49207, train_loss = 1.402, time/batch = 0.096\n",
      "Sequence 483678/3263200 (epoch 14), batch 49307, train_loss = 1.427, time/batch = 0.096\n",
      "Sequence 484658/3263200 (epoch 14), batch 49407, train_loss = 1.267, time/batch = 0.094\n",
      "Sequence 485648/3263200 (epoch 14), batch 49507, train_loss = 1.405, time/batch = 0.094\n",
      "Sequence 486618/3263200 (epoch 14), batch 49607, train_loss = 1.428, time/batch = 0.095\n",
      "Sequence 487598/3263200 (epoch 14), batch 49707, train_loss = 1.588, time/batch = 0.095\n",
      "Sequence 488558/3263200 (epoch 14), batch 49807, train_loss = 1.947, time/batch = 0.095\n",
      "Epoch 14 completed, average train loss 1.500497, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 489550/3263200 (epoch 15), batch 49907, train_loss = 1.099, time/batch = 0.098\n",
      "Sequence 490540/3263200 (epoch 15), batch 50007, train_loss = 1.701, time/batch = 0.094\n",
      "Sequence 491540/3263200 (epoch 15), batch 50107, train_loss = 2.273, time/batch = 0.094\n",
      "Sequence 492540/3263200 (epoch 15), batch 50207, train_loss = 1.788, time/batch = 0.094\n",
      "Sequence 493520/3263200 (epoch 15), batch 50307, train_loss = 0.963, time/batch = 0.094\n",
      "Sequence 494510/3263200 (epoch 15), batch 50407, train_loss = 1.212, time/batch = 0.093\n",
      "Sequence 495490/3263200 (epoch 15), batch 50507, train_loss = 1.238, time/batch = 0.093\n",
      "Sequence 496460/3263200 (epoch 15), batch 50607, train_loss = 0.979, time/batch = 0.095\n",
      "Sequence 497430/3263200 (epoch 15), batch 50707, train_loss = 1.022, time/batch = 0.094\n",
      "Sequence 498420/3263200 (epoch 15), batch 50807, train_loss = 1.243, time/batch = 0.095\n",
      "Sequence 499400/3263200 (epoch 15), batch 50907, train_loss = 3.053, time/batch = 0.095\n",
      "Sequence 500370/3263200 (epoch 15), batch 51007, train_loss = 1.070, time/batch = 0.093\n",
      "Sequence 501360/3263200 (epoch 15), batch 51107, train_loss = 1.343, time/batch = 0.094\n",
      "Sequence 502340/3263200 (epoch 15), batch 51208, train_loss = 0.711, time/batch = 0.095\n",
      "Sequence 503310/3263200 (epoch 15), batch 51308, train_loss = 1.132, time/batch = 0.095\n",
      "Sequence 504270/3263200 (epoch 15), batch 51408, train_loss = 1.043, time/batch = 0.095\n",
      "Sequence 505250/3263200 (epoch 15), batch 51508, train_loss = 1.305, time/batch = 0.095\n",
      "Sequence 506240/3263200 (epoch 15), batch 51608, train_loss = 0.865, time/batch = 0.095\n",
      "Sequence 507220/3263200 (epoch 15), batch 51708, train_loss = 1.503, time/batch = 0.094\n",
      "Sequence 508180/3263200 (epoch 15), batch 51808, train_loss = 1.376, time/batch = 0.095\n",
      "Sequence 509170/3263200 (epoch 15), batch 51908, train_loss = 1.808, time/batch = 0.094\n",
      "Sequence 510140/3263200 (epoch 15), batch 52008, train_loss = 1.277, time/batch = 0.095\n",
      "Sequence 511140/3263200 (epoch 15), batch 52108, train_loss = 1.089, time/batch = 0.094\n",
      "Sequence 512110/3263200 (epoch 15), batch 52208, train_loss = 1.452, time/batch = 0.095\n",
      "Sequence 513100/3263200 (epoch 15), batch 52308, train_loss = 1.862, time/batch = 0.094\n",
      "Sequence 514090/3263200 (epoch 15), batch 52408, train_loss = 1.405, time/batch = 0.094\n",
      "Sequence 515080/3263200 (epoch 15), batch 52508, train_loss = 1.667, time/batch = 0.094\n",
      "Sequence 516050/3263200 (epoch 15), batch 52609, train_loss = 0.897, time/batch = 0.094\n",
      "Sequence 517000/3263200 (epoch 15), batch 52709, train_loss = 1.441, time/batch = 0.095\n",
      "Sequence 518000/3263200 (epoch 15), batch 52809, train_loss = 1.304, time/batch = 0.094\n",
      "Sequence 518990/3263200 (epoch 15), batch 52909, train_loss = 1.090, time/batch = 0.095\n",
      "Sequence 519970/3263200 (epoch 15), batch 53009, train_loss = 1.827, time/batch = 0.095\n",
      "Sequence 520960/3263200 (epoch 15), batch 53109, train_loss = 1.293, time/batch = 0.094\n",
      "Sequence 521940/3263200 (epoch 15), batch 53209, train_loss = 1.693, time/batch = 0.095\n",
      "Epoch 15 completed, average train loss 1.485588, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 522912/3263200 (epoch 16), batch 53309, train_loss = 1.435, time/batch = 0.095\n",
      "Sequence 523912/3263200 (epoch 16), batch 53409, train_loss = 1.529, time/batch = 0.093\n",
      "Sequence 524902/3263200 (epoch 16), batch 53509, train_loss = 1.580, time/batch = 0.093\n",
      "Sequence 525872/3263200 (epoch 16), batch 53609, train_loss = 1.884, time/batch = 0.094\n",
      "Sequence 526852/3263200 (epoch 16), batch 53709, train_loss = 1.657, time/batch = 0.094\n",
      "Sequence 527832/3263200 (epoch 16), batch 53809, train_loss = 1.813, time/batch = 0.094\n",
      "Sequence 528802/3263200 (epoch 16), batch 53909, train_loss = 0.993, time/batch = 0.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 529772/3263200 (epoch 16), batch 54009, train_loss = 1.468, time/batch = 0.093\n",
      "Sequence 530752/3263200 (epoch 16), batch 54109, train_loss = 1.046, time/batch = 0.094\n",
      "Sequence 531722/3263200 (epoch 16), batch 54209, train_loss = 1.164, time/batch = 0.095\n",
      "Sequence 532722/3263200 (epoch 16), batch 54309, train_loss = 0.699, time/batch = 0.094\n",
      "Sequence 533702/3263200 (epoch 16), batch 54409, train_loss = 1.507, time/batch = 0.094\n",
      "Sequence 534702/3263200 (epoch 16), batch 54509, train_loss = 1.674, time/batch = 0.093\n",
      "Sequence 535692/3263200 (epoch 16), batch 54609, train_loss = 1.271, time/batch = 0.093\n",
      "Sequence 536682/3263200 (epoch 16), batch 54709, train_loss = 1.549, time/batch = 0.095\n",
      "Sequence 537662/3263200 (epoch 16), batch 54809, train_loss = 2.663, time/batch = 0.093\n",
      "Sequence 538642/3263200 (epoch 16), batch 54909, train_loss = 1.316, time/batch = 0.095\n",
      "Sequence 539622/3263200 (epoch 16), batch 55009, train_loss = 1.663, time/batch = 0.094\n",
      "Sequence 540592/3263200 (epoch 16), batch 55109, train_loss = 1.191, time/batch = 0.093\n",
      "Sequence 541582/3263200 (epoch 16), batch 55209, train_loss = 1.994, time/batch = 0.093\n",
      "Sequence 542552/3263200 (epoch 16), batch 55309, train_loss = 1.653, time/batch = 0.093\n",
      "Sequence 543552/3263200 (epoch 16), batch 55409, train_loss = 1.695, time/batch = 0.093\n",
      "Sequence 544512/3263200 (epoch 16), batch 55509, train_loss = 1.629, time/batch = 0.094\n",
      "Sequence 545482/3263200 (epoch 16), batch 55609, train_loss = 1.271, time/batch = 0.093\n",
      "Sequence 546482/3263200 (epoch 16), batch 55709, train_loss = 1.545, time/batch = 0.093\n",
      "Sequence 547442/3263200 (epoch 16), batch 55809, train_loss = 0.963, time/batch = 0.091\n",
      "Sequence 548442/3263200 (epoch 16), batch 55909, train_loss = 2.390, time/batch = 0.087\n",
      "Sequence 549412/3263200 (epoch 16), batch 56009, train_loss = 1.168, time/batch = 0.092\n",
      "Sequence 550412/3263200 (epoch 16), batch 56109, train_loss = 1.453, time/batch = 0.093\n",
      "Sequence 551392/3263200 (epoch 16), batch 56209, train_loss = 1.873, time/batch = 0.093\n",
      "Sequence 552382/3263200 (epoch 16), batch 56309, train_loss = 1.838, time/batch = 0.094\n",
      "Sequence 553352/3263200 (epoch 16), batch 56409, train_loss = 1.664, time/batch = 0.094\n",
      "Sequence 554322/3263200 (epoch 16), batch 56509, train_loss = 0.809, time/batch = 0.093\n",
      "Epoch 16 completed, average train loss 1.472884, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 555294/3263200 (epoch 17), batch 56609, train_loss = 1.559, time/batch = 0.094\n",
      "Sequence 556264/3263200 (epoch 17), batch 56709, train_loss = 1.234, time/batch = 0.094\n",
      "Sequence 557224/3263200 (epoch 17), batch 56809, train_loss = 1.333, time/batch = 0.093\n",
      "Sequence 558204/3263200 (epoch 17), batch 56909, train_loss = 1.898, time/batch = 0.094\n",
      "Sequence 559194/3263200 (epoch 17), batch 57009, train_loss = 1.538, time/batch = 0.093\n",
      "Sequence 560164/3263200 (epoch 17), batch 57109, train_loss = 1.147, time/batch = 0.092\n",
      "Sequence 561154/3263200 (epoch 17), batch 57209, train_loss = 1.161, time/batch = 0.094\n",
      "Sequence 562124/3263200 (epoch 17), batch 57309, train_loss = 2.017, time/batch = 0.093\n",
      "Sequence 563084/3263200 (epoch 17), batch 57409, train_loss = 1.637, time/batch = 0.095\n",
      "Sequence 564074/3263200 (epoch 17), batch 57509, train_loss = 0.920, time/batch = 0.093\n",
      "Sequence 565054/3263200 (epoch 17), batch 57609, train_loss = 1.137, time/batch = 0.092\n",
      "Sequence 566044/3263200 (epoch 17), batch 57709, train_loss = 1.236, time/batch = 0.094\n",
      "Sequence 567034/3263200 (epoch 17), batch 57809, train_loss = 1.180, time/batch = 0.094\n",
      "Sequence 568024/3263200 (epoch 17), batch 57909, train_loss = 1.507, time/batch = 0.092\n",
      "Sequence 569024/3263200 (epoch 17), batch 58009, train_loss = 1.004, time/batch = 0.094\n",
      "Sequence 570024/3263200 (epoch 17), batch 58109, train_loss = 1.350, time/batch = 0.092\n",
      "Sequence 571014/3263200 (epoch 17), batch 58209, train_loss = 1.546, time/batch = 0.092\n",
      "Sequence 571994/3263200 (epoch 17), batch 58309, train_loss = 1.509, time/batch = 0.093\n",
      "Sequence 572974/3263200 (epoch 17), batch 58409, train_loss = 1.059, time/batch = 0.093\n",
      "Sequence 573974/3263200 (epoch 17), batch 58509, train_loss = 1.309, time/batch = 0.093\n",
      "Sequence 574954/3263200 (epoch 17), batch 58609, train_loss = 1.832, time/batch = 0.092\n",
      "Sequence 575944/3263200 (epoch 17), batch 58709, train_loss = 1.637, time/batch = 0.092\n",
      "Sequence 576914/3263200 (epoch 17), batch 58809, train_loss = 1.855, time/batch = 0.093\n",
      "Sequence 577914/3263200 (epoch 17), batch 58909, train_loss = 1.513, time/batch = 0.093\n",
      "Sequence 578894/3263200 (epoch 17), batch 59009, train_loss = 1.026, time/batch = 0.092\n",
      "Sequence 579884/3263200 (epoch 17), batch 59109, train_loss = 2.236, time/batch = 0.092\n",
      "Sequence 580844/3263200 (epoch 17), batch 59209, train_loss = 1.575, time/batch = 0.092\n",
      "Sequence 581794/3263200 (epoch 17), batch 59309, train_loss = 1.185, time/batch = 0.092\n",
      "Sequence 582774/3263200 (epoch 17), batch 59409, train_loss = 0.885, time/batch = 0.091\n",
      "Sequence 583754/3263200 (epoch 17), batch 59509, train_loss = 1.356, time/batch = 0.093\n",
      "Sequence 584714/3263200 (epoch 17), batch 59609, train_loss = 1.297, time/batch = 0.092\n",
      "Sequence 585694/3263200 (epoch 17), batch 59709, train_loss = 1.518, time/batch = 0.093\n",
      "Sequence 586694/3263200 (epoch 17), batch 59809, train_loss = 1.481, time/batch = 0.093\n",
      "Epoch 17 completed, average train loss 1.460969, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 587676/3263200 (epoch 18), batch 59909, train_loss = 1.072, time/batch = 0.094\n",
      "Sequence 588676/3263200 (epoch 18), batch 60009, train_loss = 1.666, time/batch = 0.093\n",
      "Sequence 589656/3263200 (epoch 18), batch 60109, train_loss = 1.513, time/batch = 0.092\n",
      "Sequence 590626/3263200 (epoch 18), batch 60209, train_loss = 2.328, time/batch = 0.093\n",
      "Sequence 591616/3263200 (epoch 18), batch 60309, train_loss = 2.285, time/batch = 0.093\n",
      "Sequence 592586/3263200 (epoch 18), batch 60409, train_loss = 0.881, time/batch = 0.092\n",
      "Sequence 593586/3263200 (epoch 18), batch 60509, train_loss = 1.275, time/batch = 0.093\n",
      "Sequence 594566/3263200 (epoch 18), batch 60609, train_loss = 1.299, time/batch = 0.092\n",
      "Sequence 595536/3263200 (epoch 18), batch 60709, train_loss = 1.011, time/batch = 0.092\n",
      "Sequence 596516/3263200 (epoch 18), batch 60809, train_loss = 1.475, time/batch = 0.092\n",
      "Sequence 597496/3263200 (epoch 18), batch 60909, train_loss = 0.871, time/batch = 0.093\n",
      "Sequence 598466/3263200 (epoch 18), batch 61009, train_loss = 1.775, time/batch = 0.092\n",
      "Sequence 599436/3263200 (epoch 18), batch 61109, train_loss = 1.204, time/batch = 0.093\n",
      "Sequence 600426/3263200 (epoch 18), batch 61209, train_loss = 1.338, time/batch = 0.091\n",
      "Sequence 601396/3263200 (epoch 18), batch 61309, train_loss = 0.974, time/batch = 0.093\n",
      "Sequence 602366/3263200 (epoch 18), batch 61409, train_loss = 1.337, time/batch = 0.094\n",
      "Sequence 603346/3263200 (epoch 18), batch 61509, train_loss = 1.289, time/batch = 0.093\n",
      "Sequence 604316/3263200 (epoch 18), batch 61609, train_loss = 1.794, time/batch = 0.093\n",
      "Sequence 605286/3263200 (epoch 18), batch 61709, train_loss = 1.819, time/batch = 0.094\n",
      "Sequence 606276/3263200 (epoch 18), batch 61809, train_loss = 1.018, time/batch = 0.091\n",
      "Sequence 607246/3263200 (epoch 18), batch 61909, train_loss = 1.972, time/batch = 0.092\n",
      "Sequence 608226/3263200 (epoch 18), batch 62009, train_loss = 1.838, time/batch = 0.093\n",
      "Sequence 609196/3263200 (epoch 18), batch 62110, train_loss = 0.757, time/batch = 0.093\n",
      "Sequence 610186/3263200 (epoch 18), batch 62210, train_loss = 2.073, time/batch = 0.093\n",
      "Sequence 611186/3263200 (epoch 18), batch 62310, train_loss = 1.111, time/batch = 0.094\n",
      "Sequence 612176/3263200 (epoch 18), batch 62410, train_loss = 1.968, time/batch = 0.093\n",
      "Sequence 613156/3263200 (epoch 18), batch 62510, train_loss = 1.332, time/batch = 0.093\n",
      "Sequence 614136/3263200 (epoch 18), batch 62610, train_loss = 1.168, time/batch = 0.093\n",
      "Sequence 615126/3263200 (epoch 18), batch 62710, train_loss = 0.741, time/batch = 0.092\n",
      "Sequence 616086/3263200 (epoch 18), batch 62810, train_loss = 1.014, time/batch = 0.092\n",
      "Sequence 617086/3263200 (epoch 18), batch 62910, train_loss = 2.358, time/batch = 0.093\n",
      "Sequence 618076/3263200 (epoch 18), batch 63010, train_loss = 0.944, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 619056/3263200 (epoch 18), batch 63110, train_loss = 2.273, time/batch = 0.091\n",
      "Epoch 18 completed, average train loss 1.448525, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 620048/3263200 (epoch 19), batch 63210, train_loss = 1.112, time/batch = 0.094\n",
      "Sequence 620988/3263200 (epoch 19), batch 63310, train_loss = 1.464, time/batch = 0.093\n",
      "Sequence 621968/3263200 (epoch 19), batch 63411, train_loss = 0.565, time/batch = 0.093\n",
      "Sequence 622948/3263200 (epoch 19), batch 63511, train_loss = 0.789, time/batch = 0.092\n",
      "Sequence 623938/3263200 (epoch 19), batch 63611, train_loss = 1.848, time/batch = 0.093\n",
      "Sequence 624908/3263200 (epoch 19), batch 63711, train_loss = 1.875, time/batch = 0.093\n",
      "Sequence 625908/3263200 (epoch 19), batch 63811, train_loss = 1.041, time/batch = 0.091\n",
      "Sequence 626888/3263200 (epoch 19), batch 63911, train_loss = 1.886, time/batch = 0.092\n",
      "Sequence 627868/3263200 (epoch 19), batch 64011, train_loss = 1.167, time/batch = 0.092\n",
      "Sequence 628838/3263200 (epoch 19), batch 64111, train_loss = 1.165, time/batch = 0.093\n",
      "Sequence 629838/3263200 (epoch 19), batch 64211, train_loss = 1.450, time/batch = 0.092\n",
      "Sequence 630818/3263200 (epoch 19), batch 64311, train_loss = 1.272, time/batch = 0.093\n",
      "Sequence 631808/3263200 (epoch 19), batch 64411, train_loss = 1.818, time/batch = 0.092\n",
      "Sequence 632798/3263200 (epoch 19), batch 64511, train_loss = 2.981, time/batch = 0.092\n",
      "Sequence 633798/3263200 (epoch 19), batch 64611, train_loss = 1.526, time/batch = 0.093\n",
      "Sequence 634778/3263200 (epoch 19), batch 64712, train_loss = 0.703, time/batch = 0.092\n",
      "Sequence 635768/3263200 (epoch 19), batch 64812, train_loss = 1.562, time/batch = 0.092\n",
      "Sequence 636718/3263200 (epoch 19), batch 64912, train_loss = 1.466, time/batch = 0.092\n",
      "Sequence 637718/3263200 (epoch 19), batch 65012, train_loss = 2.043, time/batch = 0.093\n",
      "Sequence 638698/3263200 (epoch 19), batch 65112, train_loss = 2.240, time/batch = 0.092\n",
      "Sequence 639688/3263200 (epoch 19), batch 65212, train_loss = 1.651, time/batch = 0.092\n",
      "Sequence 640638/3263200 (epoch 19), batch 65312, train_loss = 1.469, time/batch = 0.092\n",
      "Sequence 641598/3263200 (epoch 19), batch 65412, train_loss = 1.129, time/batch = 0.093\n",
      "Sequence 642598/3263200 (epoch 19), batch 65512, train_loss = 1.018, time/batch = 0.092\n",
      "Sequence 643588/3263200 (epoch 19), batch 65612, train_loss = 1.348, time/batch = 0.092\n",
      "Sequence 644568/3263200 (epoch 19), batch 65712, train_loss = 1.556, time/batch = 0.094\n",
      "Sequence 645558/3263200 (epoch 19), batch 65812, train_loss = 1.669, time/batch = 0.092\n",
      "Sequence 646538/3263200 (epoch 19), batch 65912, train_loss = 1.482, time/batch = 0.092\n",
      "Sequence 647528/3263200 (epoch 19), batch 66012, train_loss = 1.098, time/batch = 0.093\n",
      "Sequence 648508/3263200 (epoch 19), batch 66112, train_loss = 1.461, time/batch = 0.092\n",
      "Sequence 649478/3263200 (epoch 19), batch 66212, train_loss = 1.235, time/batch = 0.092\n",
      "Sequence 650468/3263200 (epoch 19), batch 66312, train_loss = 1.816, time/batch = 0.093\n",
      "Sequence 651458/3263200 (epoch 19), batch 66412, train_loss = 1.796, time/batch = 0.092\n",
      "Sequence 652438/3263200 (epoch 19), batch 66513, train_loss = 0.670, time/batch = 0.092\n",
      "Epoch 19 completed, average train loss 1.438179, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 653430/3263200 (epoch 20), batch 66613, train_loss = 1.008, time/batch = 0.095\n",
      "Sequence 654410/3263200 (epoch 20), batch 66713, train_loss = 1.595, time/batch = 0.092\n",
      "Sequence 655390/3263200 (epoch 20), batch 66813, train_loss = 1.143, time/batch = 0.091\n",
      "Sequence 656360/3263200 (epoch 20), batch 66913, train_loss = 1.604, time/batch = 0.093\n",
      "Sequence 657330/3263200 (epoch 20), batch 67013, train_loss = 1.677, time/batch = 0.093\n",
      "Sequence 658310/3263200 (epoch 20), batch 67113, train_loss = 1.345, time/batch = 0.092\n",
      "Sequence 659300/3263200 (epoch 20), batch 67213, train_loss = 1.393, time/batch = 0.092\n",
      "Sequence 660270/3263200 (epoch 20), batch 67313, train_loss = 1.549, time/batch = 0.092\n",
      "Sequence 661250/3263200 (epoch 20), batch 67413, train_loss = 1.401, time/batch = 0.092\n",
      "Sequence 662200/3263200 (epoch 20), batch 67513, train_loss = 1.268, time/batch = 0.093\n",
      "Sequence 663180/3263200 (epoch 20), batch 67613, train_loss = 1.079, time/batch = 0.093\n",
      "Sequence 664180/3263200 (epoch 20), batch 67713, train_loss = 1.589, time/batch = 0.093\n",
      "Sequence 665180/3263200 (epoch 20), batch 67813, train_loss = 1.453, time/batch = 0.092\n",
      "Sequence 666150/3263200 (epoch 20), batch 67913, train_loss = 1.113, time/batch = 0.094\n",
      "Sequence 667130/3263200 (epoch 20), batch 68013, train_loss = 1.427, time/batch = 0.091\n",
      "Sequence 668110/3263200 (epoch 20), batch 68113, train_loss = 0.894, time/batch = 0.093\n",
      "Sequence 669110/3263200 (epoch 20), batch 68213, train_loss = 1.575, time/batch = 0.092\n",
      "Sequence 670100/3263200 (epoch 20), batch 68313, train_loss = 2.602, time/batch = 0.093\n",
      "Sequence 671100/3263200 (epoch 20), batch 68413, train_loss = 1.665, time/batch = 0.092\n",
      "Sequence 672080/3263200 (epoch 20), batch 68513, train_loss = 1.049, time/batch = 0.092\n",
      "Sequence 673070/3263200 (epoch 20), batch 68613, train_loss = 2.187, time/batch = 0.093\n",
      "Sequence 674030/3263200 (epoch 20), batch 68713, train_loss = 1.268, time/batch = 0.093\n",
      "Sequence 675030/3263200 (epoch 20), batch 68813, train_loss = 2.195, time/batch = 0.091\n",
      "Sequence 676010/3263200 (epoch 20), batch 68913, train_loss = 1.818, time/batch = 0.094\n",
      "Sequence 676970/3263200 (epoch 20), batch 69013, train_loss = 0.945, time/batch = 0.092\n",
      "Sequence 677970/3263200 (epoch 20), batch 69113, train_loss = 0.716, time/batch = 0.094\n",
      "Sequence 678960/3263200 (epoch 20), batch 69213, train_loss = 0.926, time/batch = 0.093\n",
      "Sequence 679920/3263200 (epoch 20), batch 69313, train_loss = 1.218, time/batch = 0.094\n",
      "Sequence 680900/3263200 (epoch 20), batch 69413, train_loss = 0.732, time/batch = 0.093\n",
      "Sequence 681880/3263200 (epoch 20), batch 69513, train_loss = 1.370, time/batch = 0.092\n",
      "Sequence 682860/3263200 (epoch 20), batch 69613, train_loss = 1.152, time/batch = 0.092\n",
      "Sequence 683840/3263200 (epoch 20), batch 69713, train_loss = 1.829, time/batch = 0.093\n",
      "Sequence 684820/3263200 (epoch 20), batch 69813, train_loss = 1.437, time/batch = 0.094\n",
      "Epoch 20 completed, average train loss 1.426777, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 685782/3263200 (epoch 21), batch 69913, train_loss = 1.051, time/batch = 0.094\n",
      "Sequence 686752/3263200 (epoch 21), batch 70013, train_loss = 1.245, time/batch = 0.091\n",
      "Sequence 687752/3263200 (epoch 21), batch 70113, train_loss = 2.137, time/batch = 0.092\n",
      "Sequence 688732/3263200 (epoch 21), batch 70213, train_loss = 1.257, time/batch = 0.091\n",
      "Sequence 689692/3263200 (epoch 21), batch 70313, train_loss = 1.603, time/batch = 0.092\n",
      "Sequence 690692/3263200 (epoch 21), batch 70413, train_loss = 1.537, time/batch = 0.092\n",
      "Sequence 691682/3263200 (epoch 21), batch 70513, train_loss = 0.785, time/batch = 0.094\n",
      "Sequence 692682/3263200 (epoch 21), batch 70613, train_loss = 1.993, time/batch = 0.092\n",
      "Sequence 693652/3263200 (epoch 21), batch 70713, train_loss = 1.222, time/batch = 0.094\n",
      "Sequence 694642/3263200 (epoch 21), batch 70813, train_loss = 1.053, time/batch = 0.091\n",
      "Sequence 695622/3263200 (epoch 21), batch 70913, train_loss = 1.795, time/batch = 0.093\n",
      "Sequence 696592/3263200 (epoch 21), batch 71013, train_loss = 1.878, time/batch = 0.091\n",
      "Sequence 697582/3263200 (epoch 21), batch 71113, train_loss = 1.804, time/batch = 0.092\n",
      "Sequence 698552/3263200 (epoch 21), batch 71213, train_loss = 1.105, time/batch = 0.094\n",
      "Sequence 699522/3263200 (epoch 21), batch 71313, train_loss = 1.512, time/batch = 0.093\n",
      "Sequence 700492/3263200 (epoch 21), batch 71413, train_loss = 1.349, time/batch = 0.092\n",
      "Sequence 701472/3263200 (epoch 21), batch 71513, train_loss = 0.678, time/batch = 0.093\n",
      "Sequence 702452/3263200 (epoch 21), batch 71613, train_loss = 1.466, time/batch = 0.091\n",
      "Sequence 703452/3263200 (epoch 21), batch 71713, train_loss = 1.472, time/batch = 0.093\n",
      "Sequence 704422/3263200 (epoch 21), batch 71813, train_loss = 1.505, time/batch = 0.092\n",
      "Sequence 705402/3263200 (epoch 21), batch 71913, train_loss = 1.909, time/batch = 0.092\n",
      "Sequence 706392/3263200 (epoch 21), batch 72013, train_loss = 0.941, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 707382/3263200 (epoch 21), batch 72113, train_loss = 1.160, time/batch = 0.093\n",
      "Sequence 708372/3263200 (epoch 21), batch 72213, train_loss = 1.928, time/batch = 0.093\n",
      "Sequence 709342/3263200 (epoch 21), batch 72313, train_loss = 1.837, time/batch = 0.091\n",
      "Sequence 710322/3263200 (epoch 21), batch 72413, train_loss = 1.608, time/batch = 0.094\n",
      "Sequence 711302/3263200 (epoch 21), batch 72513, train_loss = 1.444, time/batch = 0.093\n",
      "Sequence 712282/3263200 (epoch 21), batch 72613, train_loss = 1.125, time/batch = 0.092\n",
      "Sequence 713262/3263200 (epoch 21), batch 72713, train_loss = 2.246, time/batch = 0.092\n",
      "Sequence 714252/3263200 (epoch 21), batch 72813, train_loss = 3.152, time/batch = 0.092\n",
      "Sequence 715232/3263200 (epoch 21), batch 72913, train_loss = 1.245, time/batch = 0.093\n",
      "Sequence 716232/3263200 (epoch 21), batch 73013, train_loss = 1.061, time/batch = 0.094\n",
      "Sequence 717192/3263200 (epoch 21), batch 73113, train_loss = 1.678, time/batch = 0.093\n",
      "Epoch 21 completed, average train loss 1.419052, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 718164/3263200 (epoch 22), batch 73213, train_loss = 0.935, time/batch = 0.094\n",
      "Sequence 719154/3263200 (epoch 22), batch 73313, train_loss = 1.283, time/batch = 0.093\n",
      "Sequence 720154/3263200 (epoch 22), batch 73413, train_loss = 1.087, time/batch = 0.093\n",
      "Sequence 721154/3263200 (epoch 22), batch 73513, train_loss = 2.326, time/batch = 0.092\n",
      "Sequence 722134/3263200 (epoch 22), batch 73613, train_loss = 1.162, time/batch = 0.093\n",
      "Sequence 723124/3263200 (epoch 22), batch 73713, train_loss = 1.563, time/batch = 0.093\n",
      "Sequence 724084/3263200 (epoch 22), batch 73813, train_loss = 1.423, time/batch = 0.092\n",
      "Sequence 725044/3263200 (epoch 22), batch 73913, train_loss = 1.189, time/batch = 0.093\n",
      "Sequence 726024/3263200 (epoch 22), batch 74013, train_loss = 1.312, time/batch = 0.092\n",
      "Sequence 726994/3263200 (epoch 22), batch 74113, train_loss = 1.375, time/batch = 0.092\n",
      "Sequence 727954/3263200 (epoch 22), batch 74213, train_loss = 1.282, time/batch = 0.093\n",
      "Sequence 728944/3263200 (epoch 22), batch 74313, train_loss = 1.593, time/batch = 0.093\n",
      "Sequence 729934/3263200 (epoch 22), batch 74413, train_loss = 1.265, time/batch = 0.092\n",
      "Sequence 730904/3263200 (epoch 22), batch 74513, train_loss = 2.215, time/batch = 0.093\n",
      "Sequence 731894/3263200 (epoch 22), batch 74613, train_loss = 1.167, time/batch = 0.093\n",
      "Sequence 732884/3263200 (epoch 22), batch 74713, train_loss = 1.399, time/batch = 0.093\n",
      "Sequence 733864/3263200 (epoch 22), batch 74813, train_loss = 1.190, time/batch = 0.094\n",
      "Sequence 734844/3263200 (epoch 22), batch 74913, train_loss = 1.012, time/batch = 0.093\n",
      "Sequence 735834/3263200 (epoch 22), batch 75013, train_loss = 1.182, time/batch = 0.093\n",
      "Sequence 736804/3263200 (epoch 22), batch 75113, train_loss = 1.333, time/batch = 0.092\n",
      "Sequence 737794/3263200 (epoch 22), batch 75213, train_loss = 1.172, time/batch = 0.093\n",
      "Sequence 738774/3263200 (epoch 22), batch 75313, train_loss = 0.858, time/batch = 0.092\n",
      "Sequence 739764/3263200 (epoch 22), batch 75413, train_loss = 1.583, time/batch = 0.093\n",
      "Sequence 740764/3263200 (epoch 22), batch 75513, train_loss = 1.148, time/batch = 0.092\n",
      "Sequence 741754/3263200 (epoch 22), batch 75613, train_loss = 1.515, time/batch = 0.092\n",
      "Sequence 742744/3263200 (epoch 22), batch 75713, train_loss = 1.314, time/batch = 0.093\n",
      "Sequence 743694/3263200 (epoch 22), batch 75813, train_loss = 1.475, time/batch = 0.091\n",
      "Sequence 744694/3263200 (epoch 22), batch 75913, train_loss = 0.858, time/batch = 0.092\n",
      "Sequence 745674/3263200 (epoch 22), batch 76013, train_loss = 1.452, time/batch = 0.092\n",
      "Sequence 746664/3263200 (epoch 22), batch 76113, train_loss = 0.896, time/batch = 0.092\n",
      "Sequence 747614/3263200 (epoch 22), batch 76214, train_loss = 0.696, time/batch = 0.093\n",
      "Sequence 748584/3263200 (epoch 22), batch 76314, train_loss = 1.130, time/batch = 0.092\n",
      "Sequence 749564/3263200 (epoch 22), batch 76414, train_loss = 1.355, time/batch = 0.092\n",
      "Epoch 22 completed, average train loss 1.409437, learning rate 0.0010\n",
      "Sequence 750546/3263200 (epoch 23), batch 76514, train_loss = 0.993, time/batch = 0.091\n",
      "Shuffling training data...\n",
      "Sequence 751536/3263200 (epoch 23), batch 76614, train_loss = 1.617, time/batch = 0.094\n",
      "Sequence 752516/3263200 (epoch 23), batch 76714, train_loss = 1.527, time/batch = 0.093\n",
      "Sequence 753516/3263200 (epoch 23), batch 76814, train_loss = 1.581, time/batch = 0.093\n",
      "Sequence 754506/3263200 (epoch 23), batch 76914, train_loss = 1.086, time/batch = 0.093\n",
      "Sequence 755466/3263200 (epoch 23), batch 77014, train_loss = 1.382, time/batch = 0.093\n",
      "Sequence 756446/3263200 (epoch 23), batch 77114, train_loss = 1.134, time/batch = 0.093\n",
      "Sequence 757396/3263200 (epoch 23), batch 77214, train_loss = 0.927, time/batch = 0.091\n",
      "Sequence 758386/3263200 (epoch 23), batch 77314, train_loss = 1.826, time/batch = 0.093\n",
      "Sequence 759346/3263200 (epoch 23), batch 77414, train_loss = 1.541, time/batch = 0.093\n",
      "Sequence 760346/3263200 (epoch 23), batch 77514, train_loss = 1.279, time/batch = 0.093\n",
      "Sequence 761326/3263200 (epoch 23), batch 77614, train_loss = 1.283, time/batch = 0.092\n",
      "Sequence 762276/3263200 (epoch 23), batch 77714, train_loss = 1.233, time/batch = 0.092\n",
      "Sequence 763236/3263200 (epoch 23), batch 77814, train_loss = 1.231, time/batch = 0.093\n",
      "Sequence 764216/3263200 (epoch 23), batch 77914, train_loss = 1.918, time/batch = 0.092\n",
      "Sequence 765206/3263200 (epoch 23), batch 78014, train_loss = 1.493, time/batch = 0.093\n",
      "Sequence 766196/3263200 (epoch 23), batch 78114, train_loss = 1.320, time/batch = 0.092\n",
      "Sequence 767186/3263200 (epoch 23), batch 78214, train_loss = 1.222, time/batch = 0.093\n",
      "Sequence 768146/3263200 (epoch 23), batch 78314, train_loss = 1.124, time/batch = 0.092\n",
      "Sequence 769136/3263200 (epoch 23), batch 78414, train_loss = 1.446, time/batch = 0.092\n",
      "Sequence 770126/3263200 (epoch 23), batch 78514, train_loss = 0.895, time/batch = 0.093\n",
      "Sequence 771116/3263200 (epoch 23), batch 78614, train_loss = 1.238, time/batch = 0.092\n",
      "Sequence 772086/3263200 (epoch 23), batch 78714, train_loss = 1.243, time/batch = 0.093\n",
      "Sequence 773076/3263200 (epoch 23), batch 78814, train_loss = 1.269, time/batch = 0.093\n",
      "Sequence 774066/3263200 (epoch 23), batch 78914, train_loss = 1.287, time/batch = 0.093\n",
      "Sequence 775066/3263200 (epoch 23), batch 79014, train_loss = 0.839, time/batch = 0.093\n",
      "Sequence 776036/3263200 (epoch 23), batch 79114, train_loss = 1.025, time/batch = 0.092\n",
      "Sequence 777026/3263200 (epoch 23), batch 79214, train_loss = 1.796, time/batch = 0.093\n",
      "Sequence 778016/3263200 (epoch 23), batch 79314, train_loss = 1.029, time/batch = 0.093\n",
      "Sequence 778996/3263200 (epoch 23), batch 79414, train_loss = 0.915, time/batch = 0.092\n",
      "Sequence 779956/3263200 (epoch 23), batch 79514, train_loss = 1.509, time/batch = 0.093\n",
      "Sequence 780946/3263200 (epoch 23), batch 79615, train_loss = 1.714, time/batch = 0.093\n",
      "Sequence 781936/3263200 (epoch 23), batch 79715, train_loss = 1.141, time/batch = 0.092\n",
      "Sequence 782916/3263200 (epoch 23), batch 79815, train_loss = 1.644, time/batch = 0.092\n",
      "Epoch 23 completed, average train loss 1.400638, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 783898/3263200 (epoch 24), batch 79915, train_loss = 1.005, time/batch = 0.093\n",
      "Sequence 784868/3263200 (epoch 24), batch 80015, train_loss = 0.656, time/batch = 0.091\n",
      "Sequence 785858/3263200 (epoch 24), batch 80115, train_loss = 0.909, time/batch = 0.093\n",
      "Sequence 786858/3263200 (epoch 24), batch 80215, train_loss = 1.748, time/batch = 0.091\n",
      "Sequence 787828/3263200 (epoch 24), batch 80315, train_loss = 0.882, time/batch = 0.092\n",
      "Sequence 788808/3263200 (epoch 24), batch 80415, train_loss = 0.817, time/batch = 0.093\n",
      "Sequence 789788/3263200 (epoch 24), batch 80515, train_loss = 1.037, time/batch = 0.093\n",
      "Sequence 790758/3263200 (epoch 24), batch 80615, train_loss = 0.973, time/batch = 0.092\n",
      "Sequence 791738/3263200 (epoch 24), batch 80715, train_loss = 1.159, time/batch = 0.090\n",
      "Sequence 792718/3263200 (epoch 24), batch 80815, train_loss = 1.591, time/batch = 0.090\n",
      "Sequence 793698/3263200 (epoch 24), batch 80915, train_loss = 0.800, time/batch = 0.092\n",
      "Sequence 794698/3263200 (epoch 24), batch 81015, train_loss = 1.671, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 795668/3263200 (epoch 24), batch 81115, train_loss = 0.876, time/batch = 0.092\n",
      "Sequence 796658/3263200 (epoch 24), batch 81215, train_loss = 1.386, time/batch = 0.091\n",
      "Sequence 797658/3263200 (epoch 24), batch 81315, train_loss = 1.259, time/batch = 0.093\n",
      "Sequence 798648/3263200 (epoch 24), batch 81415, train_loss = 1.135, time/batch = 0.092\n",
      "Sequence 799638/3263200 (epoch 24), batch 81515, train_loss = 1.391, time/batch = 0.093\n",
      "Sequence 800618/3263200 (epoch 24), batch 81615, train_loss = 1.063, time/batch = 0.092\n",
      "Sequence 801608/3263200 (epoch 24), batch 81715, train_loss = 1.470, time/batch = 0.092\n",
      "Sequence 802588/3263200 (epoch 24), batch 81815, train_loss = 1.782, time/batch = 0.092\n",
      "Sequence 803578/3263200 (epoch 24), batch 81915, train_loss = 1.777, time/batch = 0.092\n",
      "Sequence 804548/3263200 (epoch 24), batch 82015, train_loss = 1.720, time/batch = 0.092\n",
      "Sequence 805528/3263200 (epoch 24), batch 82115, train_loss = 1.717, time/batch = 0.091\n",
      "Sequence 806508/3263200 (epoch 24), batch 82215, train_loss = 1.496, time/batch = 0.092\n",
      "Sequence 807488/3263200 (epoch 24), batch 82315, train_loss = 1.658, time/batch = 0.093\n",
      "Sequence 808468/3263200 (epoch 24), batch 82415, train_loss = 1.281, time/batch = 0.092\n",
      "Sequence 809468/3263200 (epoch 24), batch 82515, train_loss = 1.292, time/batch = 0.093\n",
      "Sequence 810458/3263200 (epoch 24), batch 82615, train_loss = 1.581, time/batch = 0.093\n",
      "Sequence 811448/3263200 (epoch 24), batch 82715, train_loss = 1.360, time/batch = 0.092\n",
      "Sequence 812398/3263200 (epoch 24), batch 82815, train_loss = 1.150, time/batch = 0.094\n",
      "Sequence 813358/3263200 (epoch 24), batch 82915, train_loss = 1.477, time/batch = 0.092\n",
      "Sequence 814348/3263200 (epoch 24), batch 83015, train_loss = 1.728, time/batch = 0.092\n",
      "Sequence 815318/3263200 (epoch 24), batch 83115, train_loss = 1.558, time/batch = 0.092\n",
      "Epoch 24 completed, average train loss 1.393048, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 816290/3263200 (epoch 25), batch 83215, train_loss = 1.378, time/batch = 0.094\n",
      "Sequence 817270/3263200 (epoch 25), batch 83315, train_loss = 1.530, time/batch = 0.093\n",
      "Sequence 818260/3263200 (epoch 25), batch 83415, train_loss = 1.435, time/batch = 0.093\n",
      "Sequence 819230/3263200 (epoch 25), batch 83515, train_loss = 1.621, time/batch = 0.093\n",
      "Sequence 820230/3263200 (epoch 25), batch 83615, train_loss = 1.104, time/batch = 0.092\n",
      "Sequence 821170/3263200 (epoch 25), batch 83715, train_loss = 1.094, time/batch = 0.092\n",
      "Sequence 822160/3263200 (epoch 25), batch 83815, train_loss = 1.014, time/batch = 0.093\n",
      "Sequence 823140/3263200 (epoch 25), batch 83915, train_loss = 1.422, time/batch = 0.092\n",
      "Sequence 824110/3263200 (epoch 25), batch 84015, train_loss = 1.935, time/batch = 0.093\n",
      "Sequence 825090/3263200 (epoch 25), batch 84115, train_loss = 1.062, time/batch = 0.092\n",
      "Sequence 826090/3263200 (epoch 25), batch 84215, train_loss = 0.979, time/batch = 0.093\n",
      "Sequence 827080/3263200 (epoch 25), batch 84315, train_loss = 1.549, time/batch = 0.093\n",
      "Sequence 828060/3263200 (epoch 25), batch 84415, train_loss = 1.581, time/batch = 0.093\n",
      "Sequence 829050/3263200 (epoch 25), batch 84515, train_loss = 0.796, time/batch = 0.094\n",
      "Sequence 830050/3263200 (epoch 25), batch 84615, train_loss = 1.242, time/batch = 0.093\n",
      "Sequence 831050/3263200 (epoch 25), batch 84715, train_loss = 1.243, time/batch = 0.092\n",
      "Sequence 832020/3263200 (epoch 25), batch 84815, train_loss = 1.717, time/batch = 0.092\n",
      "Sequence 833000/3263200 (epoch 25), batch 84915, train_loss = 2.127, time/batch = 0.100\n",
      "Sequence 833990/3263200 (epoch 25), batch 85015, train_loss = 1.671, time/batch = 0.101\n",
      "Sequence 834980/3263200 (epoch 25), batch 85115, train_loss = 0.904, time/batch = 0.092\n",
      "Sequence 835960/3263200 (epoch 25), batch 85215, train_loss = 1.491, time/batch = 0.093\n",
      "Sequence 836930/3263200 (epoch 25), batch 85315, train_loss = 1.346, time/batch = 0.094\n",
      "Sequence 837900/3263200 (epoch 25), batch 85415, train_loss = 1.267, time/batch = 0.093\n",
      "Sequence 838890/3263200 (epoch 25), batch 85515, train_loss = 1.087, time/batch = 0.093\n",
      "Sequence 839880/3263200 (epoch 25), batch 85615, train_loss = 2.460, time/batch = 0.093\n",
      "Sequence 840840/3263200 (epoch 25), batch 85715, train_loss = 1.096, time/batch = 0.093\n",
      "Sequence 841790/3263200 (epoch 25), batch 85815, train_loss = 1.094, time/batch = 0.102\n",
      "Sequence 842770/3263200 (epoch 25), batch 85915, train_loss = 1.837, time/batch = 0.104\n",
      "Sequence 843760/3263200 (epoch 25), batch 86015, train_loss = 1.260, time/batch = 0.104\n",
      "Sequence 844750/3263200 (epoch 25), batch 86115, train_loss = 1.162, time/batch = 0.101\n",
      "Sequence 845710/3263200 (epoch 25), batch 86215, train_loss = 1.404, time/batch = 0.103\n",
      "Sequence 846700/3263200 (epoch 25), batch 86315, train_loss = 0.687, time/batch = 0.101\n",
      "Sequence 847670/3263200 (epoch 25), batch 86415, train_loss = 1.136, time/batch = 0.105\n",
      "Epoch 25 completed, average train loss 1.384505, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 848662/3263200 (epoch 26), batch 86515, train_loss = 1.777, time/batch = 0.103\n",
      "Sequence 849642/3263200 (epoch 26), batch 86615, train_loss = 0.953, time/batch = 0.101\n",
      "Sequence 850632/3263200 (epoch 26), batch 86715, train_loss = 1.930, time/batch = 0.104\n",
      "Sequence 851632/3263200 (epoch 26), batch 86815, train_loss = 1.698, time/batch = 0.103\n",
      "Sequence 852632/3263200 (epoch 26), batch 86915, train_loss = 2.364, time/batch = 0.098\n",
      "Sequence 853592/3263200 (epoch 26), batch 87015, train_loss = 1.638, time/batch = 0.100\n",
      "Sequence 854572/3263200 (epoch 26), batch 87115, train_loss = 1.099, time/batch = 0.100\n",
      "Sequence 855552/3263200 (epoch 26), batch 87215, train_loss = 1.661, time/batch = 0.102\n",
      "Sequence 856532/3263200 (epoch 26), batch 87315, train_loss = 1.255, time/batch = 0.104\n",
      "Sequence 857532/3263200 (epoch 26), batch 87415, train_loss = 1.357, time/batch = 0.104\n",
      "Sequence 858522/3263200 (epoch 26), batch 87515, train_loss = 2.149, time/batch = 0.103\n",
      "Sequence 859492/3263200 (epoch 26), batch 87616, train_loss = 0.914, time/batch = 0.099\n",
      "Sequence 860452/3263200 (epoch 26), batch 87716, train_loss = 1.106, time/batch = 0.097\n",
      "Sequence 861442/3263200 (epoch 26), batch 87816, train_loss = 1.877, time/batch = 0.098\n",
      "Sequence 862422/3263200 (epoch 26), batch 87917, train_loss = 0.719, time/batch = 0.098\n",
      "Sequence 863402/3263200 (epoch 26), batch 88017, train_loss = 2.799, time/batch = 0.098\n",
      "Sequence 864362/3263200 (epoch 26), batch 88117, train_loss = 0.954, time/batch = 0.098\n",
      "Sequence 865322/3263200 (epoch 26), batch 88217, train_loss = 1.375, time/batch = 0.098\n",
      "Sequence 866312/3263200 (epoch 26), batch 88317, train_loss = 1.588, time/batch = 0.099\n",
      "Sequence 867302/3263200 (epoch 26), batch 88417, train_loss = 1.890, time/batch = 0.096\n",
      "Sequence 868302/3263200 (epoch 26), batch 88517, train_loss = 1.919, time/batch = 0.098\n",
      "Sequence 869252/3263200 (epoch 26), batch 88617, train_loss = 0.581, time/batch = 0.103\n",
      "Sequence 870242/3263200 (epoch 26), batch 88717, train_loss = 1.257, time/batch = 0.097\n",
      "Sequence 871232/3263200 (epoch 26), batch 88817, train_loss = 1.271, time/batch = 0.098\n",
      "Sequence 872222/3263200 (epoch 26), batch 88917, train_loss = 1.426, time/batch = 0.097\n",
      "Sequence 873212/3263200 (epoch 26), batch 89017, train_loss = 1.336, time/batch = 0.099\n",
      "Sequence 874192/3263200 (epoch 26), batch 89117, train_loss = 1.405, time/batch = 0.099\n",
      "Sequence 875172/3263200 (epoch 26), batch 89217, train_loss = 1.531, time/batch = 0.098\n",
      "Sequence 876142/3263200 (epoch 26), batch 89317, train_loss = 1.355, time/batch = 0.096\n",
      "Sequence 877132/3263200 (epoch 26), batch 89417, train_loss = 1.333, time/batch = 0.099\n",
      "Sequence 878112/3263200 (epoch 26), batch 89517, train_loss = 1.592, time/batch = 0.099\n",
      "Sequence 879072/3263200 (epoch 26), batch 89617, train_loss = 1.002, time/batch = 0.100\n",
      "Sequence 880052/3263200 (epoch 26), batch 89717, train_loss = 1.074, time/batch = 0.100\n",
      "Sequence 881042/3263200 (epoch 26), batch 89817, train_loss = 1.545, time/batch = 0.102\n",
      "Epoch 26 completed, average train loss 1.375883, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 882024/3263200 (epoch 27), batch 89917, train_loss = 1.037, time/batch = 0.101\n",
      "Sequence 883014/3263200 (epoch 27), batch 90017, train_loss = 0.931, time/batch = 0.104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 883994/3263200 (epoch 27), batch 90117, train_loss = 1.281, time/batch = 0.101\n",
      "Sequence 884974/3263200 (epoch 27), batch 90217, train_loss = 1.006, time/batch = 0.102\n",
      "Sequence 885964/3263200 (epoch 27), batch 90317, train_loss = 1.470, time/batch = 0.102\n",
      "Sequence 886934/3263200 (epoch 27), batch 90417, train_loss = 1.229, time/batch = 0.104\n",
      "Sequence 887874/3263200 (epoch 27), batch 90517, train_loss = 1.323, time/batch = 0.104\n",
      "Sequence 888864/3263200 (epoch 27), batch 90617, train_loss = 0.865, time/batch = 0.102\n",
      "Sequence 889844/3263200 (epoch 27), batch 90717, train_loss = 1.419, time/batch = 0.100\n",
      "Sequence 890824/3263200 (epoch 27), batch 90817, train_loss = 1.102, time/batch = 0.095\n",
      "Sequence 891764/3263200 (epoch 27), batch 90917, train_loss = 1.293, time/batch = 0.094\n",
      "Sequence 892754/3263200 (epoch 27), batch 91017, train_loss = 1.377, time/batch = 0.095\n",
      "Sequence 893704/3263200 (epoch 27), batch 91117, train_loss = 1.289, time/batch = 0.102\n",
      "Sequence 894704/3263200 (epoch 27), batch 91217, train_loss = 1.542, time/batch = 0.105\n",
      "Sequence 895684/3263200 (epoch 27), batch 91317, train_loss = 1.136, time/batch = 0.104\n",
      "Sequence 896684/3263200 (epoch 27), batch 91417, train_loss = 1.585, time/batch = 0.105\n",
      "Sequence 897684/3263200 (epoch 27), batch 91517, train_loss = 1.335, time/batch = 0.103\n",
      "Sequence 898684/3263200 (epoch 27), batch 91617, train_loss = 1.134, time/batch = 0.103\n",
      "Sequence 899674/3263200 (epoch 27), batch 91717, train_loss = 1.659, time/batch = 0.101\n",
      "Sequence 900654/3263200 (epoch 27), batch 91817, train_loss = 1.730, time/batch = 0.103\n",
      "Sequence 901644/3263200 (epoch 27), batch 91917, train_loss = 0.903, time/batch = 0.104\n",
      "Sequence 902624/3263200 (epoch 27), batch 92017, train_loss = 1.721, time/batch = 0.106\n",
      "Sequence 903604/3263200 (epoch 27), batch 92117, train_loss = 1.421, time/batch = 0.102\n",
      "Sequence 904574/3263200 (epoch 27), batch 92217, train_loss = 1.314, time/batch = 0.102\n",
      "Sequence 905554/3263200 (epoch 27), batch 92317, train_loss = 1.224, time/batch = 0.103\n",
      "Sequence 906544/3263200 (epoch 27), batch 92418, train_loss = 1.271, time/batch = 0.107\n",
      "Sequence 907544/3263200 (epoch 27), batch 92518, train_loss = 0.685, time/batch = 0.103\n",
      "Sequence 908534/3263200 (epoch 27), batch 92618, train_loss = 1.723, time/batch = 0.099\n",
      "Sequence 909524/3263200 (epoch 27), batch 92718, train_loss = 1.451, time/batch = 0.098\n",
      "Sequence 910504/3263200 (epoch 27), batch 92818, train_loss = 1.557, time/batch = 0.099\n",
      "Sequence 911474/3263200 (epoch 27), batch 92918, train_loss = 1.655, time/batch = 0.099\n",
      "Sequence 912454/3263200 (epoch 27), batch 93018, train_loss = 1.259, time/batch = 0.100\n",
      "Sequence 913444/3263200 (epoch 27), batch 93118, train_loss = 1.370, time/batch = 0.101\n",
      "Epoch 27 completed, average train loss 1.368834, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 914406/3263200 (epoch 28), batch 93218, train_loss = 1.263, time/batch = 0.103\n",
      "Sequence 915396/3263200 (epoch 28), batch 93318, train_loss = 0.925, time/batch = 0.103\n",
      "Sequence 916366/3263200 (epoch 28), batch 93418, train_loss = 0.689, time/batch = 0.104\n",
      "Sequence 917366/3263200 (epoch 28), batch 93518, train_loss = 2.044, time/batch = 0.103\n",
      "Sequence 918326/3263200 (epoch 28), batch 93618, train_loss = 0.975, time/batch = 0.102\n",
      "Sequence 919316/3263200 (epoch 28), batch 93718, train_loss = 1.125, time/batch = 0.105\n",
      "Sequence 920296/3263200 (epoch 28), batch 93818, train_loss = 1.447, time/batch = 0.100\n",
      "Sequence 921256/3263200 (epoch 28), batch 93918, train_loss = 1.073, time/batch = 0.099\n",
      "Sequence 922236/3263200 (epoch 28), batch 94018, train_loss = 1.169, time/batch = 0.099\n",
      "Sequence 923226/3263200 (epoch 28), batch 94118, train_loss = 1.279, time/batch = 0.093\n",
      "Sequence 924196/3263200 (epoch 28), batch 94218, train_loss = 1.282, time/batch = 0.093\n",
      "Sequence 925186/3263200 (epoch 28), batch 94318, train_loss = 1.497, time/batch = 0.096\n",
      "Sequence 926136/3263200 (epoch 28), batch 94418, train_loss = 1.129, time/batch = 0.100\n",
      "Sequence 927106/3263200 (epoch 28), batch 94518, train_loss = 1.386, time/batch = 0.098\n",
      "Sequence 928096/3263200 (epoch 28), batch 94618, train_loss = 1.122, time/batch = 0.098\n",
      "Sequence 929056/3263200 (epoch 28), batch 94718, train_loss = 2.318, time/batch = 0.101\n",
      "Sequence 930036/3263200 (epoch 28), batch 94818, train_loss = 0.845, time/batch = 0.104\n",
      "Sequence 931036/3263200 (epoch 28), batch 94919, train_loss = 0.451, time/batch = 0.102\n",
      "Sequence 932026/3263200 (epoch 28), batch 95019, train_loss = 1.450, time/batch = 0.106\n",
      "Sequence 933026/3263200 (epoch 28), batch 95119, train_loss = 0.874, time/batch = 0.101\n",
      "Sequence 934006/3263200 (epoch 28), batch 95219, train_loss = 1.284, time/batch = 0.105\n",
      "Sequence 934966/3263200 (epoch 28), batch 95319, train_loss = 0.844, time/batch = 0.105\n",
      "Sequence 935946/3263200 (epoch 28), batch 95419, train_loss = 1.038, time/batch = 0.101\n",
      "Sequence 936916/3263200 (epoch 28), batch 95519, train_loss = 1.122, time/batch = 0.101\n",
      "Sequence 937896/3263200 (epoch 28), batch 95619, train_loss = 1.359, time/batch = 0.103\n",
      "Sequence 938886/3263200 (epoch 28), batch 95719, train_loss = 1.408, time/batch = 0.103\n",
      "Sequence 939886/3263200 (epoch 28), batch 95819, train_loss = 2.280, time/batch = 0.103\n",
      "Sequence 940876/3263200 (epoch 28), batch 95919, train_loss = 1.012, time/batch = 0.102\n",
      "Sequence 941846/3263200 (epoch 28), batch 96019, train_loss = 0.896, time/batch = 0.102\n",
      "Sequence 942826/3263200 (epoch 28), batch 96119, train_loss = 1.278, time/batch = 0.102\n",
      "Sequence 943816/3263200 (epoch 28), batch 96219, train_loss = 1.276, time/batch = 0.101\n",
      "Sequence 944816/3263200 (epoch 28), batch 96319, train_loss = 1.385, time/batch = 0.103\n",
      "Sequence 945806/3263200 (epoch 28), batch 96419, train_loss = 1.567, time/batch = 0.101\n",
      "Epoch 28 completed, average train loss 1.360952, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 946768/3263200 (epoch 29), batch 96519, train_loss = 1.301, time/batch = 0.103\n",
      "Sequence 947748/3263200 (epoch 29), batch 96619, train_loss = 1.371, time/batch = 0.099\n",
      "Sequence 948738/3263200 (epoch 29), batch 96719, train_loss = 1.769, time/batch = 0.098\n",
      "Sequence 949718/3263200 (epoch 29), batch 96819, train_loss = 1.536, time/batch = 0.096\n",
      "Sequence 950708/3263200 (epoch 29), batch 96919, train_loss = 1.071, time/batch = 0.098\n",
      "Sequence 951708/3263200 (epoch 29), batch 97019, train_loss = 0.670, time/batch = 0.098\n",
      "Sequence 952678/3263200 (epoch 29), batch 97119, train_loss = 1.372, time/batch = 0.099\n",
      "Sequence 953658/3263200 (epoch 29), batch 97219, train_loss = 1.110, time/batch = 0.099\n",
      "Sequence 954608/3263200 (epoch 29), batch 97319, train_loss = 1.278, time/batch = 0.099\n",
      "Sequence 955578/3263200 (epoch 29), batch 97419, train_loss = 1.264, time/batch = 0.100\n",
      "Sequence 956558/3263200 (epoch 29), batch 97520, train_loss = 0.619, time/batch = 0.105\n",
      "Sequence 957558/3263200 (epoch 29), batch 97620, train_loss = 1.516, time/batch = 0.107\n",
      "Sequence 958538/3263200 (epoch 29), batch 97720, train_loss = 1.077, time/batch = 0.105\n",
      "Sequence 959528/3263200 (epoch 29), batch 97820, train_loss = 1.092, time/batch = 0.104\n",
      "Sequence 960528/3263200 (epoch 29), batch 97920, train_loss = 1.837, time/batch = 0.095\n",
      "Sequence 961488/3263200 (epoch 29), batch 98020, train_loss = 1.181, time/batch = 0.094\n",
      "Sequence 962468/3263200 (epoch 29), batch 98120, train_loss = 1.239, time/batch = 0.094\n",
      "Sequence 963428/3263200 (epoch 29), batch 98220, train_loss = 1.360, time/batch = 0.097\n",
      "Sequence 964418/3263200 (epoch 29), batch 98320, train_loss = 1.400, time/batch = 0.102\n",
      "Sequence 965398/3263200 (epoch 29), batch 98420, train_loss = 1.039, time/batch = 0.104\n",
      "Sequence 966388/3263200 (epoch 29), batch 98520, train_loss = 1.697, time/batch = 0.102\n",
      "Sequence 967338/3263200 (epoch 29), batch 98620, train_loss = 1.046, time/batch = 0.105\n",
      "Sequence 968328/3263200 (epoch 29), batch 98720, train_loss = 1.040, time/batch = 0.102\n",
      "Sequence 969318/3263200 (epoch 29), batch 98820, train_loss = 1.444, time/batch = 0.101\n",
      "Sequence 970278/3263200 (epoch 29), batch 98920, train_loss = 1.151, time/batch = 0.104\n",
      "Sequence 971278/3263200 (epoch 29), batch 99020, train_loss = 1.221, time/batch = 0.103\n",
      "Sequence 972248/3263200 (epoch 29), batch 99120, train_loss = 0.598, time/batch = 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 973248/3263200 (epoch 29), batch 99220, train_loss = 0.943, time/batch = 0.093\n",
      "Sequence 974218/3263200 (epoch 29), batch 99320, train_loss = 1.374, time/batch = 0.101\n",
      "Sequence 975208/3263200 (epoch 29), batch 99420, train_loss = 1.399, time/batch = 0.098\n",
      "Sequence 976188/3263200 (epoch 29), batch 99520, train_loss = 1.436, time/batch = 0.093\n",
      "Sequence 977168/3263200 (epoch 29), batch 99620, train_loss = 1.354, time/batch = 0.095\n",
      "Sequence 978168/3263200 (epoch 29), batch 99720, train_loss = 1.005, time/batch = 0.095\n",
      "Epoch 29 completed, average train loss 1.355342, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 979160/3263200 (epoch 30), batch 99820, train_loss = 1.615, time/batch = 0.096\n",
      "Sequence 980140/3263200 (epoch 30), batch 99920, train_loss = 1.065, time/batch = 0.094\n",
      "Sequence 981120/3263200 (epoch 30), batch 100020, train_loss = 0.810, time/batch = 0.095\n",
      "Sequence 982080/3263200 (epoch 30), batch 100120, train_loss = 1.636, time/batch = 0.093\n",
      "Sequence 983040/3263200 (epoch 30), batch 100220, train_loss = 1.896, time/batch = 0.102\n",
      "Sequence 984030/3263200 (epoch 30), batch 100320, train_loss = 1.289, time/batch = 0.093\n",
      "Sequence 985000/3263200 (epoch 30), batch 100420, train_loss = 1.354, time/batch = 0.093\n",
      "Sequence 986000/3263200 (epoch 30), batch 100520, train_loss = 0.766, time/batch = 0.094\n",
      "Sequence 986990/3263200 (epoch 30), batch 100620, train_loss = 1.194, time/batch = 0.094\n",
      "Sequence 987950/3263200 (epoch 30), batch 100720, train_loss = 1.044, time/batch = 0.092\n",
      "Sequence 988920/3263200 (epoch 30), batch 100820, train_loss = 1.434, time/batch = 0.093\n",
      "Sequence 989910/3263200 (epoch 30), batch 100920, train_loss = 1.510, time/batch = 0.092\n",
      "Sequence 990910/3263200 (epoch 30), batch 101020, train_loss = 1.304, time/batch = 0.093\n",
      "Sequence 991900/3263200 (epoch 30), batch 101120, train_loss = 1.489, time/batch = 0.093\n",
      "Sequence 992860/3263200 (epoch 30), batch 101220, train_loss = 0.864, time/batch = 0.093\n",
      "Sequence 993840/3263200 (epoch 30), batch 101320, train_loss = 1.598, time/batch = 0.093\n",
      "Sequence 994830/3263200 (epoch 30), batch 101420, train_loss = 1.186, time/batch = 0.093\n",
      "Sequence 995810/3263200 (epoch 30), batch 101520, train_loss = 1.175, time/batch = 0.091\n",
      "Sequence 996810/3263200 (epoch 30), batch 101620, train_loss = 1.259, time/batch = 0.092\n",
      "Sequence 997760/3263200 (epoch 30), batch 101720, train_loss = 1.551, time/batch = 0.093\n",
      "Sequence 998750/3263200 (epoch 30), batch 101820, train_loss = 1.394, time/batch = 0.093\n",
      "Sequence 999740/3263200 (epoch 30), batch 101920, train_loss = 2.113, time/batch = 0.092\n",
      "Sequence 1000730/3263200 (epoch 30), batch 102020, train_loss = 1.159, time/batch = 0.094\n",
      "Sequence 1001710/3263200 (epoch 30), batch 102120, train_loss = 1.373, time/batch = 0.092\n",
      "Sequence 1002700/3263200 (epoch 30), batch 102220, train_loss = 1.469, time/batch = 0.093\n",
      "Sequence 1003670/3263200 (epoch 30), batch 102320, train_loss = 1.581, time/batch = 0.091\n",
      "Sequence 1004670/3263200 (epoch 30), batch 102420, train_loss = 1.501, time/batch = 0.092\n",
      "Sequence 1005650/3263200 (epoch 30), batch 102521, train_loss = 0.650, time/batch = 0.092\n",
      "Sequence 1006620/3263200 (epoch 30), batch 102621, train_loss = 1.409, time/batch = 0.092\n",
      "Sequence 1007620/3263200 (epoch 30), batch 102721, train_loss = 1.272, time/batch = 0.093\n",
      "Sequence 1008600/3263200 (epoch 30), batch 102821, train_loss = 1.392, time/batch = 0.092\n",
      "Sequence 1009580/3263200 (epoch 30), batch 102921, train_loss = 1.675, time/batch = 0.093\n",
      "Sequence 1010560/3263200 (epoch 30), batch 103021, train_loss = 1.257, time/batch = 0.093\n",
      "Sequence 1011540/3263200 (epoch 30), batch 103121, train_loss = 1.350, time/batch = 0.092\n",
      "Epoch 30 completed, average train loss 1.350530, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1012512/3263200 (epoch 31), batch 103221, train_loss = 2.835, time/batch = 0.094\n",
      "Sequence 1013502/3263200 (epoch 31), batch 103321, train_loss = 1.468, time/batch = 0.092\n",
      "Sequence 1014482/3263200 (epoch 31), batch 103421, train_loss = 1.100, time/batch = 0.093\n",
      "Sequence 1015472/3263200 (epoch 31), batch 103521, train_loss = 1.204, time/batch = 0.093\n",
      "Sequence 1016422/3263200 (epoch 31), batch 103621, train_loss = 1.510, time/batch = 0.093\n",
      "Sequence 1017422/3263200 (epoch 31), batch 103721, train_loss = 0.916, time/batch = 0.093\n",
      "Sequence 1018412/3263200 (epoch 31), batch 103821, train_loss = 1.305, time/batch = 0.093\n",
      "Sequence 1019382/3263200 (epoch 31), batch 103921, train_loss = 0.744, time/batch = 0.092\n",
      "Sequence 1020352/3263200 (epoch 31), batch 104021, train_loss = 1.264, time/batch = 0.092\n",
      "Sequence 1021352/3263200 (epoch 31), batch 104121, train_loss = 1.400, time/batch = 0.094\n",
      "Sequence 1022342/3263200 (epoch 31), batch 104221, train_loss = 0.785, time/batch = 0.093\n",
      "Sequence 1023312/3263200 (epoch 31), batch 104321, train_loss = 1.749, time/batch = 0.093\n",
      "Sequence 1024292/3263200 (epoch 31), batch 104421, train_loss = 1.059, time/batch = 0.094\n",
      "Sequence 1025262/3263200 (epoch 31), batch 104521, train_loss = 1.026, time/batch = 0.092\n",
      "Sequence 1026232/3263200 (epoch 31), batch 104621, train_loss = 1.532, time/batch = 0.092\n",
      "Sequence 1027212/3263200 (epoch 31), batch 104721, train_loss = 1.066, time/batch = 0.092\n",
      "Sequence 1028182/3263200 (epoch 31), batch 104821, train_loss = 1.332, time/batch = 0.094\n",
      "Sequence 1029162/3263200 (epoch 31), batch 104921, train_loss = 1.372, time/batch = 0.093\n",
      "Sequence 1030142/3263200 (epoch 31), batch 105021, train_loss = 2.869, time/batch = 0.092\n",
      "Sequence 1031122/3263200 (epoch 31), batch 105121, train_loss = 1.220, time/batch = 0.092\n",
      "Sequence 1032122/3263200 (epoch 31), batch 105221, train_loss = 1.694, time/batch = 0.092\n",
      "Sequence 1033122/3263200 (epoch 31), batch 105321, train_loss = 0.933, time/batch = 0.091\n",
      "Sequence 1034112/3263200 (epoch 31), batch 105421, train_loss = 1.125, time/batch = 0.092\n",
      "Sequence 1035072/3263200 (epoch 31), batch 105521, train_loss = 1.981, time/batch = 0.092\n",
      "Sequence 1036072/3263200 (epoch 31), batch 105621, train_loss = 1.666, time/batch = 0.093\n",
      "Sequence 1037072/3263200 (epoch 31), batch 105721, train_loss = 0.909, time/batch = 0.093\n",
      "Sequence 1038052/3263200 (epoch 31), batch 105821, train_loss = 1.970, time/batch = 0.093\n",
      "Sequence 1039042/3263200 (epoch 31), batch 105921, train_loss = 1.119, time/batch = 0.092\n",
      "Sequence 1040032/3263200 (epoch 31), batch 106021, train_loss = 1.047, time/batch = 0.093\n",
      "Sequence 1041022/3263200 (epoch 31), batch 106121, train_loss = 0.692, time/batch = 0.093\n",
      "Sequence 1041982/3263200 (epoch 31), batch 106221, train_loss = 1.075, time/batch = 0.092\n",
      "Sequence 1042942/3263200 (epoch 31), batch 106321, train_loss = 1.417, time/batch = 0.091\n",
      "Sequence 1043912/3263200 (epoch 31), batch 106421, train_loss = 1.208, time/batch = 0.093\n",
      "Epoch 31 completed, average train loss 1.343782, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1044894/3263200 (epoch 32), batch 106521, train_loss = 1.193, time/batch = 0.092\n",
      "Sequence 1045894/3263200 (epoch 32), batch 106621, train_loss = 1.824, time/batch = 0.092\n",
      "Sequence 1046864/3263200 (epoch 32), batch 106721, train_loss = 1.061, time/batch = 0.093\n",
      "Sequence 1047834/3263200 (epoch 32), batch 106821, train_loss = 1.031, time/batch = 0.093\n",
      "Sequence 1048824/3263200 (epoch 32), batch 106921, train_loss = 0.924, time/batch = 0.093\n",
      "Sequence 1049804/3263200 (epoch 32), batch 107021, train_loss = 0.950, time/batch = 0.091\n",
      "Sequence 1050794/3263200 (epoch 32), batch 107121, train_loss = 1.104, time/batch = 0.093\n",
      "Sequence 1051794/3263200 (epoch 32), batch 107221, train_loss = 1.461, time/batch = 0.091\n",
      "Sequence 1052734/3263200 (epoch 32), batch 107321, train_loss = 0.955, time/batch = 0.092\n",
      "Sequence 1053724/3263200 (epoch 32), batch 107421, train_loss = 0.884, time/batch = 0.092\n",
      "Sequence 1054724/3263200 (epoch 32), batch 107521, train_loss = 1.573, time/batch = 0.093\n",
      "Sequence 1055724/3263200 (epoch 32), batch 107621, train_loss = 1.819, time/batch = 0.093\n",
      "Sequence 1056704/3263200 (epoch 32), batch 107721, train_loss = 1.442, time/batch = 0.092\n",
      "Sequence 1057694/3263200 (epoch 32), batch 107821, train_loss = 1.045, time/batch = 0.092\n",
      "Sequence 1058664/3263200 (epoch 32), batch 107921, train_loss = 2.243, time/batch = 0.091\n",
      "Sequence 1059634/3263200 (epoch 32), batch 108021, train_loss = 0.837, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1060614/3263200 (epoch 32), batch 108121, train_loss = 1.316, time/batch = 0.093\n",
      "Sequence 1061614/3263200 (epoch 32), batch 108221, train_loss = 0.923, time/batch = 0.093\n",
      "Sequence 1062604/3263200 (epoch 32), batch 108321, train_loss = 1.491, time/batch = 0.093\n",
      "Sequence 1063584/3263200 (epoch 32), batch 108421, train_loss = 1.235, time/batch = 0.092\n",
      "Sequence 1064564/3263200 (epoch 32), batch 108521, train_loss = 1.207, time/batch = 0.092\n",
      "Sequence 1065554/3263200 (epoch 32), batch 108621, train_loss = 1.114, time/batch = 0.093\n",
      "Sequence 1066514/3263200 (epoch 32), batch 108721, train_loss = 1.018, time/batch = 0.092\n",
      "Sequence 1067504/3263200 (epoch 32), batch 108821, train_loss = 1.626, time/batch = 0.093\n",
      "Sequence 1068484/3263200 (epoch 32), batch 108921, train_loss = 1.484, time/batch = 0.093\n",
      "Sequence 1069474/3263200 (epoch 32), batch 109021, train_loss = 0.863, time/batch = 0.093\n",
      "Sequence 1070424/3263200 (epoch 32), batch 109121, train_loss = 1.375, time/batch = 0.093\n",
      "Sequence 1071394/3263200 (epoch 32), batch 109221, train_loss = 1.361, time/batch = 0.092\n",
      "Sequence 1072374/3263200 (epoch 32), batch 109321, train_loss = 1.075, time/batch = 0.092\n",
      "Sequence 1073364/3263200 (epoch 32), batch 109421, train_loss = 1.176, time/batch = 0.092\n",
      "Sequence 1074354/3263200 (epoch 32), batch 109521, train_loss = 0.912, time/batch = 0.093\n",
      "Sequence 1075344/3263200 (epoch 32), batch 109621, train_loss = 0.766, time/batch = 0.094\n",
      "Sequence 1076304/3263200 (epoch 32), batch 109721, train_loss = 1.479, time/batch = 0.092\n",
      "Epoch 32 completed, average train loss 1.336252, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1077276/3263200 (epoch 33), batch 109821, train_loss = 1.242, time/batch = 0.094\n",
      "Sequence 1078256/3263200 (epoch 33), batch 109921, train_loss = 1.464, time/batch = 0.094\n",
      "Sequence 1079256/3263200 (epoch 33), batch 110021, train_loss = 0.853, time/batch = 0.092\n",
      "Sequence 1080246/3263200 (epoch 33), batch 110121, train_loss = 1.443, time/batch = 0.092\n",
      "Sequence 1081226/3263200 (epoch 33), batch 110221, train_loss = 1.082, time/batch = 0.093\n",
      "Sequence 1082216/3263200 (epoch 33), batch 110321, train_loss = 1.595, time/batch = 0.093\n",
      "Sequence 1083216/3263200 (epoch 33), batch 110421, train_loss = 1.399, time/batch = 0.091\n",
      "Sequence 1084206/3263200 (epoch 33), batch 110521, train_loss = 1.855, time/batch = 0.093\n",
      "Sequence 1085196/3263200 (epoch 33), batch 110621, train_loss = 1.023, time/batch = 0.092\n",
      "Sequence 1086176/3263200 (epoch 33), batch 110721, train_loss = 0.979, time/batch = 0.093\n",
      "Sequence 1087146/3263200 (epoch 33), batch 110821, train_loss = 1.489, time/batch = 0.092\n",
      "Sequence 1088106/3263200 (epoch 33), batch 110921, train_loss = 1.080, time/batch = 0.092\n",
      "Sequence 1089086/3263200 (epoch 33), batch 111021, train_loss = 1.468, time/batch = 0.093\n",
      "Sequence 1090076/3263200 (epoch 33), batch 111121, train_loss = 1.353, time/batch = 0.093\n",
      "Sequence 1091056/3263200 (epoch 33), batch 111221, train_loss = 1.771, time/batch = 0.094\n",
      "Sequence 1092046/3263200 (epoch 33), batch 111321, train_loss = 1.948, time/batch = 0.093\n",
      "Sequence 1093026/3263200 (epoch 33), batch 111421, train_loss = 1.324, time/batch = 0.093\n",
      "Sequence 1094016/3263200 (epoch 33), batch 111521, train_loss = 1.274, time/batch = 0.092\n",
      "Sequence 1095006/3263200 (epoch 33), batch 111621, train_loss = 1.383, time/batch = 0.092\n",
      "Sequence 1096006/3263200 (epoch 33), batch 111721, train_loss = 1.479, time/batch = 0.093\n",
      "Sequence 1096996/3263200 (epoch 33), batch 111821, train_loss = 0.827, time/batch = 0.092\n",
      "Sequence 1097956/3263200 (epoch 33), batch 111921, train_loss = 0.817, time/batch = 0.093\n",
      "Sequence 1098926/3263200 (epoch 33), batch 112021, train_loss = 1.572, time/batch = 0.092\n",
      "Sequence 1099896/3263200 (epoch 33), batch 112121, train_loss = 1.481, time/batch = 0.092\n",
      "Sequence 1100866/3263200 (epoch 33), batch 112221, train_loss = 1.140, time/batch = 0.092\n",
      "Sequence 1101856/3263200 (epoch 33), batch 112321, train_loss = 1.274, time/batch = 0.094\n",
      "Sequence 1102826/3263200 (epoch 33), batch 112421, train_loss = 1.208, time/batch = 0.093\n",
      "Sequence 1103796/3263200 (epoch 33), batch 112521, train_loss = 1.038, time/batch = 0.092\n",
      "Sequence 1104726/3263200 (epoch 33), batch 112621, train_loss = 2.235, time/batch = 0.093\n",
      "Sequence 1105686/3263200 (epoch 33), batch 112721, train_loss = 1.070, time/batch = 0.091\n",
      "Sequence 1106666/3263200 (epoch 33), batch 112821, train_loss = 1.865, time/batch = 0.091\n",
      "Sequence 1107656/3263200 (epoch 33), batch 112921, train_loss = 0.979, time/batch = 0.088\n",
      "Sequence 1108656/3263200 (epoch 33), batch 113021, train_loss = 1.134, time/batch = 0.092\n",
      "Epoch 33 completed, average train loss 1.330913, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1109638/3263200 (epoch 34), batch 113121, train_loss = 0.816, time/batch = 0.094\n",
      "Sequence 1110608/3263200 (epoch 34), batch 113221, train_loss = 1.868, time/batch = 0.092\n",
      "Sequence 1111578/3263200 (epoch 34), batch 113321, train_loss = 1.578, time/batch = 0.092\n",
      "Sequence 1112548/3263200 (epoch 34), batch 113421, train_loss = 1.441, time/batch = 0.092\n",
      "Sequence 1113528/3263200 (epoch 34), batch 113521, train_loss = 1.494, time/batch = 0.092\n",
      "Sequence 1114508/3263200 (epoch 34), batch 113621, train_loss = 1.762, time/batch = 0.092\n",
      "Sequence 1115488/3263200 (epoch 34), batch 113721, train_loss = 1.103, time/batch = 0.093\n",
      "Sequence 1116468/3263200 (epoch 34), batch 113821, train_loss = 1.478, time/batch = 0.092\n",
      "Sequence 1117468/3263200 (epoch 34), batch 113921, train_loss = 1.516, time/batch = 0.092\n",
      "Sequence 1118458/3263200 (epoch 34), batch 114021, train_loss = 0.952, time/batch = 0.093\n",
      "Sequence 1119428/3263200 (epoch 34), batch 114121, train_loss = 1.344, time/batch = 0.094\n",
      "Sequence 1120408/3263200 (epoch 34), batch 114221, train_loss = 1.081, time/batch = 0.093\n",
      "Sequence 1121388/3263200 (epoch 34), batch 114321, train_loss = 0.945, time/batch = 0.092\n",
      "Sequence 1122388/3263200 (epoch 34), batch 114421, train_loss = 1.634, time/batch = 0.091\n",
      "Sequence 1123378/3263200 (epoch 34), batch 114521, train_loss = 1.069, time/batch = 0.091\n",
      "Sequence 1124348/3263200 (epoch 34), batch 114621, train_loss = 1.259, time/batch = 0.094\n",
      "Sequence 1125338/3263200 (epoch 34), batch 114721, train_loss = 1.626, time/batch = 0.092\n",
      "Sequence 1126328/3263200 (epoch 34), batch 114821, train_loss = 1.296, time/batch = 0.092\n",
      "Sequence 1127308/3263200 (epoch 34), batch 114921, train_loss = 1.333, time/batch = 0.092\n",
      "Sequence 1128308/3263200 (epoch 34), batch 115021, train_loss = 2.085, time/batch = 0.094\n",
      "Sequence 1129268/3263200 (epoch 34), batch 115121, train_loss = 1.146, time/batch = 0.092\n",
      "Sequence 1130228/3263200 (epoch 34), batch 115221, train_loss = 1.614, time/batch = 0.094\n",
      "Sequence 1131208/3263200 (epoch 34), batch 115321, train_loss = 1.472, time/batch = 0.093\n",
      "Sequence 1132188/3263200 (epoch 34), batch 115421, train_loss = 2.044, time/batch = 0.093\n",
      "Sequence 1133168/3263200 (epoch 34), batch 115521, train_loss = 1.970, time/batch = 0.092\n",
      "Sequence 1134158/3263200 (epoch 34), batch 115621, train_loss = 1.270, time/batch = 0.092\n",
      "Sequence 1135158/3263200 (epoch 34), batch 115721, train_loss = 1.189, time/batch = 0.093\n",
      "Sequence 1136138/3263200 (epoch 34), batch 115821, train_loss = 2.049, time/batch = 0.093\n",
      "Sequence 1137108/3263200 (epoch 34), batch 115921, train_loss = 1.098, time/batch = 0.093\n",
      "Sequence 1138098/3263200 (epoch 34), batch 116021, train_loss = 1.644, time/batch = 0.091\n",
      "Sequence 1139078/3263200 (epoch 34), batch 116121, train_loss = 0.407, time/batch = 0.092\n",
      "Sequence 1140038/3263200 (epoch 34), batch 116221, train_loss = 1.729, time/batch = 0.092\n",
      "Sequence 1141028/3263200 (epoch 34), batch 116321, train_loss = 1.454, time/batch = 0.093\n",
      "Sequence 1142018/3263200 (epoch 34), batch 116421, train_loss = 1.011, time/batch = 0.093\n",
      "Epoch 34 completed, average train loss 1.325388, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 1142990/3263200 (epoch 35), batch 116521, train_loss = 1.119, time/batch = 0.096\n",
      "Sequence 1143980/3263200 (epoch 35), batch 116621, train_loss = 0.657, time/batch = 0.092\n",
      "Sequence 1144940/3263200 (epoch 35), batch 116721, train_loss = 0.750, time/batch = 0.093\n",
      "Sequence 1145900/3263200 (epoch 35), batch 116821, train_loss = 1.975, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1146880/3263200 (epoch 35), batch 116921, train_loss = 0.936, time/batch = 0.092\n",
      "Sequence 1147870/3263200 (epoch 35), batch 117021, train_loss = 1.367, time/batch = 0.093\n",
      "Sequence 1148840/3263200 (epoch 35), batch 117121, train_loss = 1.904, time/batch = 0.093\n",
      "Sequence 1149840/3263200 (epoch 35), batch 117221, train_loss = 1.548, time/batch = 0.093\n",
      "Sequence 1150840/3263200 (epoch 35), batch 117321, train_loss = 1.037, time/batch = 0.091\n",
      "Sequence 1151810/3263200 (epoch 35), batch 117421, train_loss = 1.352, time/batch = 0.092\n",
      "Sequence 1152790/3263200 (epoch 35), batch 117521, train_loss = 1.630, time/batch = 0.092\n",
      "Sequence 1153790/3263200 (epoch 35), batch 117621, train_loss = 1.798, time/batch = 0.095\n",
      "Sequence 1154750/3263200 (epoch 35), batch 117721, train_loss = 1.025, time/batch = 0.092\n",
      "Sequence 1155730/3263200 (epoch 35), batch 117821, train_loss = 1.294, time/batch = 0.092\n",
      "Sequence 1156730/3263200 (epoch 35), batch 117921, train_loss = 0.901, time/batch = 0.093\n",
      "Sequence 1157720/3263200 (epoch 35), batch 118021, train_loss = 1.159, time/batch = 0.092\n",
      "Sequence 1158690/3263200 (epoch 35), batch 118121, train_loss = 1.133, time/batch = 0.093\n",
      "Sequence 1159690/3263200 (epoch 35), batch 118221, train_loss = 1.083, time/batch = 0.093\n",
      "Sequence 1160650/3263200 (epoch 35), batch 118321, train_loss = 1.124, time/batch = 0.092\n",
      "Sequence 1161650/3263200 (epoch 35), batch 118421, train_loss = 0.687, time/batch = 0.092\n",
      "Sequence 1162650/3263200 (epoch 35), batch 118521, train_loss = 1.435, time/batch = 0.093\n",
      "Sequence 1163600/3263200 (epoch 35), batch 118621, train_loss = 1.795, time/batch = 0.093\n",
      "Sequence 1164600/3263200 (epoch 35), batch 118721, train_loss = 1.135, time/batch = 0.091\n",
      "Sequence 1165570/3263200 (epoch 35), batch 118821, train_loss = 1.282, time/batch = 0.092\n",
      "Sequence 1166560/3263200 (epoch 35), batch 118921, train_loss = 1.670, time/batch = 0.092\n",
      "Sequence 1167550/3263200 (epoch 35), batch 119021, train_loss = 0.778, time/batch = 0.093\n",
      "Sequence 1168540/3263200 (epoch 35), batch 119121, train_loss = 1.684, time/batch = 0.093\n",
      "Sequence 1169510/3263200 (epoch 35), batch 119221, train_loss = 1.694, time/batch = 0.092\n",
      "Sequence 1170510/3263200 (epoch 35), batch 119321, train_loss = 1.057, time/batch = 0.094\n",
      "Sequence 1171460/3263200 (epoch 35), batch 119421, train_loss = 1.087, time/batch = 0.092\n",
      "Sequence 1172450/3263200 (epoch 35), batch 119521, train_loss = 1.095, time/batch = 0.093\n",
      "Sequence 1173420/3263200 (epoch 35), batch 119621, train_loss = 1.273, time/batch = 0.092\n",
      "Sequence 1174390/3263200 (epoch 35), batch 119722, train_loss = 0.864, time/batch = 0.093\n",
      "Epoch 35 completed, average train loss 1.319876, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1175382/3263200 (epoch 36), batch 119822, train_loss = 1.500, time/batch = 0.094\n",
      "Sequence 1176372/3263200 (epoch 36), batch 119922, train_loss = 1.264, time/batch = 0.091\n",
      "Sequence 1177362/3263200 (epoch 36), batch 120022, train_loss = 1.320, time/batch = 0.092\n",
      "Sequence 1178352/3263200 (epoch 36), batch 120122, train_loss = 1.383, time/batch = 0.092\n",
      "Sequence 1179332/3263200 (epoch 36), batch 120222, train_loss = 1.683, time/batch = 0.093\n",
      "Sequence 1180322/3263200 (epoch 36), batch 120322, train_loss = 1.384, time/batch = 0.092\n",
      "Sequence 1181312/3263200 (epoch 36), batch 120422, train_loss = 1.364, time/batch = 0.092\n",
      "Sequence 1182292/3263200 (epoch 36), batch 120522, train_loss = 1.484, time/batch = 0.092\n",
      "Sequence 1183242/3263200 (epoch 36), batch 120622, train_loss = 1.272, time/batch = 0.092\n",
      "Sequence 1184212/3263200 (epoch 36), batch 120722, train_loss = 1.527, time/batch = 0.092\n",
      "Sequence 1185162/3263200 (epoch 36), batch 120822, train_loss = 0.865, time/batch = 0.091\n",
      "Sequence 1186122/3263200 (epoch 36), batch 120922, train_loss = 1.294, time/batch = 0.093\n",
      "Sequence 1187082/3263200 (epoch 36), batch 121022, train_loss = 1.145, time/batch = 0.092\n",
      "Sequence 1188072/3263200 (epoch 36), batch 121122, train_loss = 1.218, time/batch = 0.092\n",
      "Sequence 1189052/3263200 (epoch 36), batch 121222, train_loss = 1.052, time/batch = 0.093\n",
      "Sequence 1190042/3263200 (epoch 36), batch 121322, train_loss = 0.778, time/batch = 0.092\n",
      "Sequence 1191032/3263200 (epoch 36), batch 121422, train_loss = 1.135, time/batch = 0.092\n",
      "Sequence 1191992/3263200 (epoch 36), batch 121522, train_loss = 1.108, time/batch = 0.091\n",
      "Sequence 1192982/3263200 (epoch 36), batch 121622, train_loss = 1.049, time/batch = 0.093\n",
      "Sequence 1193972/3263200 (epoch 36), batch 121722, train_loss = 0.944, time/batch = 0.092\n",
      "Sequence 1194962/3263200 (epoch 36), batch 121822, train_loss = 1.153, time/batch = 0.093\n",
      "Sequence 1195962/3263200 (epoch 36), batch 121922, train_loss = 1.057, time/batch = 0.092\n",
      "Sequence 1196942/3263200 (epoch 36), batch 122022, train_loss = 1.222, time/batch = 0.093\n",
      "Sequence 1197932/3263200 (epoch 36), batch 122122, train_loss = 1.113, time/batch = 0.094\n",
      "Sequence 1198902/3263200 (epoch 36), batch 122222, train_loss = 1.016, time/batch = 0.093\n",
      "Sequence 1199852/3263200 (epoch 36), batch 122322, train_loss = 1.391, time/batch = 0.092\n",
      "Sequence 1200842/3263200 (epoch 36), batch 122422, train_loss = 0.918, time/batch = 0.092\n",
      "Sequence 1201822/3263200 (epoch 36), batch 122522, train_loss = 0.899, time/batch = 0.093\n",
      "Sequence 1202812/3263200 (epoch 36), batch 122622, train_loss = 1.182, time/batch = 0.092\n",
      "Sequence 1203792/3263200 (epoch 36), batch 122722, train_loss = 0.805, time/batch = 0.093\n",
      "Sequence 1204782/3263200 (epoch 36), batch 122822, train_loss = 1.147, time/batch = 0.093\n",
      "Sequence 1205782/3263200 (epoch 36), batch 122922, train_loss = 1.174, time/batch = 0.091\n",
      "Sequence 1206772/3263200 (epoch 36), batch 123023, train_loss = 0.505, time/batch = 0.092\n",
      "Epoch 36 completed, average train loss 1.316437, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1207744/3263200 (epoch 37), batch 123123, train_loss = 1.437, time/batch = 0.093\n",
      "Sequence 1208734/3263200 (epoch 37), batch 123223, train_loss = 1.948, time/batch = 0.093\n",
      "Sequence 1209694/3263200 (epoch 37), batch 123323, train_loss = 1.221, time/batch = 0.094\n",
      "Sequence 1210674/3263200 (epoch 37), batch 123423, train_loss = 0.756, time/batch = 0.092\n",
      "Sequence 1211654/3263200 (epoch 37), batch 123523, train_loss = 0.908, time/batch = 0.093\n",
      "Sequence 1212624/3263200 (epoch 37), batch 123623, train_loss = 1.138, time/batch = 0.093\n",
      "Sequence 1213604/3263200 (epoch 37), batch 123723, train_loss = 0.922, time/batch = 0.091\n",
      "Sequence 1214594/3263200 (epoch 37), batch 123823, train_loss = 1.725, time/batch = 0.093\n",
      "Sequence 1215564/3263200 (epoch 37), batch 123923, train_loss = 1.356, time/batch = 0.092\n",
      "Sequence 1216564/3263200 (epoch 37), batch 124023, train_loss = 1.278, time/batch = 0.093\n",
      "Sequence 1217554/3263200 (epoch 37), batch 124123, train_loss = 1.197, time/batch = 0.094\n",
      "Sequence 1218494/3263200 (epoch 37), batch 124223, train_loss = 1.476, time/batch = 0.094\n",
      "Sequence 1219464/3263200 (epoch 37), batch 124323, train_loss = 2.046, time/batch = 0.093\n",
      "Sequence 1220454/3263200 (epoch 37), batch 124423, train_loss = 1.183, time/batch = 0.092\n",
      "Sequence 1221424/3263200 (epoch 37), batch 124523, train_loss = 1.482, time/batch = 0.093\n",
      "Sequence 1222394/3263200 (epoch 37), batch 124623, train_loss = 1.045, time/batch = 0.092\n",
      "Sequence 1223394/3263200 (epoch 37), batch 124723, train_loss = 1.002, time/batch = 0.093\n",
      "Sequence 1224394/3263200 (epoch 37), batch 124823, train_loss = 1.097, time/batch = 0.092\n",
      "Sequence 1225374/3263200 (epoch 37), batch 124923, train_loss = 1.461, time/batch = 0.094\n",
      "Sequence 1226364/3263200 (epoch 37), batch 125023, train_loss = 0.953, time/batch = 0.093\n",
      "Sequence 1227354/3263200 (epoch 37), batch 125123, train_loss = 1.200, time/batch = 0.092\n",
      "Sequence 1228334/3263200 (epoch 37), batch 125223, train_loss = 1.536, time/batch = 0.092\n",
      "Sequence 1229314/3263200 (epoch 37), batch 125323, train_loss = 1.312, time/batch = 0.093\n",
      "Sequence 1230304/3263200 (epoch 37), batch 125423, train_loss = 1.266, time/batch = 0.091\n",
      "Sequence 1231304/3263200 (epoch 37), batch 125523, train_loss = 1.118, time/batch = 0.093\n",
      "Sequence 1232294/3263200 (epoch 37), batch 125623, train_loss = 1.237, time/batch = 0.093\n",
      "Sequence 1233244/3263200 (epoch 37), batch 125723, train_loss = 1.489, time/batch = 0.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1234224/3263200 (epoch 37), batch 125823, train_loss = 1.043, time/batch = 0.093\n",
      "Sequence 1235214/3263200 (epoch 37), batch 125923, train_loss = 1.354, time/batch = 0.092\n",
      "Sequence 1236174/3263200 (epoch 37), batch 126023, train_loss = 1.662, time/batch = 0.093\n",
      "Sequence 1237154/3263200 (epoch 37), batch 126123, train_loss = 0.996, time/batch = 0.092\n",
      "Sequence 1238154/3263200 (epoch 37), batch 126223, train_loss = 1.141, time/batch = 0.093\n",
      "Sequence 1239144/3263200 (epoch 37), batch 126323, train_loss = 1.317, time/batch = 0.094\n",
      "Epoch 37 completed, average train loss 1.307795, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1240116/3263200 (epoch 38), batch 126423, train_loss = 1.774, time/batch = 0.093\n",
      "Sequence 1241066/3263200 (epoch 38), batch 126523, train_loss = 1.199, time/batch = 0.092\n",
      "Sequence 1242046/3263200 (epoch 38), batch 126623, train_loss = 1.130, time/batch = 0.093\n",
      "Sequence 1243026/3263200 (epoch 38), batch 126723, train_loss = 0.882, time/batch = 0.091\n",
      "Sequence 1244016/3263200 (epoch 38), batch 126823, train_loss = 0.728, time/batch = 0.093\n",
      "Sequence 1245016/3263200 (epoch 38), batch 126923, train_loss = 1.355, time/batch = 0.094\n",
      "Sequence 1245986/3263200 (epoch 38), batch 127023, train_loss = 0.969, time/batch = 0.092\n",
      "Sequence 1246966/3263200 (epoch 38), batch 127123, train_loss = 1.211, time/batch = 0.093\n",
      "Sequence 1247956/3263200 (epoch 38), batch 127223, train_loss = 0.908, time/batch = 0.094\n",
      "Sequence 1248926/3263200 (epoch 38), batch 127323, train_loss = 1.153, time/batch = 0.093\n",
      "Sequence 1249916/3263200 (epoch 38), batch 127423, train_loss = 1.163, time/batch = 0.092\n",
      "Sequence 1250906/3263200 (epoch 38), batch 127523, train_loss = 2.089, time/batch = 0.092\n",
      "Sequence 1251886/3263200 (epoch 38), batch 127623, train_loss = 1.164, time/batch = 0.092\n",
      "Sequence 1252866/3263200 (epoch 38), batch 127723, train_loss = 1.641, time/batch = 0.093\n",
      "Sequence 1253856/3263200 (epoch 38), batch 127823, train_loss = 1.097, time/batch = 0.092\n",
      "Sequence 1254826/3263200 (epoch 38), batch 127923, train_loss = 0.914, time/batch = 0.092\n",
      "Sequence 1255816/3263200 (epoch 38), batch 128023, train_loss = 1.483, time/batch = 0.093\n",
      "Sequence 1256776/3263200 (epoch 38), batch 128123, train_loss = 1.060, time/batch = 0.093\n",
      "Sequence 1257776/3263200 (epoch 38), batch 128223, train_loss = 0.913, time/batch = 0.092\n",
      "Sequence 1258766/3263200 (epoch 38), batch 128323, train_loss = 1.377, time/batch = 0.093\n",
      "Sequence 1259746/3263200 (epoch 38), batch 128423, train_loss = 1.661, time/batch = 0.091\n",
      "Sequence 1260736/3263200 (epoch 38), batch 128523, train_loss = 1.249, time/batch = 0.092\n",
      "Sequence 1261736/3263200 (epoch 38), batch 128623, train_loss = 1.343, time/batch = 0.092\n",
      "Sequence 1262726/3263200 (epoch 38), batch 128723, train_loss = 1.019, time/batch = 0.093\n",
      "Sequence 1263706/3263200 (epoch 38), batch 128823, train_loss = 1.799, time/batch = 0.093\n",
      "Sequence 1264676/3263200 (epoch 38), batch 128923, train_loss = 0.917, time/batch = 0.093\n",
      "Sequence 1265636/3263200 (epoch 38), batch 129023, train_loss = 1.464, time/batch = 0.092\n",
      "Sequence 1266616/3263200 (epoch 38), batch 129123, train_loss = 1.370, time/batch = 0.093\n",
      "Sequence 1267616/3263200 (epoch 38), batch 129223, train_loss = 1.268, time/batch = 0.092\n",
      "Sequence 1268586/3263200 (epoch 38), batch 129323, train_loss = 1.210, time/batch = 0.093\n",
      "Sequence 1269586/3263200 (epoch 38), batch 129423, train_loss = 1.612, time/batch = 0.092\n",
      "Sequence 1270536/3263200 (epoch 38), batch 129523, train_loss = 1.399, time/batch = 0.093\n",
      "Sequence 1271496/3263200 (epoch 38), batch 129623, train_loss = 1.736, time/batch = 0.093\n",
      "Sequence 1272486/3263200 (epoch 38), batch 129723, train_loss = 1.170, time/batch = 0.092\n",
      "Epoch 38 completed, average train loss 1.306005, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1273478/3263200 (epoch 39), batch 129823, train_loss = 1.570, time/batch = 0.093\n",
      "Sequence 1274428/3263200 (epoch 39), batch 129923, train_loss = 0.928, time/batch = 0.093\n",
      "Sequence 1275358/3263200 (epoch 39), batch 130023, train_loss = 1.922, time/batch = 0.093\n",
      "Sequence 1276358/3263200 (epoch 39), batch 130123, train_loss = 1.483, time/batch = 0.093\n",
      "Sequence 1277348/3263200 (epoch 39), batch 130223, train_loss = 1.128, time/batch = 0.091\n",
      "Sequence 1278348/3263200 (epoch 39), batch 130323, train_loss = 1.367, time/batch = 0.092\n",
      "Sequence 1279328/3263200 (epoch 39), batch 130423, train_loss = 0.765, time/batch = 0.094\n",
      "Sequence 1280298/3263200 (epoch 39), batch 130523, train_loss = 1.733, time/batch = 0.092\n",
      "Sequence 1281298/3263200 (epoch 39), batch 130623, train_loss = 1.180, time/batch = 0.093\n",
      "Sequence 1282248/3263200 (epoch 39), batch 130723, train_loss = 0.939, time/batch = 0.092\n",
      "Sequence 1283228/3263200 (epoch 39), batch 130823, train_loss = 1.431, time/batch = 0.093\n",
      "Sequence 1284208/3263200 (epoch 39), batch 130923, train_loss = 1.491, time/batch = 0.093\n",
      "Sequence 1285188/3263200 (epoch 39), batch 131023, train_loss = 1.183, time/batch = 0.093\n",
      "Sequence 1286178/3263200 (epoch 39), batch 131123, train_loss = 1.679, time/batch = 0.092\n",
      "Sequence 1287158/3263200 (epoch 39), batch 131223, train_loss = 1.451, time/batch = 0.092\n",
      "Sequence 1288148/3263200 (epoch 39), batch 131323, train_loss = 1.050, time/batch = 0.092\n",
      "Sequence 1289138/3263200 (epoch 39), batch 131423, train_loss = 1.399, time/batch = 0.092\n",
      "Sequence 1290108/3263200 (epoch 39), batch 131523, train_loss = 2.119, time/batch = 0.093\n",
      "Sequence 1291088/3263200 (epoch 39), batch 131623, train_loss = 1.118, time/batch = 0.092\n",
      "Sequence 1292068/3263200 (epoch 39), batch 131723, train_loss = 1.489, time/batch = 0.093\n",
      "Sequence 1293068/3263200 (epoch 39), batch 131823, train_loss = 0.721, time/batch = 0.093\n",
      "Sequence 1294048/3263200 (epoch 39), batch 131923, train_loss = 1.387, time/batch = 0.093\n",
      "Sequence 1295038/3263200 (epoch 39), batch 132023, train_loss = 1.301, time/batch = 0.093\n",
      "Sequence 1296038/3263200 (epoch 39), batch 132123, train_loss = 0.668, time/batch = 0.093\n",
      "Sequence 1296998/3263200 (epoch 39), batch 132223, train_loss = 1.120, time/batch = 0.093\n",
      "Sequence 1297998/3263200 (epoch 39), batch 132323, train_loss = 1.404, time/batch = 0.093\n",
      "Sequence 1298958/3263200 (epoch 39), batch 132423, train_loss = 0.904, time/batch = 0.093\n",
      "Sequence 1299928/3263200 (epoch 39), batch 132523, train_loss = 1.258, time/batch = 0.093\n",
      "Sequence 1300918/3263200 (epoch 39), batch 132623, train_loss = 1.955, time/batch = 0.093\n",
      "Sequence 1301898/3263200 (epoch 39), batch 132723, train_loss = 1.333, time/batch = 0.093\n",
      "Sequence 1302878/3263200 (epoch 39), batch 132823, train_loss = 1.094, time/batch = 0.093\n",
      "Sequence 1303858/3263200 (epoch 39), batch 132923, train_loss = 1.539, time/batch = 0.093\n",
      "Sequence 1304858/3263200 (epoch 39), batch 133023, train_loss = 1.331, time/batch = 0.093\n",
      "Epoch 39 completed, average train loss 1.299051, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 1305860/3263200 (epoch 40), batch 133123, train_loss = 1.420, time/batch = 0.101\n",
      "Sequence 1306860/3263200 (epoch 40), batch 133223, train_loss = 1.172, time/batch = 0.094\n",
      "Sequence 1307850/3263200 (epoch 40), batch 133323, train_loss = 1.562, time/batch = 0.092\n",
      "Sequence 1308830/3263200 (epoch 40), batch 133423, train_loss = 1.376, time/batch = 0.093\n",
      "Sequence 1309820/3263200 (epoch 40), batch 133523, train_loss = 0.549, time/batch = 0.093\n",
      "Sequence 1310800/3263200 (epoch 40), batch 133623, train_loss = 0.961, time/batch = 0.092\n",
      "Sequence 1311780/3263200 (epoch 40), batch 133723, train_loss = 0.893, time/batch = 0.092\n",
      "Sequence 1312750/3263200 (epoch 40), batch 133823, train_loss = 1.318, time/batch = 0.094\n",
      "Sequence 1313730/3263200 (epoch 40), batch 133923, train_loss = 0.871, time/batch = 0.094\n",
      "Sequence 1314690/3263200 (epoch 40), batch 134023, train_loss = 1.449, time/batch = 0.093\n",
      "Sequence 1315670/3263200 (epoch 40), batch 134123, train_loss = 1.989, time/batch = 0.092\n",
      "Sequence 1316650/3263200 (epoch 40), batch 134223, train_loss = 1.250, time/batch = 0.093\n",
      "Sequence 1317640/3263200 (epoch 40), batch 134323, train_loss = 1.149, time/batch = 0.094\n",
      "Sequence 1318620/3263200 (epoch 40), batch 134423, train_loss = 0.848, time/batch = 0.092\n",
      "Sequence 1319610/3263200 (epoch 40), batch 134523, train_loss = 1.405, time/batch = 0.091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1320570/3263200 (epoch 40), batch 134623, train_loss = 0.778, time/batch = 0.103\n",
      "Sequence 1321550/3263200 (epoch 40), batch 134723, train_loss = 0.956, time/batch = 0.101\n",
      "Sequence 1322520/3263200 (epoch 40), batch 134823, train_loss = 1.370, time/batch = 0.105\n",
      "Sequence 1323510/3263200 (epoch 40), batch 134923, train_loss = 1.026, time/batch = 0.101\n",
      "Sequence 1324490/3263200 (epoch 40), batch 135023, train_loss = 0.715, time/batch = 0.092\n",
      "Sequence 1325490/3263200 (epoch 40), batch 135123, train_loss = 1.623, time/batch = 0.092\n",
      "Sequence 1326480/3263200 (epoch 40), batch 135223, train_loss = 1.752, time/batch = 0.093\n",
      "Sequence 1327460/3263200 (epoch 40), batch 135323, train_loss = 1.198, time/batch = 0.092\n",
      "Sequence 1328430/3263200 (epoch 40), batch 135423, train_loss = 1.411, time/batch = 0.093\n",
      "Sequence 1329420/3263200 (epoch 40), batch 135523, train_loss = 1.242, time/batch = 0.092\n",
      "Sequence 1330400/3263200 (epoch 40), batch 135623, train_loss = 1.502, time/batch = 0.092\n",
      "Sequence 1331390/3263200 (epoch 40), batch 135723, train_loss = 1.581, time/batch = 0.093\n",
      "Sequence 1332380/3263200 (epoch 40), batch 135823, train_loss = 1.294, time/batch = 0.091\n",
      "Sequence 1333370/3263200 (epoch 40), batch 135923, train_loss = 1.683, time/batch = 0.093\n",
      "Sequence 1334340/3263200 (epoch 40), batch 136023, train_loss = 1.471, time/batch = 0.092\n",
      "Sequence 1335290/3263200 (epoch 40), batch 136123, train_loss = 2.078, time/batch = 0.093\n",
      "Sequence 1336270/3263200 (epoch 40), batch 136223, train_loss = 1.509, time/batch = 0.093\n",
      "Sequence 1337240/3263200 (epoch 40), batch 136323, train_loss = 1.308, time/batch = 0.092\n",
      "Epoch 40 completed, average train loss 1.293268, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1338212/3263200 (epoch 41), batch 136423, train_loss = 1.705, time/batch = 0.095\n",
      "Sequence 1339202/3263200 (epoch 41), batch 136523, train_loss = 0.944, time/batch = 0.094\n",
      "Sequence 1340192/3263200 (epoch 41), batch 136623, train_loss = 1.401, time/batch = 0.092\n",
      "Sequence 1341182/3263200 (epoch 41), batch 136723, train_loss = 1.274, time/batch = 0.092\n",
      "Sequence 1342162/3263200 (epoch 41), batch 136823, train_loss = 1.461, time/batch = 0.094\n",
      "Sequence 1343112/3263200 (epoch 41), batch 136923, train_loss = 0.693, time/batch = 0.093\n",
      "Sequence 1344102/3263200 (epoch 41), batch 137023, train_loss = 1.027, time/batch = 0.093\n",
      "Sequence 1345052/3263200 (epoch 41), batch 137123, train_loss = 1.204, time/batch = 0.093\n",
      "Sequence 1346022/3263200 (epoch 41), batch 137223, train_loss = 0.655, time/batch = 0.093\n",
      "Sequence 1347012/3263200 (epoch 41), batch 137323, train_loss = 1.165, time/batch = 0.091\n",
      "Sequence 1347992/3263200 (epoch 41), batch 137423, train_loss = 1.114, time/batch = 0.093\n",
      "Sequence 1348972/3263200 (epoch 41), batch 137523, train_loss = 0.779, time/batch = 0.093\n",
      "Sequence 1349962/3263200 (epoch 41), batch 137623, train_loss = 0.982, time/batch = 0.092\n",
      "Sequence 1350942/3263200 (epoch 41), batch 137723, train_loss = 1.926, time/batch = 0.093\n",
      "Sequence 1351932/3263200 (epoch 41), batch 137823, train_loss = 0.958, time/batch = 0.092\n",
      "Sequence 1352932/3263200 (epoch 41), batch 137923, train_loss = 0.825, time/batch = 0.092\n",
      "Sequence 1353932/3263200 (epoch 41), batch 138023, train_loss = 1.742, time/batch = 0.093\n",
      "Sequence 1354892/3263200 (epoch 41), batch 138123, train_loss = 1.316, time/batch = 0.093\n",
      "Sequence 1355842/3263200 (epoch 41), batch 138223, train_loss = 1.386, time/batch = 0.092\n",
      "Sequence 1356812/3263200 (epoch 41), batch 138323, train_loss = 1.607, time/batch = 0.092\n",
      "Sequence 1357802/3263200 (epoch 41), batch 138423, train_loss = 0.905, time/batch = 0.092\n",
      "Sequence 1358792/3263200 (epoch 41), batch 138523, train_loss = 2.019, time/batch = 0.093\n",
      "Sequence 1359792/3263200 (epoch 41), batch 138623, train_loss = 1.420, time/batch = 0.093\n",
      "Sequence 1360782/3263200 (epoch 41), batch 138723, train_loss = 1.201, time/batch = 0.092\n",
      "Sequence 1361782/3263200 (epoch 41), batch 138823, train_loss = 1.313, time/batch = 0.093\n",
      "Sequence 1362772/3263200 (epoch 41), batch 138923, train_loss = 0.901, time/batch = 0.092\n",
      "Sequence 1363732/3263200 (epoch 41), batch 139023, train_loss = 1.584, time/batch = 0.092\n",
      "Sequence 1364722/3263200 (epoch 41), batch 139123, train_loss = 1.723, time/batch = 0.094\n",
      "Sequence 1365682/3263200 (epoch 41), batch 139223, train_loss = 1.115, time/batch = 0.093\n",
      "Sequence 1366662/3263200 (epoch 41), batch 139323, train_loss = 0.963, time/batch = 0.092\n",
      "Sequence 1367662/3263200 (epoch 41), batch 139423, train_loss = 0.967, time/batch = 0.094\n",
      "Sequence 1368632/3263200 (epoch 41), batch 139523, train_loss = 1.492, time/batch = 0.092\n",
      "Sequence 1369592/3263200 (epoch 41), batch 139623, train_loss = 1.214, time/batch = 0.093\n",
      "Epoch 41 completed, average train loss 1.292107, learning rate 0.0010\n",
      "Sequence 1370584/3263200 (epoch 42), batch 139723, train_loss = 1.973, time/batch = 0.091\n",
      "Shuffling training data...\n",
      "Sequence 1371534/3263200 (epoch 42), batch 139823, train_loss = 1.382, time/batch = 0.089\n",
      "Sequence 1372524/3263200 (epoch 42), batch 139923, train_loss = 1.306, time/batch = 0.091\n",
      "Sequence 1373504/3263200 (epoch 42), batch 140023, train_loss = 1.086, time/batch = 0.093\n",
      "Sequence 1374494/3263200 (epoch 42), batch 140123, train_loss = 1.170, time/batch = 0.094\n",
      "Sequence 1375484/3263200 (epoch 42), batch 140223, train_loss = 1.323, time/batch = 0.093\n",
      "Sequence 1376434/3263200 (epoch 42), batch 140323, train_loss = 1.904, time/batch = 0.093\n",
      "Sequence 1377424/3263200 (epoch 42), batch 140423, train_loss = 1.239, time/batch = 0.093\n",
      "Sequence 1378394/3263200 (epoch 42), batch 140523, train_loss = 1.234, time/batch = 0.093\n",
      "Sequence 1379364/3263200 (epoch 42), batch 140623, train_loss = 1.330, time/batch = 0.092\n",
      "Sequence 1380354/3263200 (epoch 42), batch 140723, train_loss = 1.109, time/batch = 0.094\n",
      "Sequence 1381344/3263200 (epoch 42), batch 140823, train_loss = 1.049, time/batch = 0.093\n",
      "Sequence 1382324/3263200 (epoch 42), batch 140923, train_loss = 1.024, time/batch = 0.093\n",
      "Sequence 1383284/3263200 (epoch 42), batch 141023, train_loss = 1.388, time/batch = 0.093\n",
      "Sequence 1384274/3263200 (epoch 42), batch 141123, train_loss = 1.668, time/batch = 0.093\n",
      "Sequence 1385254/3263200 (epoch 42), batch 141223, train_loss = 0.816, time/batch = 0.092\n",
      "Sequence 1386234/3263200 (epoch 42), batch 141323, train_loss = 1.359, time/batch = 0.092\n",
      "Sequence 1387204/3263200 (epoch 42), batch 141423, train_loss = 1.350, time/batch = 0.091\n",
      "Sequence 1388194/3263200 (epoch 42), batch 141523, train_loss = 0.882, time/batch = 0.092\n",
      "Sequence 1389144/3263200 (epoch 42), batch 141623, train_loss = 2.419, time/batch = 0.094\n",
      "Sequence 1390124/3263200 (epoch 42), batch 141723, train_loss = 1.208, time/batch = 0.093\n",
      "Sequence 1391114/3263200 (epoch 42), batch 141823, train_loss = 1.164, time/batch = 0.092\n",
      "Sequence 1392114/3263200 (epoch 42), batch 141923, train_loss = 1.120, time/batch = 0.092\n",
      "Sequence 1393114/3263200 (epoch 42), batch 142023, train_loss = 1.392, time/batch = 0.092\n",
      "Sequence 1394094/3263200 (epoch 42), batch 142123, train_loss = 0.871, time/batch = 0.093\n",
      "Sequence 1395084/3263200 (epoch 42), batch 142223, train_loss = 0.783, time/batch = 0.092\n",
      "Sequence 1396064/3263200 (epoch 42), batch 142323, train_loss = 1.605, time/batch = 0.093\n",
      "Sequence 1397034/3263200 (epoch 42), batch 142423, train_loss = 1.004, time/batch = 0.092\n",
      "Sequence 1398034/3263200 (epoch 42), batch 142523, train_loss = 0.867, time/batch = 0.092\n",
      "Sequence 1399024/3263200 (epoch 42), batch 142623, train_loss = 1.254, time/batch = 0.093\n",
      "Sequence 1400024/3263200 (epoch 42), batch 142723, train_loss = 1.281, time/batch = 0.092\n",
      "Sequence 1401004/3263200 (epoch 42), batch 142823, train_loss = 1.554, time/batch = 0.093\n",
      "Sequence 1401984/3263200 (epoch 42), batch 142923, train_loss = 1.554, time/batch = 0.093\n",
      "Sequence 1402954/3263200 (epoch 42), batch 143023, train_loss = 0.474, time/batch = 0.092\n",
      "Epoch 42 completed, average train loss 1.288271, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1403936/3263200 (epoch 43), batch 143123, train_loss = 0.837, time/batch = 0.093\n",
      "Sequence 1404916/3263200 (epoch 43), batch 143223, train_loss = 1.232, time/batch = 0.093\n",
      "Sequence 1405896/3263200 (epoch 43), batch 143323, train_loss = 0.726, time/batch = 0.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1406876/3263200 (epoch 43), batch 143423, train_loss = 1.547, time/batch = 0.092\n",
      "Sequence 1407826/3263200 (epoch 43), batch 143523, train_loss = 0.950, time/batch = 0.094\n",
      "Sequence 1408816/3263200 (epoch 43), batch 143623, train_loss = 1.246, time/batch = 0.093\n",
      "Sequence 1409786/3263200 (epoch 43), batch 143723, train_loss = 1.435, time/batch = 0.092\n",
      "Sequence 1410726/3263200 (epoch 43), batch 143823, train_loss = 1.418, time/batch = 0.093\n",
      "Sequence 1411706/3263200 (epoch 43), batch 143924, train_loss = 1.342, time/batch = 0.092\n",
      "Sequence 1412686/3263200 (epoch 43), batch 144024, train_loss = 0.926, time/batch = 0.092\n",
      "Sequence 1413666/3263200 (epoch 43), batch 144124, train_loss = 1.476, time/batch = 0.092\n",
      "Sequence 1414636/3263200 (epoch 43), batch 144224, train_loss = 1.985, time/batch = 0.092\n",
      "Sequence 1415636/3263200 (epoch 43), batch 144324, train_loss = 0.909, time/batch = 0.092\n",
      "Sequence 1416626/3263200 (epoch 43), batch 144424, train_loss = 1.109, time/batch = 0.092\n",
      "Sequence 1417606/3263200 (epoch 43), batch 144524, train_loss = 0.825, time/batch = 0.092\n",
      "Sequence 1418586/3263200 (epoch 43), batch 144624, train_loss = 1.315, time/batch = 0.093\n",
      "Sequence 1419576/3263200 (epoch 43), batch 144724, train_loss = 1.194, time/batch = 0.093\n",
      "Sequence 1420556/3263200 (epoch 43), batch 144824, train_loss = 1.643, time/batch = 0.091\n",
      "Sequence 1421536/3263200 (epoch 43), batch 144924, train_loss = 1.636, time/batch = 0.092\n",
      "Sequence 1422506/3263200 (epoch 43), batch 145024, train_loss = 1.377, time/batch = 0.092\n",
      "Sequence 1423496/3263200 (epoch 43), batch 145124, train_loss = 1.397, time/batch = 0.092\n",
      "Sequence 1424486/3263200 (epoch 43), batch 145224, train_loss = 1.158, time/batch = 0.092\n",
      "Sequence 1425456/3263200 (epoch 43), batch 145324, train_loss = 1.215, time/batch = 0.093\n",
      "Sequence 1426446/3263200 (epoch 43), batch 145424, train_loss = 1.745, time/batch = 0.092\n",
      "Sequence 1427446/3263200 (epoch 43), batch 145524, train_loss = 2.433, time/batch = 0.092\n",
      "Sequence 1428426/3263200 (epoch 43), batch 145624, train_loss = 1.863, time/batch = 0.092\n",
      "Sequence 1429426/3263200 (epoch 43), batch 145724, train_loss = 0.963, time/batch = 0.092\n",
      "Sequence 1430426/3263200 (epoch 43), batch 145824, train_loss = 1.249, time/batch = 0.090\n",
      "Sequence 1431406/3263200 (epoch 43), batch 145924, train_loss = 1.690, time/batch = 0.094\n",
      "Sequence 1432386/3263200 (epoch 43), batch 146024, train_loss = 1.133, time/batch = 0.094\n",
      "Sequence 1433376/3263200 (epoch 43), batch 146124, train_loss = 1.127, time/batch = 0.092\n",
      "Sequence 1434336/3263200 (epoch 43), batch 146224, train_loss = 1.308, time/batch = 0.092\n",
      "Sequence 1435326/3263200 (epoch 43), batch 146324, train_loss = 1.958, time/batch = 0.094\n",
      "Epoch 43 completed, average train loss 1.281395, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1436308/3263200 (epoch 44), batch 146424, train_loss = 1.090, time/batch = 0.092\n",
      "Sequence 1437278/3263200 (epoch 44), batch 146524, train_loss = 1.197, time/batch = 0.093\n",
      "Sequence 1438268/3263200 (epoch 44), batch 146624, train_loss = 1.716, time/batch = 0.093\n",
      "Sequence 1439238/3263200 (epoch 44), batch 146724, train_loss = 0.697, time/batch = 0.093\n",
      "Sequence 1440218/3263200 (epoch 44), batch 146824, train_loss = 1.421, time/batch = 0.093\n",
      "Sequence 1441198/3263200 (epoch 44), batch 146924, train_loss = 1.764, time/batch = 0.093\n",
      "Sequence 1442188/3263200 (epoch 44), batch 147024, train_loss = 0.961, time/batch = 0.092\n",
      "Sequence 1443148/3263200 (epoch 44), batch 147124, train_loss = 1.618, time/batch = 0.094\n",
      "Sequence 1444138/3263200 (epoch 44), batch 147225, train_loss = 0.690, time/batch = 0.093\n",
      "Sequence 1445128/3263200 (epoch 44), batch 147325, train_loss = 1.435, time/batch = 0.092\n",
      "Sequence 1446128/3263200 (epoch 44), batch 147425, train_loss = 1.010, time/batch = 0.095\n",
      "Sequence 1447108/3263200 (epoch 44), batch 147525, train_loss = 1.154, time/batch = 0.092\n",
      "Sequence 1448098/3263200 (epoch 44), batch 147625, train_loss = 1.165, time/batch = 0.093\n",
      "Sequence 1449078/3263200 (epoch 44), batch 147725, train_loss = 0.943, time/batch = 0.093\n",
      "Sequence 1450078/3263200 (epoch 44), batch 147825, train_loss = 1.062, time/batch = 0.092\n",
      "Sequence 1451068/3263200 (epoch 44), batch 147925, train_loss = 1.524, time/batch = 0.092\n",
      "Sequence 1452058/3263200 (epoch 44), batch 148025, train_loss = 1.625, time/batch = 0.093\n",
      "Sequence 1453058/3263200 (epoch 44), batch 148125, train_loss = 1.381, time/batch = 0.092\n",
      "Sequence 1454018/3263200 (epoch 44), batch 148225, train_loss = 1.334, time/batch = 0.093\n",
      "Sequence 1455008/3263200 (epoch 44), batch 148325, train_loss = 1.366, time/batch = 0.093\n",
      "Sequence 1455988/3263200 (epoch 44), batch 148425, train_loss = 1.543, time/batch = 0.093\n",
      "Sequence 1456968/3263200 (epoch 44), batch 148525, train_loss = 1.078, time/batch = 0.092\n",
      "Sequence 1457948/3263200 (epoch 44), batch 148625, train_loss = 1.235, time/batch = 0.092\n",
      "Sequence 1458908/3263200 (epoch 44), batch 148725, train_loss = 1.266, time/batch = 0.092\n",
      "Sequence 1459858/3263200 (epoch 44), batch 148825, train_loss = 1.316, time/batch = 0.092\n",
      "Sequence 1460848/3263200 (epoch 44), batch 148925, train_loss = 1.138, time/batch = 0.091\n",
      "Sequence 1461838/3263200 (epoch 44), batch 149025, train_loss = 1.174, time/batch = 0.093\n",
      "Sequence 1462818/3263200 (epoch 44), batch 149125, train_loss = 1.721, time/batch = 0.093\n",
      "Sequence 1463818/3263200 (epoch 44), batch 149225, train_loss = 1.112, time/batch = 0.093\n",
      "Sequence 1464778/3263200 (epoch 44), batch 149325, train_loss = 2.116, time/batch = 0.092\n",
      "Sequence 1465758/3263200 (epoch 44), batch 149425, train_loss = 1.088, time/batch = 0.092\n",
      "Sequence 1466738/3263200 (epoch 44), batch 149525, train_loss = 1.422, time/batch = 0.093\n",
      "Sequence 1467728/3263200 (epoch 44), batch 149625, train_loss = 1.559, time/batch = 0.092\n",
      "Epoch 44 completed, average train loss 1.274710, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 1468700/3263200 (epoch 45), batch 149725, train_loss = 0.980, time/batch = 0.095\n",
      "Sequence 1469680/3263200 (epoch 45), batch 149825, train_loss = 1.103, time/batch = 0.093\n",
      "Sequence 1470640/3263200 (epoch 45), batch 149925, train_loss = 1.308, time/batch = 0.094\n",
      "Sequence 1471640/3263200 (epoch 45), batch 150025, train_loss = 1.575, time/batch = 0.092\n",
      "Sequence 1472620/3263200 (epoch 45), batch 150125, train_loss = 1.134, time/batch = 0.092\n",
      "Sequence 1473610/3263200 (epoch 45), batch 150225, train_loss = 0.857, time/batch = 0.094\n",
      "Sequence 1474590/3263200 (epoch 45), batch 150325, train_loss = 1.401, time/batch = 0.092\n",
      "Sequence 1475580/3263200 (epoch 45), batch 150425, train_loss = 1.300, time/batch = 0.093\n",
      "Sequence 1476560/3263200 (epoch 45), batch 150525, train_loss = 1.025, time/batch = 0.092\n",
      "Sequence 1477540/3263200 (epoch 45), batch 150625, train_loss = 1.672, time/batch = 0.092\n",
      "Sequence 1478500/3263200 (epoch 45), batch 150725, train_loss = 1.057, time/batch = 0.091\n",
      "Sequence 1479480/3263200 (epoch 45), batch 150825, train_loss = 1.401, time/batch = 0.093\n",
      "Sequence 1480440/3263200 (epoch 45), batch 150925, train_loss = 1.088, time/batch = 0.093\n",
      "Sequence 1481440/3263200 (epoch 45), batch 151025, train_loss = 2.094, time/batch = 0.093\n",
      "Sequence 1482440/3263200 (epoch 45), batch 151125, train_loss = 1.100, time/batch = 0.093\n",
      "Sequence 1483430/3263200 (epoch 45), batch 151225, train_loss = 1.421, time/batch = 0.093\n",
      "Sequence 1484420/3263200 (epoch 45), batch 151325, train_loss = 1.164, time/batch = 0.093\n",
      "Sequence 1485390/3263200 (epoch 45), batch 151425, train_loss = 1.195, time/batch = 0.092\n",
      "Sequence 1486380/3263200 (epoch 45), batch 151525, train_loss = 1.381, time/batch = 0.093\n",
      "Sequence 1487360/3263200 (epoch 45), batch 151625, train_loss = 1.554, time/batch = 0.092\n",
      "Sequence 1488320/3263200 (epoch 45), batch 151726, train_loss = 0.602, time/batch = 0.093\n",
      "Sequence 1489310/3263200 (epoch 45), batch 151826, train_loss = 1.474, time/batch = 0.093\n",
      "Sequence 1490290/3263200 (epoch 45), batch 151926, train_loss = 1.244, time/batch = 0.094\n",
      "Sequence 1491270/3263200 (epoch 45), batch 152026, train_loss = 0.667, time/batch = 0.093\n",
      "Sequence 1492240/3263200 (epoch 45), batch 152126, train_loss = 1.038, time/batch = 0.092\n",
      "Sequence 1493210/3263200 (epoch 45), batch 152226, train_loss = 1.448, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1494200/3263200 (epoch 45), batch 152326, train_loss = 1.057, time/batch = 0.092\n",
      "Sequence 1495190/3263200 (epoch 45), batch 152426, train_loss = 1.380, time/batch = 0.092\n",
      "Sequence 1496170/3263200 (epoch 45), batch 152526, train_loss = 1.637, time/batch = 0.092\n",
      "Sequence 1497130/3263200 (epoch 45), batch 152626, train_loss = 1.519, time/batch = 0.093\n",
      "Sequence 1498110/3263200 (epoch 45), batch 152726, train_loss = 0.645, time/batch = 0.092\n",
      "Sequence 1499100/3263200 (epoch 45), batch 152826, train_loss = 1.555, time/batch = 0.093\n",
      "Sequence 1500090/3263200 (epoch 45), batch 152926, train_loss = 0.996, time/batch = 0.093\n",
      "Epoch 45 completed, average train loss 1.273815, learning rate 0.0010\n",
      "Sequence 1501072/3263200 (epoch 46), batch 153026, train_loss = 0.692, time/batch = 0.092\n",
      "Shuffling training data...\n",
      "Sequence 1502062/3263200 (epoch 46), batch 153126, train_loss = 1.961, time/batch = 0.093\n",
      "Sequence 1503062/3263200 (epoch 46), batch 153226, train_loss = 1.073, time/batch = 0.092\n",
      "Sequence 1504052/3263200 (epoch 46), batch 153326, train_loss = 1.091, time/batch = 0.092\n",
      "Sequence 1505042/3263200 (epoch 46), batch 153426, train_loss = 1.260, time/batch = 0.093\n",
      "Sequence 1506032/3263200 (epoch 46), batch 153526, train_loss = 1.768, time/batch = 0.092\n",
      "Sequence 1507002/3263200 (epoch 46), batch 153626, train_loss = 2.133, time/batch = 0.092\n",
      "Sequence 1508002/3263200 (epoch 46), batch 153726, train_loss = 0.752, time/batch = 0.092\n",
      "Sequence 1508992/3263200 (epoch 46), batch 153826, train_loss = 0.796, time/batch = 0.092\n",
      "Sequence 1509972/3263200 (epoch 46), batch 153926, train_loss = 1.214, time/batch = 0.093\n",
      "Sequence 1510932/3263200 (epoch 46), batch 154026, train_loss = 0.738, time/batch = 0.090\n",
      "Sequence 1511902/3263200 (epoch 46), batch 154126, train_loss = 0.879, time/batch = 0.092\n",
      "Sequence 1512892/3263200 (epoch 46), batch 154226, train_loss = 0.851, time/batch = 0.093\n",
      "Sequence 1513882/3263200 (epoch 46), batch 154326, train_loss = 1.276, time/batch = 0.092\n",
      "Sequence 1514872/3263200 (epoch 46), batch 154426, train_loss = 0.621, time/batch = 0.093\n",
      "Sequence 1515852/3263200 (epoch 46), batch 154526, train_loss = 0.926, time/batch = 0.092\n",
      "Sequence 1516812/3263200 (epoch 46), batch 154626, train_loss = 1.134, time/batch = 0.094\n",
      "Sequence 1517792/3263200 (epoch 46), batch 154726, train_loss = 1.827, time/batch = 0.093\n",
      "Sequence 1518782/3263200 (epoch 46), batch 154826, train_loss = 1.215, time/batch = 0.093\n",
      "Sequence 1519752/3263200 (epoch 46), batch 154926, train_loss = 1.518, time/batch = 0.092\n",
      "Sequence 1520732/3263200 (epoch 46), batch 155026, train_loss = 1.665, time/batch = 0.092\n",
      "Sequence 1521732/3263200 (epoch 46), batch 155126, train_loss = 1.185, time/batch = 0.094\n",
      "Sequence 1522682/3263200 (epoch 46), batch 155226, train_loss = 1.192, time/batch = 0.093\n",
      "Sequence 1523672/3263200 (epoch 46), batch 155326, train_loss = 1.321, time/batch = 0.093\n",
      "Sequence 1524672/3263200 (epoch 46), batch 155426, train_loss = 1.114, time/batch = 0.092\n",
      "Sequence 1525632/3263200 (epoch 46), batch 155526, train_loss = 1.183, time/batch = 0.092\n",
      "Sequence 1526612/3263200 (epoch 46), batch 155626, train_loss = 1.454, time/batch = 0.093\n",
      "Sequence 1527572/3263200 (epoch 46), batch 155726, train_loss = 0.848, time/batch = 0.093\n",
      "Sequence 1528532/3263200 (epoch 46), batch 155826, train_loss = 0.900, time/batch = 0.092\n",
      "Sequence 1529512/3263200 (epoch 46), batch 155926, train_loss = 1.895, time/batch = 0.093\n",
      "Sequence 1530482/3263200 (epoch 46), batch 156026, train_loss = 1.727, time/batch = 0.092\n",
      "Sequence 1531472/3263200 (epoch 46), batch 156126, train_loss = 1.011, time/batch = 0.094\n",
      "Sequence 1532442/3263200 (epoch 46), batch 156226, train_loss = 1.414, time/batch = 0.093\n",
      "Sequence 1533432/3263200 (epoch 46), batch 156326, train_loss = 1.738, time/batch = 0.093\n",
      "Epoch 46 completed, average train loss 1.266748, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1534434/3263200 (epoch 47), batch 156426, train_loss = 1.348, time/batch = 0.095\n",
      "Sequence 1535424/3263200 (epoch 47), batch 156526, train_loss = 1.311, time/batch = 0.092\n",
      "Sequence 1536414/3263200 (epoch 47), batch 156626, train_loss = 1.277, time/batch = 0.092\n",
      "Sequence 1537374/3263200 (epoch 47), batch 156726, train_loss = 1.015, time/batch = 0.093\n",
      "Sequence 1538364/3263200 (epoch 47), batch 156826, train_loss = 1.472, time/batch = 0.092\n",
      "Sequence 1539364/3263200 (epoch 47), batch 156926, train_loss = 1.205, time/batch = 0.092\n",
      "Sequence 1540354/3263200 (epoch 47), batch 157026, train_loss = 1.557, time/batch = 0.094\n",
      "Sequence 1541344/3263200 (epoch 47), batch 157126, train_loss = 2.068, time/batch = 0.092\n",
      "Sequence 1542334/3263200 (epoch 47), batch 157226, train_loss = 1.377, time/batch = 0.093\n",
      "Sequence 1543334/3263200 (epoch 47), batch 157326, train_loss = 0.807, time/batch = 0.091\n",
      "Sequence 1544284/3263200 (epoch 47), batch 157426, train_loss = 1.014, time/batch = 0.093\n",
      "Sequence 1545264/3263200 (epoch 47), batch 157526, train_loss = 1.132, time/batch = 0.094\n",
      "Sequence 1546234/3263200 (epoch 47), batch 157626, train_loss = 1.346, time/batch = 0.092\n",
      "Sequence 1547224/3263200 (epoch 47), batch 157726, train_loss = 1.197, time/batch = 0.092\n",
      "Sequence 1548204/3263200 (epoch 47), batch 157826, train_loss = 1.459, time/batch = 0.092\n",
      "Sequence 1549194/3263200 (epoch 47), batch 157926, train_loss = 1.454, time/batch = 0.092\n",
      "Sequence 1550184/3263200 (epoch 47), batch 158026, train_loss = 0.881, time/batch = 0.093\n",
      "Sequence 1551164/3263200 (epoch 47), batch 158126, train_loss = 1.145, time/batch = 0.091\n",
      "Sequence 1552154/3263200 (epoch 47), batch 158226, train_loss = 1.201, time/batch = 0.094\n",
      "Sequence 1553154/3263200 (epoch 47), batch 158326, train_loss = 0.986, time/batch = 0.093\n",
      "Sequence 1554144/3263200 (epoch 47), batch 158426, train_loss = 1.063, time/batch = 0.093\n",
      "Sequence 1555134/3263200 (epoch 47), batch 158526, train_loss = 1.066, time/batch = 0.092\n",
      "Sequence 1556114/3263200 (epoch 47), batch 158626, train_loss = 1.811, time/batch = 0.093\n",
      "Sequence 1557054/3263200 (epoch 47), batch 158726, train_loss = 1.211, time/batch = 0.092\n",
      "Sequence 1558034/3263200 (epoch 47), batch 158826, train_loss = 1.905, time/batch = 0.093\n",
      "Sequence 1559024/3263200 (epoch 47), batch 158926, train_loss = 1.381, time/batch = 0.093\n",
      "Sequence 1559994/3263200 (epoch 47), batch 159026, train_loss = 1.048, time/batch = 0.092\n",
      "Sequence 1560974/3263200 (epoch 47), batch 159126, train_loss = 1.033, time/batch = 0.093\n",
      "Sequence 1561944/3263200 (epoch 47), batch 159226, train_loss = 0.856, time/batch = 0.093\n",
      "Sequence 1562904/3263200 (epoch 47), batch 159327, train_loss = 1.338, time/batch = 0.093\n",
      "Sequence 1563904/3263200 (epoch 47), batch 159427, train_loss = 1.853, time/batch = 0.094\n",
      "Sequence 1564834/3263200 (epoch 47), batch 159527, train_loss = 1.511, time/batch = 0.093\n",
      "Sequence 1565814/3263200 (epoch 47), batch 159627, train_loss = 1.587, time/batch = 0.092\n",
      "Epoch 47 completed, average train loss 1.265764, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1566796/3263200 (epoch 48), batch 159727, train_loss = 1.734, time/batch = 0.093\n",
      "Sequence 1567776/3263200 (epoch 48), batch 159827, train_loss = 1.559, time/batch = 0.096\n",
      "Sequence 1568766/3263200 (epoch 48), batch 159928, train_loss = 0.912, time/batch = 0.103\n",
      "Sequence 1569726/3263200 (epoch 48), batch 160028, train_loss = 1.804, time/batch = 0.102\n",
      "Sequence 1570716/3263200 (epoch 48), batch 160128, train_loss = 1.101, time/batch = 0.101\n",
      "Sequence 1571686/3263200 (epoch 48), batch 160228, train_loss = 1.334, time/batch = 0.099\n",
      "Sequence 1572676/3263200 (epoch 48), batch 160328, train_loss = 1.161, time/batch = 0.093\n",
      "Sequence 1573646/3263200 (epoch 48), batch 160428, train_loss = 0.979, time/batch = 0.093\n",
      "Sequence 1574626/3263200 (epoch 48), batch 160528, train_loss = 1.544, time/batch = 0.093\n",
      "Sequence 1575596/3263200 (epoch 48), batch 160628, train_loss = 0.989, time/batch = 0.093\n",
      "Sequence 1576596/3263200 (epoch 48), batch 160728, train_loss = 1.283, time/batch = 0.093\n",
      "Sequence 1577586/3263200 (epoch 48), batch 160828, train_loss = 1.581, time/batch = 0.093\n",
      "Sequence 1578566/3263200 (epoch 48), batch 160928, train_loss = 1.822, time/batch = 0.093\n",
      "Sequence 1579546/3263200 (epoch 48), batch 161028, train_loss = 1.754, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1580536/3263200 (epoch 48), batch 161128, train_loss = 0.897, time/batch = 0.092\n",
      "Sequence 1581516/3263200 (epoch 48), batch 161228, train_loss = 0.987, time/batch = 0.092\n",
      "Sequence 1582506/3263200 (epoch 48), batch 161328, train_loss = 1.167, time/batch = 0.092\n",
      "Sequence 1583496/3263200 (epoch 48), batch 161428, train_loss = 1.039, time/batch = 0.091\n",
      "Sequence 1584486/3263200 (epoch 48), batch 161528, train_loss = 1.235, time/batch = 0.092\n",
      "Sequence 1585456/3263200 (epoch 48), batch 161628, train_loss = 0.836, time/batch = 0.092\n",
      "Sequence 1586416/3263200 (epoch 48), batch 161728, train_loss = 1.240, time/batch = 0.092\n",
      "Sequence 1587396/3263200 (epoch 48), batch 161828, train_loss = 1.155, time/batch = 0.093\n",
      "Sequence 1588376/3263200 (epoch 48), batch 161928, train_loss = 1.400, time/batch = 0.093\n",
      "Sequence 1589346/3263200 (epoch 48), batch 162028, train_loss = 1.513, time/batch = 0.093\n",
      "Sequence 1590346/3263200 (epoch 48), batch 162128, train_loss = 1.473, time/batch = 0.093\n",
      "Sequence 1591336/3263200 (epoch 48), batch 162228, train_loss = 1.302, time/batch = 0.093\n",
      "Sequence 1592326/3263200 (epoch 48), batch 162328, train_loss = 1.660, time/batch = 0.093\n",
      "Sequence 1593326/3263200 (epoch 48), batch 162428, train_loss = 1.166, time/batch = 0.092\n",
      "Sequence 1594306/3263200 (epoch 48), batch 162528, train_loss = 1.422, time/batch = 0.093\n",
      "Sequence 1595286/3263200 (epoch 48), batch 162628, train_loss = 0.940, time/batch = 0.092\n",
      "Sequence 1596256/3263200 (epoch 48), batch 162729, train_loss = 0.637, time/batch = 0.093\n",
      "Sequence 1597236/3263200 (epoch 48), batch 162829, train_loss = 0.968, time/batch = 0.093\n",
      "Sequence 1598206/3263200 (epoch 48), batch 162929, train_loss = 1.671, time/batch = 0.092\n",
      "Epoch 48 completed, average train loss 1.261238, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1599188/3263200 (epoch 49), batch 163029, train_loss = 1.837, time/batch = 0.093\n",
      "Sequence 1600168/3263200 (epoch 49), batch 163129, train_loss = 1.319, time/batch = 0.094\n",
      "Sequence 1601148/3263200 (epoch 49), batch 163229, train_loss = 2.045, time/batch = 0.092\n",
      "Sequence 1602138/3263200 (epoch 49), batch 163329, train_loss = 0.992, time/batch = 0.092\n",
      "Sequence 1603118/3263200 (epoch 49), batch 163429, train_loss = 1.013, time/batch = 0.093\n",
      "Sequence 1604118/3263200 (epoch 49), batch 163529, train_loss = 0.956, time/batch = 0.093\n",
      "Sequence 1605098/3263200 (epoch 49), batch 163629, train_loss = 1.301, time/batch = 0.093\n",
      "Sequence 1606088/3263200 (epoch 49), batch 163729, train_loss = 1.015, time/batch = 0.093\n",
      "Sequence 1607068/3263200 (epoch 49), batch 163829, train_loss = 0.807, time/batch = 0.093\n",
      "Sequence 1608068/3263200 (epoch 49), batch 163929, train_loss = 1.349, time/batch = 0.093\n",
      "Sequence 1609028/3263200 (epoch 49), batch 164029, train_loss = 0.827, time/batch = 0.093\n",
      "Sequence 1610008/3263200 (epoch 49), batch 164129, train_loss = 1.308, time/batch = 0.093\n",
      "Sequence 1610988/3263200 (epoch 49), batch 164229, train_loss = 1.748, time/batch = 0.092\n",
      "Sequence 1611958/3263200 (epoch 49), batch 164329, train_loss = 1.428, time/batch = 0.092\n",
      "Sequence 1612948/3263200 (epoch 49), batch 164429, train_loss = 1.308, time/batch = 0.092\n",
      "Sequence 1613908/3263200 (epoch 49), batch 164529, train_loss = 1.367, time/batch = 0.092\n",
      "Sequence 1614888/3263200 (epoch 49), batch 164629, train_loss = 1.406, time/batch = 0.093\n",
      "Sequence 1615868/3263200 (epoch 49), batch 164729, train_loss = 1.175, time/batch = 0.092\n",
      "Sequence 1616858/3263200 (epoch 49), batch 164829, train_loss = 1.451, time/batch = 0.093\n",
      "Sequence 1617838/3263200 (epoch 49), batch 164929, train_loss = 1.436, time/batch = 0.093\n",
      "Sequence 1618818/3263200 (epoch 49), batch 165029, train_loss = 1.174, time/batch = 0.093\n",
      "Sequence 1619788/3263200 (epoch 49), batch 165129, train_loss = 1.108, time/batch = 0.092\n",
      "Sequence 1620778/3263200 (epoch 49), batch 165229, train_loss = 0.930, time/batch = 0.092\n",
      "Sequence 1621758/3263200 (epoch 49), batch 165329, train_loss = 1.728, time/batch = 0.092\n",
      "Sequence 1622738/3263200 (epoch 49), batch 165429, train_loss = 0.910, time/batch = 0.093\n",
      "Sequence 1623698/3263200 (epoch 49), batch 165529, train_loss = 1.144, time/batch = 0.091\n",
      "Sequence 1624678/3263200 (epoch 49), batch 165629, train_loss = 0.977, time/batch = 0.093\n",
      "Sequence 1625668/3263200 (epoch 49), batch 165729, train_loss = 1.102, time/batch = 0.092\n",
      "Sequence 1626658/3263200 (epoch 49), batch 165830, train_loss = 0.675, time/batch = 0.092\n",
      "Sequence 1627648/3263200 (epoch 49), batch 165930, train_loss = 0.866, time/batch = 0.093\n",
      "Sequence 1628618/3263200 (epoch 49), batch 166030, train_loss = 1.593, time/batch = 0.091\n",
      "Sequence 1629608/3263200 (epoch 49), batch 166130, train_loss = 0.456, time/batch = 0.092\n",
      "Sequence 1630598/3263200 (epoch 49), batch 166230, train_loss = 0.844, time/batch = 0.092\n",
      "Sequence 1631558/3263200 (epoch 49), batch 166330, train_loss = 1.221, time/batch = 0.092\n",
      "Epoch 49 completed, average train loss 1.255589, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 1632540/3263200 (epoch 50), batch 166430, train_loss = 1.016, time/batch = 0.094\n",
      "Sequence 1633490/3263200 (epoch 50), batch 166530, train_loss = 1.398, time/batch = 0.093\n",
      "Sequence 1634490/3263200 (epoch 50), batch 166630, train_loss = 1.124, time/batch = 0.094\n",
      "Sequence 1635460/3263200 (epoch 50), batch 166730, train_loss = 1.194, time/batch = 0.092\n",
      "Sequence 1636460/3263200 (epoch 50), batch 166830, train_loss = 1.183, time/batch = 0.093\n",
      "Sequence 1637460/3263200 (epoch 50), batch 166930, train_loss = 1.299, time/batch = 0.093\n",
      "Sequence 1638450/3263200 (epoch 50), batch 167030, train_loss = 1.141, time/batch = 0.093\n",
      "Sequence 1639420/3263200 (epoch 50), batch 167130, train_loss = 1.409, time/batch = 0.092\n",
      "Sequence 1640410/3263200 (epoch 50), batch 167230, train_loss = 1.013, time/batch = 0.092\n",
      "Sequence 1641380/3263200 (epoch 50), batch 167330, train_loss = 0.955, time/batch = 0.093\n",
      "Sequence 1642360/3263200 (epoch 50), batch 167431, train_loss = 0.716, time/batch = 0.094\n",
      "Sequence 1643350/3263200 (epoch 50), batch 167531, train_loss = 1.327, time/batch = 0.093\n",
      "Sequence 1644340/3263200 (epoch 50), batch 167631, train_loss = 0.861, time/batch = 0.092\n",
      "Sequence 1645330/3263200 (epoch 50), batch 167731, train_loss = 1.040, time/batch = 0.093\n",
      "Sequence 1646290/3263200 (epoch 50), batch 167831, train_loss = 0.790, time/batch = 0.093\n",
      "Sequence 1647280/3263200 (epoch 50), batch 167931, train_loss = 1.004, time/batch = 0.094\n",
      "Sequence 1648270/3263200 (epoch 50), batch 168031, train_loss = 1.152, time/batch = 0.092\n",
      "Sequence 1649270/3263200 (epoch 50), batch 168131, train_loss = 1.043, time/batch = 0.092\n",
      "Sequence 1650260/3263200 (epoch 50), batch 168231, train_loss = 1.248, time/batch = 0.091\n",
      "Sequence 1651260/3263200 (epoch 50), batch 168331, train_loss = 1.217, time/batch = 0.092\n",
      "Sequence 1652240/3263200 (epoch 50), batch 168431, train_loss = 0.808, time/batch = 0.093\n",
      "Sequence 1653190/3263200 (epoch 50), batch 168531, train_loss = 1.097, time/batch = 0.094\n",
      "Sequence 1654170/3263200 (epoch 50), batch 168631, train_loss = 1.040, time/batch = 0.092\n",
      "Sequence 1655150/3263200 (epoch 50), batch 168731, train_loss = 1.220, time/batch = 0.093\n",
      "Sequence 1656110/3263200 (epoch 50), batch 168831, train_loss = 1.269, time/batch = 0.092\n",
      "Sequence 1657080/3263200 (epoch 50), batch 168931, train_loss = 1.185, time/batch = 0.091\n",
      "Sequence 1658060/3263200 (epoch 50), batch 169031, train_loss = 1.077, time/batch = 0.095\n",
      "Sequence 1659030/3263200 (epoch 50), batch 169131, train_loss = 1.179, time/batch = 0.091\n",
      "Sequence 1660020/3263200 (epoch 50), batch 169231, train_loss = 0.970, time/batch = 0.102\n",
      "Sequence 1661000/3263200 (epoch 50), batch 169331, train_loss = 1.122, time/batch = 0.101\n",
      "Sequence 1661980/3263200 (epoch 50), batch 169431, train_loss = 0.842, time/batch = 0.102\n",
      "Sequence 1662940/3263200 (epoch 50), batch 169531, train_loss = 1.079, time/batch = 0.094\n",
      "Sequence 1663940/3263200 (epoch 50), batch 169631, train_loss = 1.972, time/batch = 0.093\n",
      "Epoch 50 completed, average train loss 1.253480, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1664922/3263200 (epoch 51), batch 169731, train_loss = 1.121, time/batch = 0.093\n",
      "Sequence 1665922/3263200 (epoch 51), batch 169831, train_loss = 1.551, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1666882/3263200 (epoch 51), batch 169931, train_loss = 1.424, time/batch = 0.093\n",
      "Sequence 1667862/3263200 (epoch 51), batch 170031, train_loss = 1.042, time/batch = 0.092\n",
      "Sequence 1668862/3263200 (epoch 51), batch 170131, train_loss = 2.384, time/batch = 0.093\n",
      "Sequence 1669852/3263200 (epoch 51), batch 170231, train_loss = 0.814, time/batch = 0.093\n",
      "Sequence 1670832/3263200 (epoch 51), batch 170331, train_loss = 1.184, time/batch = 0.091\n",
      "Sequence 1671782/3263200 (epoch 51), batch 170431, train_loss = 1.482, time/batch = 0.092\n",
      "Sequence 1672762/3263200 (epoch 51), batch 170531, train_loss = 1.264, time/batch = 0.095\n",
      "Sequence 1673712/3263200 (epoch 51), batch 170631, train_loss = 1.662, time/batch = 0.095\n",
      "Sequence 1674692/3263200 (epoch 51), batch 170731, train_loss = 0.914, time/batch = 0.093\n",
      "Sequence 1675672/3263200 (epoch 51), batch 170831, train_loss = 1.027, time/batch = 0.093\n",
      "Sequence 1676662/3263200 (epoch 51), batch 170931, train_loss = 1.518, time/batch = 0.092\n",
      "Sequence 1677652/3263200 (epoch 51), batch 171031, train_loss = 1.286, time/batch = 0.092\n",
      "Sequence 1678632/3263200 (epoch 51), batch 171131, train_loss = 1.422, time/batch = 0.091\n",
      "Sequence 1679622/3263200 (epoch 51), batch 171231, train_loss = 0.865, time/batch = 0.094\n",
      "Sequence 1680602/3263200 (epoch 51), batch 171331, train_loss = 1.531, time/batch = 0.093\n",
      "Sequence 1681602/3263200 (epoch 51), batch 171431, train_loss = 0.989, time/batch = 0.092\n",
      "Sequence 1682592/3263200 (epoch 51), batch 171531, train_loss = 1.181, time/batch = 0.093\n",
      "Sequence 1683582/3263200 (epoch 51), batch 171631, train_loss = 2.389, time/batch = 0.093\n",
      "Sequence 1684552/3263200 (epoch 51), batch 171731, train_loss = 1.000, time/batch = 0.092\n",
      "Sequence 1685532/3263200 (epoch 51), batch 171831, train_loss = 1.876, time/batch = 0.092\n",
      "Sequence 1686522/3263200 (epoch 51), batch 171931, train_loss = 0.864, time/batch = 0.091\n",
      "Sequence 1687522/3263200 (epoch 51), batch 172031, train_loss = 1.179, time/batch = 0.094\n",
      "Sequence 1688482/3263200 (epoch 51), batch 172131, train_loss = 1.291, time/batch = 0.092\n",
      "Sequence 1689482/3263200 (epoch 51), batch 172231, train_loss = 1.838, time/batch = 0.091\n",
      "Sequence 1690452/3263200 (epoch 51), batch 172331, train_loss = 1.242, time/batch = 0.091\n",
      "Sequence 1691432/3263200 (epoch 51), batch 172431, train_loss = 0.792, time/batch = 0.093\n",
      "Sequence 1692422/3263200 (epoch 51), batch 172531, train_loss = 1.034, time/batch = 0.092\n",
      "Sequence 1693412/3263200 (epoch 51), batch 172631, train_loss = 1.888, time/batch = 0.092\n",
      "Sequence 1694382/3263200 (epoch 51), batch 172731, train_loss = 0.985, time/batch = 0.092\n",
      "Sequence 1695322/3263200 (epoch 51), batch 172831, train_loss = 0.901, time/batch = 0.093\n",
      "Sequence 1696302/3263200 (epoch 51), batch 172931, train_loss = 1.060, time/batch = 0.093\n",
      "Epoch 51 completed, average train loss 1.248276, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1697284/3263200 (epoch 52), batch 173031, train_loss = 1.454, time/batch = 0.093\n",
      "Sequence 1698254/3263200 (epoch 52), batch 173131, train_loss = 1.463, time/batch = 0.093\n",
      "Sequence 1699244/3263200 (epoch 52), batch 173231, train_loss = 0.878, time/batch = 0.092\n",
      "Sequence 1700214/3263200 (epoch 52), batch 173331, train_loss = 1.184, time/batch = 0.094\n",
      "Sequence 1701214/3263200 (epoch 52), batch 173431, train_loss = 0.963, time/batch = 0.092\n",
      "Sequence 1702204/3263200 (epoch 52), batch 173531, train_loss = 2.364, time/batch = 0.093\n",
      "Sequence 1703204/3263200 (epoch 52), batch 173631, train_loss = 1.488, time/batch = 0.094\n",
      "Sequence 1704174/3263200 (epoch 52), batch 173731, train_loss = 1.163, time/batch = 0.094\n",
      "Sequence 1705164/3263200 (epoch 52), batch 173831, train_loss = 0.707, time/batch = 0.091\n",
      "Sequence 1706124/3263200 (epoch 52), batch 173931, train_loss = 1.971, time/batch = 0.092\n",
      "Sequence 1707104/3263200 (epoch 52), batch 174031, train_loss = 1.270, time/batch = 0.092\n",
      "Sequence 1708094/3263200 (epoch 52), batch 174131, train_loss = 1.494, time/batch = 0.091\n",
      "Sequence 1709084/3263200 (epoch 52), batch 174231, train_loss = 1.379, time/batch = 0.092\n",
      "Sequence 1710084/3263200 (epoch 52), batch 174331, train_loss = 1.239, time/batch = 0.093\n",
      "Sequence 1711074/3263200 (epoch 52), batch 174431, train_loss = 1.562, time/batch = 0.092\n",
      "Sequence 1712054/3263200 (epoch 52), batch 174531, train_loss = 0.781, time/batch = 0.093\n",
      "Sequence 1713024/3263200 (epoch 52), batch 174631, train_loss = 1.189, time/batch = 0.093\n",
      "Sequence 1714014/3263200 (epoch 52), batch 174731, train_loss = 1.473, time/batch = 0.093\n",
      "Sequence 1714974/3263200 (epoch 52), batch 174831, train_loss = 0.909, time/batch = 0.092\n",
      "Sequence 1715934/3263200 (epoch 52), batch 174931, train_loss = 1.669, time/batch = 0.093\n",
      "Sequence 1716934/3263200 (epoch 52), batch 175031, train_loss = 1.458, time/batch = 0.093\n",
      "Sequence 1717914/3263200 (epoch 52), batch 175131, train_loss = 1.324, time/batch = 0.093\n",
      "Sequence 1718894/3263200 (epoch 52), batch 175231, train_loss = 1.147, time/batch = 0.092\n",
      "Sequence 1719854/3263200 (epoch 52), batch 175331, train_loss = 1.224, time/batch = 0.091\n",
      "Sequence 1720834/3263200 (epoch 52), batch 175431, train_loss = 1.169, time/batch = 0.093\n",
      "Sequence 1721814/3263200 (epoch 52), batch 175531, train_loss = 0.892, time/batch = 0.093\n",
      "Sequence 1722804/3263200 (epoch 52), batch 175631, train_loss = 1.380, time/batch = 0.091\n",
      "Sequence 1723794/3263200 (epoch 52), batch 175731, train_loss = 1.854, time/batch = 0.092\n",
      "Sequence 1724744/3263200 (epoch 52), batch 175831, train_loss = 1.211, time/batch = 0.093\n",
      "Sequence 1725724/3263200 (epoch 52), batch 175931, train_loss = 1.162, time/batch = 0.092\n",
      "Sequence 1726704/3263200 (epoch 52), batch 176031, train_loss = 0.834, time/batch = 0.093\n",
      "Sequence 1727694/3263200 (epoch 52), batch 176131, train_loss = 1.706, time/batch = 0.093\n",
      "Sequence 1728674/3263200 (epoch 52), batch 176231, train_loss = 2.014, time/batch = 0.092\n",
      "Epoch 52 completed, average train loss 1.246746, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1729666/3263200 (epoch 53), batch 176331, train_loss = 1.789, time/batch = 0.093\n",
      "Sequence 1730656/3263200 (epoch 53), batch 176431, train_loss = 1.142, time/batch = 0.091\n",
      "Sequence 1731646/3263200 (epoch 53), batch 176531, train_loss = 1.187, time/batch = 0.094\n",
      "Sequence 1732616/3263200 (epoch 53), batch 176631, train_loss = 0.987, time/batch = 0.093\n",
      "Sequence 1733606/3263200 (epoch 53), batch 176731, train_loss = 0.965, time/batch = 0.093\n",
      "Sequence 1734566/3263200 (epoch 53), batch 176831, train_loss = 1.408, time/batch = 0.092\n",
      "Sequence 1735566/3263200 (epoch 53), batch 176931, train_loss = 1.552, time/batch = 0.092\n",
      "Sequence 1736546/3263200 (epoch 53), batch 177031, train_loss = 1.363, time/batch = 0.093\n",
      "Sequence 1737546/3263200 (epoch 53), batch 177131, train_loss = 1.070, time/batch = 0.092\n",
      "Sequence 1738516/3263200 (epoch 53), batch 177231, train_loss = 0.788, time/batch = 0.094\n",
      "Sequence 1739466/3263200 (epoch 53), batch 177331, train_loss = 1.300, time/batch = 0.092\n",
      "Sequence 1740456/3263200 (epoch 53), batch 177431, train_loss = 1.323, time/batch = 0.093\n",
      "Sequence 1741446/3263200 (epoch 53), batch 177531, train_loss = 1.623, time/batch = 0.092\n",
      "Sequence 1742416/3263200 (epoch 53), batch 177631, train_loss = 1.237, time/batch = 0.093\n",
      "Sequence 1743406/3263200 (epoch 53), batch 177731, train_loss = 1.141, time/batch = 0.092\n",
      "Sequence 1744366/3263200 (epoch 53), batch 177831, train_loss = 1.233, time/batch = 0.093\n",
      "Sequence 1745356/3263200 (epoch 53), batch 177931, train_loss = 1.711, time/batch = 0.091\n",
      "Sequence 1746336/3263200 (epoch 53), batch 178032, train_loss = 0.666, time/batch = 0.092\n",
      "Sequence 1747306/3263200 (epoch 53), batch 178132, train_loss = 1.237, time/batch = 0.092\n",
      "Sequence 1748286/3263200 (epoch 53), batch 178232, train_loss = 1.492, time/batch = 0.093\n",
      "Sequence 1749286/3263200 (epoch 53), batch 178332, train_loss = 1.447, time/batch = 0.092\n",
      "Sequence 1750256/3263200 (epoch 53), batch 178432, train_loss = 0.690, time/batch = 0.093\n",
      "Sequence 1751246/3263200 (epoch 53), batch 178532, train_loss = 1.125, time/batch = 0.092\n",
      "Sequence 1752246/3263200 (epoch 53), batch 178632, train_loss = 0.966, time/batch = 0.092\n",
      "Sequence 1753226/3263200 (epoch 53), batch 178732, train_loss = 1.296, time/batch = 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1754196/3263200 (epoch 53), batch 178832, train_loss = 0.881, time/batch = 0.092\n",
      "Sequence 1755176/3263200 (epoch 53), batch 178932, train_loss = 0.889, time/batch = 0.094\n",
      "Sequence 1756156/3263200 (epoch 53), batch 179032, train_loss = 2.206, time/batch = 0.093\n",
      "Sequence 1757146/3263200 (epoch 53), batch 179132, train_loss = 1.387, time/batch = 0.092\n",
      "Sequence 1758146/3263200 (epoch 53), batch 179232, train_loss = 1.237, time/batch = 0.092\n",
      "Sequence 1759116/3263200 (epoch 53), batch 179332, train_loss = 0.859, time/batch = 0.092\n",
      "Sequence 1760076/3263200 (epoch 53), batch 179432, train_loss = 1.649, time/batch = 0.092\n",
      "Sequence 1761066/3263200 (epoch 53), batch 179532, train_loss = 0.872, time/batch = 0.092\n",
      "Sequence 1762046/3263200 (epoch 53), batch 179632, train_loss = 1.375, time/batch = 0.093\n",
      "Epoch 53 completed, average train loss 1.243329, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1763038/3263200 (epoch 54), batch 179732, train_loss = 1.116, time/batch = 0.094\n",
      "Sequence 1764018/3263200 (epoch 54), batch 179832, train_loss = 1.433, time/batch = 0.093\n",
      "Sequence 1765008/3263200 (epoch 54), batch 179932, train_loss = 1.079, time/batch = 0.092\n",
      "Sequence 1765978/3263200 (epoch 54), batch 180032, train_loss = 1.111, time/batch = 0.092\n",
      "Sequence 1766968/3263200 (epoch 54), batch 180132, train_loss = 0.902, time/batch = 0.092\n",
      "Sequence 1767958/3263200 (epoch 54), batch 180233, train_loss = 0.894, time/batch = 0.093\n",
      "Sequence 1768938/3263200 (epoch 54), batch 180333, train_loss = 0.952, time/batch = 0.091\n",
      "Sequence 1769908/3263200 (epoch 54), batch 180433, train_loss = 1.252, time/batch = 0.092\n",
      "Sequence 1770908/3263200 (epoch 54), batch 180533, train_loss = 1.050, time/batch = 0.093\n",
      "Sequence 1771888/3263200 (epoch 54), batch 180633, train_loss = 1.755, time/batch = 0.093\n",
      "Sequence 1772858/3263200 (epoch 54), batch 180733, train_loss = 1.576, time/batch = 0.092\n",
      "Sequence 1773858/3263200 (epoch 54), batch 180833, train_loss = 0.984, time/batch = 0.092\n",
      "Sequence 1774818/3263200 (epoch 54), batch 180933, train_loss = 0.888, time/batch = 0.092\n",
      "Sequence 1775798/3263200 (epoch 54), batch 181033, train_loss = 0.997, time/batch = 0.093\n",
      "Sequence 1776748/3263200 (epoch 54), batch 181133, train_loss = 1.235, time/batch = 0.093\n",
      "Sequence 1777708/3263200 (epoch 54), batch 181233, train_loss = 0.832, time/batch = 0.094\n",
      "Sequence 1778688/3263200 (epoch 54), batch 181333, train_loss = 1.522, time/batch = 0.093\n",
      "Sequence 1779658/3263200 (epoch 54), batch 181433, train_loss = 1.058, time/batch = 0.092\n",
      "Sequence 1780638/3263200 (epoch 54), batch 181533, train_loss = 1.486, time/batch = 0.092\n",
      "Sequence 1781628/3263200 (epoch 54), batch 181633, train_loss = 1.147, time/batch = 0.092\n",
      "Sequence 1782618/3263200 (epoch 54), batch 181733, train_loss = 1.440, time/batch = 0.093\n",
      "Sequence 1783618/3263200 (epoch 54), batch 181833, train_loss = 1.580, time/batch = 0.091\n",
      "Sequence 1784618/3263200 (epoch 54), batch 181933, train_loss = 1.077, time/batch = 0.092\n",
      "Sequence 1785608/3263200 (epoch 54), batch 182033, train_loss = 0.825, time/batch = 0.092\n",
      "Sequence 1786578/3263200 (epoch 54), batch 182133, train_loss = 0.784, time/batch = 0.093\n",
      "Sequence 1787568/3263200 (epoch 54), batch 182233, train_loss = 1.081, time/batch = 0.092\n",
      "Sequence 1788558/3263200 (epoch 54), batch 182333, train_loss = 1.228, time/batch = 0.093\n",
      "Sequence 1789538/3263200 (epoch 54), batch 182433, train_loss = 1.306, time/batch = 0.091\n",
      "Sequence 1790538/3263200 (epoch 54), batch 182533, train_loss = 1.453, time/batch = 0.092\n",
      "Sequence 1791508/3263200 (epoch 54), batch 182633, train_loss = 0.889, time/batch = 0.092\n",
      "Sequence 1792498/3263200 (epoch 54), batch 182733, train_loss = 1.583, time/batch = 0.093\n",
      "Sequence 1793488/3263200 (epoch 54), batch 182833, train_loss = 0.987, time/batch = 0.094\n",
      "Sequence 1794448/3263200 (epoch 54), batch 182933, train_loss = 1.011, time/batch = 0.093\n",
      "Epoch 54 completed, average train loss 1.237734, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 1795410/3263200 (epoch 55), batch 183033, train_loss = 2.004, time/batch = 0.095\n",
      "Sequence 1796410/3263200 (epoch 55), batch 183133, train_loss = 1.049, time/batch = 0.093\n",
      "Sequence 1797400/3263200 (epoch 55), batch 183233, train_loss = 0.903, time/batch = 0.094\n",
      "Sequence 1798380/3263200 (epoch 55), batch 183333, train_loss = 0.969, time/batch = 0.092\n",
      "Sequence 1799350/3263200 (epoch 55), batch 183433, train_loss = 0.905, time/batch = 0.092\n",
      "Sequence 1800310/3263200 (epoch 55), batch 183533, train_loss = 1.180, time/batch = 0.092\n",
      "Sequence 1801270/3263200 (epoch 55), batch 183633, train_loss = 1.841, time/batch = 0.092\n",
      "Sequence 1802260/3263200 (epoch 55), batch 183733, train_loss = 1.473, time/batch = 0.092\n",
      "Sequence 1803240/3263200 (epoch 55), batch 183833, train_loss = 0.856, time/batch = 0.093\n",
      "Sequence 1804210/3263200 (epoch 55), batch 183933, train_loss = 1.620, time/batch = 0.093\n",
      "Sequence 1805190/3263200 (epoch 55), batch 184033, train_loss = 0.956, time/batch = 0.093\n",
      "Sequence 1806170/3263200 (epoch 55), batch 184133, train_loss = 1.162, time/batch = 0.095\n",
      "Sequence 1807160/3263200 (epoch 55), batch 184233, train_loss = 1.214, time/batch = 0.094\n",
      "Sequence 1808140/3263200 (epoch 55), batch 184333, train_loss = 1.036, time/batch = 0.092\n",
      "Sequence 1809120/3263200 (epoch 55), batch 184433, train_loss = 0.923, time/batch = 0.090\n",
      "Sequence 1810110/3263200 (epoch 55), batch 184533, train_loss = 1.010, time/batch = 0.090\n",
      "Sequence 1811090/3263200 (epoch 55), batch 184633, train_loss = 1.312, time/batch = 0.092\n",
      "Sequence 1812090/3263200 (epoch 55), batch 184733, train_loss = 1.028, time/batch = 0.092\n",
      "Sequence 1813080/3263200 (epoch 55), batch 184833, train_loss = 1.273, time/batch = 0.093\n",
      "Sequence 1814070/3263200 (epoch 55), batch 184933, train_loss = 0.973, time/batch = 0.094\n",
      "Sequence 1815060/3263200 (epoch 55), batch 185033, train_loss = 1.166, time/batch = 0.092\n",
      "Sequence 1816010/3263200 (epoch 55), batch 185133, train_loss = 1.652, time/batch = 0.093\n",
      "Sequence 1817000/3263200 (epoch 55), batch 185233, train_loss = 1.126, time/batch = 0.092\n",
      "Sequence 1817990/3263200 (epoch 55), batch 185333, train_loss = 0.882, time/batch = 0.093\n",
      "Sequence 1818970/3263200 (epoch 55), batch 185433, train_loss = 1.816, time/batch = 0.093\n",
      "Sequence 1819940/3263200 (epoch 55), batch 185533, train_loss = 1.572, time/batch = 0.093\n",
      "Sequence 1820930/3263200 (epoch 55), batch 185633, train_loss = 1.346, time/batch = 0.093\n",
      "Sequence 1821920/3263200 (epoch 55), batch 185733, train_loss = 1.572, time/batch = 0.093\n",
      "Sequence 1822880/3263200 (epoch 55), batch 185833, train_loss = 1.298, time/batch = 0.092\n",
      "Sequence 1823870/3263200 (epoch 55), batch 185933, train_loss = 1.387, time/batch = 0.093\n",
      "Sequence 1824860/3263200 (epoch 55), batch 186033, train_loss = 1.204, time/batch = 0.092\n",
      "Sequence 1825850/3263200 (epoch 55), batch 186133, train_loss = 1.378, time/batch = 0.092\n",
      "Sequence 1826820/3263200 (epoch 55), batch 186233, train_loss = 1.906, time/batch = 0.092\n",
      "Epoch 55 completed, average train loss 1.235145, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1827782/3263200 (epoch 56), batch 186333, train_loss = 0.979, time/batch = 0.093\n",
      "Sequence 1828782/3263200 (epoch 56), batch 186433, train_loss = 0.872, time/batch = 0.093\n",
      "Sequence 1829772/3263200 (epoch 56), batch 186533, train_loss = 1.409, time/batch = 0.092\n",
      "Sequence 1830752/3263200 (epoch 56), batch 186633, train_loss = 1.929, time/batch = 0.091\n",
      "Sequence 1831722/3263200 (epoch 56), batch 186733, train_loss = 1.314, time/batch = 0.093\n",
      "Sequence 1832712/3263200 (epoch 56), batch 186833, train_loss = 1.598, time/batch = 0.093\n",
      "Sequence 1833712/3263200 (epoch 56), batch 186933, train_loss = 1.238, time/batch = 0.094\n",
      "Sequence 1834702/3263200 (epoch 56), batch 187033, train_loss = 2.174, time/batch = 0.093\n",
      "Sequence 1835692/3263200 (epoch 56), batch 187133, train_loss = 0.857, time/batch = 0.095\n",
      "Sequence 1836682/3263200 (epoch 56), batch 187233, train_loss = 0.776, time/batch = 0.093\n",
      "Sequence 1837672/3263200 (epoch 56), batch 187333, train_loss = 1.350, time/batch = 0.093\n",
      "Sequence 1838632/3263200 (epoch 56), batch 187433, train_loss = 1.729, time/batch = 0.093\n",
      "Sequence 1839612/3263200 (epoch 56), batch 187533, train_loss = 0.919, time/batch = 0.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1840562/3263200 (epoch 56), batch 187633, train_loss = 1.382, time/batch = 0.092\n",
      "Sequence 1841532/3263200 (epoch 56), batch 187733, train_loss = 0.840, time/batch = 0.092\n",
      "Sequence 1842502/3263200 (epoch 56), batch 187833, train_loss = 1.013, time/batch = 0.092\n",
      "Sequence 1843482/3263200 (epoch 56), batch 187933, train_loss = 1.094, time/batch = 0.092\n",
      "Sequence 1844452/3263200 (epoch 56), batch 188033, train_loss = 0.792, time/batch = 0.092\n",
      "Sequence 1845442/3263200 (epoch 56), batch 188133, train_loss = 1.215, time/batch = 0.093\n",
      "Sequence 1846432/3263200 (epoch 56), batch 188233, train_loss = 1.056, time/batch = 0.093\n",
      "Sequence 1847402/3263200 (epoch 56), batch 188333, train_loss = 1.292, time/batch = 0.094\n",
      "Sequence 1848382/3263200 (epoch 56), batch 188433, train_loss = 1.201, time/batch = 0.093\n",
      "Sequence 1849362/3263200 (epoch 56), batch 188533, train_loss = 1.306, time/batch = 0.094\n",
      "Sequence 1850332/3263200 (epoch 56), batch 188633, train_loss = 1.325, time/batch = 0.092\n",
      "Sequence 1851302/3263200 (epoch 56), batch 188733, train_loss = 1.288, time/batch = 0.093\n",
      "Sequence 1852302/3263200 (epoch 56), batch 188833, train_loss = 1.213, time/batch = 0.093\n",
      "Sequence 1853292/3263200 (epoch 56), batch 188933, train_loss = 1.036, time/batch = 0.092\n",
      "Sequence 1854282/3263200 (epoch 56), batch 189033, train_loss = 0.959, time/batch = 0.092\n",
      "Sequence 1855272/3263200 (epoch 56), batch 189133, train_loss = 1.237, time/batch = 0.092\n",
      "Sequence 1856232/3263200 (epoch 56), batch 189233, train_loss = 1.096, time/batch = 0.093\n",
      "Sequence 1857232/3263200 (epoch 56), batch 189333, train_loss = 1.223, time/batch = 0.092\n",
      "Sequence 1858202/3263200 (epoch 56), batch 189433, train_loss = 0.809, time/batch = 0.092\n",
      "Sequence 1859192/3263200 (epoch 56), batch 189533, train_loss = 1.128, time/batch = 0.092\n",
      "Epoch 56 completed, average train loss 1.232938, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1860164/3263200 (epoch 57), batch 189633, train_loss = 1.175, time/batch = 0.094\n",
      "Sequence 1861154/3263200 (epoch 57), batch 189733, train_loss = 1.630, time/batch = 0.092\n",
      "Sequence 1862124/3263200 (epoch 57), batch 189833, train_loss = 1.375, time/batch = 0.091\n",
      "Sequence 1863104/3263200 (epoch 57), batch 189933, train_loss = 1.122, time/batch = 0.093\n",
      "Sequence 1864084/3263200 (epoch 57), batch 190033, train_loss = 1.122, time/batch = 0.092\n",
      "Sequence 1865084/3263200 (epoch 57), batch 190133, train_loss = 1.203, time/batch = 0.093\n",
      "Sequence 1866064/3263200 (epoch 57), batch 190233, train_loss = 0.964, time/batch = 0.093\n",
      "Sequence 1867064/3263200 (epoch 57), batch 190333, train_loss = 1.161, time/batch = 0.092\n",
      "Sequence 1868034/3263200 (epoch 57), batch 190433, train_loss = 1.098, time/batch = 0.091\n",
      "Sequence 1869024/3263200 (epoch 57), batch 190533, train_loss = 1.323, time/batch = 0.093\n",
      "Sequence 1870014/3263200 (epoch 57), batch 190633, train_loss = 0.914, time/batch = 0.093\n",
      "Sequence 1871004/3263200 (epoch 57), batch 190733, train_loss = 1.811, time/batch = 0.093\n",
      "Sequence 1871994/3263200 (epoch 57), batch 190833, train_loss = 1.314, time/batch = 0.095\n",
      "Sequence 1872974/3263200 (epoch 57), batch 190933, train_loss = 1.136, time/batch = 0.093\n",
      "Sequence 1873954/3263200 (epoch 57), batch 191033, train_loss = 0.995, time/batch = 0.093\n",
      "Sequence 1874934/3263200 (epoch 57), batch 191133, train_loss = 0.985, time/batch = 0.093\n",
      "Sequence 1875884/3263200 (epoch 57), batch 191233, train_loss = 0.907, time/batch = 0.093\n",
      "Sequence 1876854/3263200 (epoch 57), batch 191333, train_loss = 1.178, time/batch = 0.093\n",
      "Sequence 1877834/3263200 (epoch 57), batch 191433, train_loss = 0.670, time/batch = 0.092\n",
      "Sequence 1878804/3263200 (epoch 57), batch 191533, train_loss = 0.970, time/batch = 0.092\n",
      "Sequence 1879794/3263200 (epoch 57), batch 191633, train_loss = 2.138, time/batch = 0.092\n",
      "Sequence 1880764/3263200 (epoch 57), batch 191733, train_loss = 1.377, time/batch = 0.092\n",
      "Sequence 1881744/3263200 (epoch 57), batch 191833, train_loss = 1.171, time/batch = 0.092\n",
      "Sequence 1882744/3263200 (epoch 57), batch 191933, train_loss = 1.304, time/batch = 0.093\n",
      "Sequence 1883734/3263200 (epoch 57), batch 192033, train_loss = 0.832, time/batch = 0.093\n",
      "Sequence 1884694/3263200 (epoch 57), batch 192133, train_loss = 1.065, time/batch = 0.093\n",
      "Sequence 1885684/3263200 (epoch 57), batch 192233, train_loss = 1.115, time/batch = 0.092\n",
      "Sequence 1886664/3263200 (epoch 57), batch 192333, train_loss = 1.240, time/batch = 0.094\n",
      "Sequence 1887624/3263200 (epoch 57), batch 192433, train_loss = 1.026, time/batch = 0.093\n",
      "Sequence 1888624/3263200 (epoch 57), batch 192533, train_loss = 0.773, time/batch = 0.092\n",
      "Sequence 1889594/3263200 (epoch 57), batch 192633, train_loss = 0.823, time/batch = 0.093\n",
      "Sequence 1890574/3263200 (epoch 57), batch 192733, train_loss = 1.252, time/batch = 0.092\n",
      "Sequence 1891534/3263200 (epoch 57), batch 192833, train_loss = 1.401, time/batch = 0.092\n",
      "Sequence 1892524/3263200 (epoch 57), batch 192933, train_loss = 0.942, time/batch = 0.092\n",
      "Epoch 57 completed, average train loss 1.228575, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1893496/3263200 (epoch 58), batch 193033, train_loss = 0.612, time/batch = 0.093\n",
      "Sequence 1894476/3263200 (epoch 58), batch 193133, train_loss = 0.827, time/batch = 0.093\n",
      "Sequence 1895466/3263200 (epoch 58), batch 193233, train_loss = 0.942, time/batch = 0.092\n",
      "Sequence 1896456/3263200 (epoch 58), batch 193333, train_loss = 1.096, time/batch = 0.094\n",
      "Sequence 1897456/3263200 (epoch 58), batch 193433, train_loss = 1.176, time/batch = 0.091\n",
      "Sequence 1898446/3263200 (epoch 58), batch 193533, train_loss = 1.162, time/batch = 0.093\n",
      "Sequence 1899426/3263200 (epoch 58), batch 193633, train_loss = 1.946, time/batch = 0.093\n",
      "Sequence 1900416/3263200 (epoch 58), batch 193733, train_loss = 1.412, time/batch = 0.094\n",
      "Sequence 1901396/3263200 (epoch 58), batch 193833, train_loss = 1.437, time/batch = 0.092\n",
      "Sequence 1902386/3263200 (epoch 58), batch 193933, train_loss = 0.865, time/batch = 0.093\n",
      "Sequence 1903366/3263200 (epoch 58), batch 194033, train_loss = 0.987, time/batch = 0.092\n",
      "Sequence 1904356/3263200 (epoch 58), batch 194133, train_loss = 0.664, time/batch = 0.093\n",
      "Sequence 1905326/3263200 (epoch 58), batch 194233, train_loss = 0.738, time/batch = 0.093\n",
      "Sequence 1906326/3263200 (epoch 58), batch 194333, train_loss = 1.123, time/batch = 0.093\n",
      "Sequence 1907306/3263200 (epoch 58), batch 194433, train_loss = 1.141, time/batch = 0.092\n",
      "Sequence 1908296/3263200 (epoch 58), batch 194533, train_loss = 1.117, time/batch = 0.093\n",
      "Sequence 1909246/3263200 (epoch 58), batch 194633, train_loss = 1.741, time/batch = 0.094\n",
      "Sequence 1910226/3263200 (epoch 58), batch 194734, train_loss = 0.856, time/batch = 0.093\n",
      "Sequence 1911196/3263200 (epoch 58), batch 194834, train_loss = 1.585, time/batch = 0.093\n",
      "Sequence 1912166/3263200 (epoch 58), batch 194934, train_loss = 1.232, time/batch = 0.094\n",
      "Sequence 1913156/3263200 (epoch 58), batch 195034, train_loss = 0.682, time/batch = 0.093\n",
      "Sequence 1914126/3263200 (epoch 58), batch 195134, train_loss = 1.188, time/batch = 0.093\n",
      "Sequence 1915096/3263200 (epoch 58), batch 195234, train_loss = 0.903, time/batch = 0.092\n",
      "Sequence 1916086/3263200 (epoch 58), batch 195334, train_loss = 0.977, time/batch = 0.091\n",
      "Sequence 1917066/3263200 (epoch 58), batch 195434, train_loss = 0.836, time/batch = 0.093\n",
      "Sequence 1918056/3263200 (epoch 58), batch 195534, train_loss = 1.771, time/batch = 0.093\n",
      "Sequence 1919046/3263200 (epoch 58), batch 195634, train_loss = 1.557, time/batch = 0.092\n",
      "Sequence 1920036/3263200 (epoch 58), batch 195734, train_loss = 1.097, time/batch = 0.093\n",
      "Sequence 1921026/3263200 (epoch 58), batch 195834, train_loss = 1.177, time/batch = 0.093\n",
      "Sequence 1921996/3263200 (epoch 58), batch 195934, train_loss = 0.898, time/batch = 0.093\n",
      "Sequence 1922936/3263200 (epoch 58), batch 196034, train_loss = 1.151, time/batch = 0.093\n",
      "Sequence 1923916/3263200 (epoch 58), batch 196134, train_loss = 1.041, time/batch = 0.093\n",
      "Sequence 1924906/3263200 (epoch 58), batch 196234, train_loss = 1.017, time/batch = 0.093\n",
      "Epoch 58 completed, average train loss 1.226710, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 1925868/3263200 (epoch 59), batch 196334, train_loss = 1.152, time/batch = 0.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1926858/3263200 (epoch 59), batch 196434, train_loss = 1.239, time/batch = 0.093\n",
      "Sequence 1927828/3263200 (epoch 59), batch 196534, train_loss = 0.823, time/batch = 0.093\n",
      "Sequence 1928808/3263200 (epoch 59), batch 196634, train_loss = 1.251, time/batch = 0.092\n",
      "Sequence 1929798/3263200 (epoch 59), batch 196734, train_loss = 0.989, time/batch = 0.094\n",
      "Sequence 1930728/3263200 (epoch 59), batch 196834, train_loss = 1.242, time/batch = 0.093\n",
      "Sequence 1931708/3263200 (epoch 59), batch 196934, train_loss = 1.127, time/batch = 0.093\n",
      "Sequence 1932708/3263200 (epoch 59), batch 197034, train_loss = 1.806, time/batch = 0.092\n",
      "Sequence 1933708/3263200 (epoch 59), batch 197134, train_loss = 1.102, time/batch = 0.094\n",
      "Sequence 1934688/3263200 (epoch 59), batch 197234, train_loss = 1.514, time/batch = 0.093\n",
      "Sequence 1935668/3263200 (epoch 59), batch 197334, train_loss = 1.667, time/batch = 0.092\n",
      "Sequence 1936648/3263200 (epoch 59), batch 197434, train_loss = 1.591, time/batch = 0.092\n",
      "Sequence 1937628/3263200 (epoch 59), batch 197534, train_loss = 0.916, time/batch = 0.094\n",
      "Sequence 1938628/3263200 (epoch 59), batch 197634, train_loss = 1.089, time/batch = 0.091\n",
      "Sequence 1939598/3263200 (epoch 59), batch 197734, train_loss = 1.608, time/batch = 0.093\n",
      "Sequence 1940598/3263200 (epoch 59), batch 197834, train_loss = 1.792, time/batch = 0.092\n",
      "Sequence 1941558/3263200 (epoch 59), batch 197934, train_loss = 1.394, time/batch = 0.092\n",
      "Sequence 1942548/3263200 (epoch 59), batch 198035, train_loss = 0.830, time/batch = 0.094\n",
      "Sequence 1943538/3263200 (epoch 59), batch 198135, train_loss = 1.075, time/batch = 0.094\n",
      "Sequence 1944518/3263200 (epoch 59), batch 198235, train_loss = 1.727, time/batch = 0.095\n",
      "Sequence 1945518/3263200 (epoch 59), batch 198335, train_loss = 1.479, time/batch = 0.092\n",
      "Sequence 1946508/3263200 (epoch 59), batch 198435, train_loss = 0.801, time/batch = 0.093\n",
      "Sequence 1947468/3263200 (epoch 59), batch 198535, train_loss = 1.053, time/batch = 0.095\n",
      "Sequence 1948458/3263200 (epoch 59), batch 198635, train_loss = 1.110, time/batch = 0.091\n",
      "Sequence 1949448/3263200 (epoch 59), batch 198735, train_loss = 1.210, time/batch = 0.087\n",
      "Sequence 1950408/3263200 (epoch 59), batch 198835, train_loss = 1.031, time/batch = 0.093\n",
      "Sequence 1951398/3263200 (epoch 59), batch 198935, train_loss = 1.640, time/batch = 0.092\n",
      "Sequence 1952368/3263200 (epoch 59), batch 199035, train_loss = 1.290, time/batch = 0.093\n",
      "Sequence 1953358/3263200 (epoch 59), batch 199135, train_loss = 1.739, time/batch = 0.094\n",
      "Sequence 1954328/3263200 (epoch 59), batch 199235, train_loss = 1.713, time/batch = 0.094\n",
      "Sequence 1955318/3263200 (epoch 59), batch 199335, train_loss = 0.747, time/batch = 0.092\n",
      "Sequence 1956278/3263200 (epoch 59), batch 199435, train_loss = 1.071, time/batch = 0.093\n",
      "Sequence 1957268/3263200 (epoch 59), batch 199535, train_loss = 0.956, time/batch = 0.094\n",
      "Epoch 59 completed, average train loss 1.222708, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 1958270/3263200 (epoch 60), batch 199635, train_loss = 1.414, time/batch = 0.095\n",
      "Sequence 1959230/3263200 (epoch 60), batch 199735, train_loss = 1.223, time/batch = 0.092\n",
      "Sequence 1960220/3263200 (epoch 60), batch 199835, train_loss = 1.049, time/batch = 0.093\n",
      "Sequence 1961180/3263200 (epoch 60), batch 199935, train_loss = 0.835, time/batch = 0.093\n",
      "Sequence 1962170/3263200 (epoch 60), batch 200035, train_loss = 1.327, time/batch = 0.092\n",
      "Sequence 1963150/3263200 (epoch 60), batch 200135, train_loss = 0.505, time/batch = 0.092\n",
      "Sequence 1964120/3263200 (epoch 60), batch 200235, train_loss = 0.899, time/batch = 0.093\n",
      "Sequence 1965110/3263200 (epoch 60), batch 200335, train_loss = 1.690, time/batch = 0.092\n",
      "Sequence 1966110/3263200 (epoch 60), batch 200435, train_loss = 1.082, time/batch = 0.092\n",
      "Sequence 1967090/3263200 (epoch 60), batch 200535, train_loss = 1.264, time/batch = 0.092\n",
      "Sequence 1968090/3263200 (epoch 60), batch 200635, train_loss = 1.002, time/batch = 0.092\n",
      "Sequence 1969080/3263200 (epoch 60), batch 200735, train_loss = 1.337, time/batch = 0.092\n",
      "Sequence 1970070/3263200 (epoch 60), batch 200835, train_loss = 1.196, time/batch = 0.091\n",
      "Sequence 1971040/3263200 (epoch 60), batch 200935, train_loss = 1.225, time/batch = 0.092\n",
      "Sequence 1972010/3263200 (epoch 60), batch 201035, train_loss = 1.004, time/batch = 0.093\n",
      "Sequence 1972990/3263200 (epoch 60), batch 201135, train_loss = 1.299, time/batch = 0.093\n",
      "Sequence 1973990/3263200 (epoch 60), batch 201235, train_loss = 0.857, time/batch = 0.093\n",
      "Sequence 1974970/3263200 (epoch 60), batch 201335, train_loss = 0.962, time/batch = 0.092\n",
      "Sequence 1975950/3263200 (epoch 60), batch 201436, train_loss = 0.616, time/batch = 0.093\n",
      "Sequence 1976910/3263200 (epoch 60), batch 201536, train_loss = 0.998, time/batch = 0.093\n",
      "Sequence 1977910/3263200 (epoch 60), batch 201636, train_loss = 1.457, time/batch = 0.093\n",
      "Sequence 1978890/3263200 (epoch 60), batch 201736, train_loss = 0.694, time/batch = 0.092\n",
      "Sequence 1979840/3263200 (epoch 60), batch 201836, train_loss = 0.862, time/batch = 0.093\n",
      "Sequence 1980790/3263200 (epoch 60), batch 201936, train_loss = 1.462, time/batch = 0.092\n",
      "Sequence 1981790/3263200 (epoch 60), batch 202036, train_loss = 1.507, time/batch = 0.093\n",
      "Sequence 1982750/3263200 (epoch 60), batch 202136, train_loss = 1.095, time/batch = 0.092\n",
      "Sequence 1983740/3263200 (epoch 60), batch 202236, train_loss = 1.149, time/batch = 0.092\n",
      "Sequence 1984740/3263200 (epoch 60), batch 202336, train_loss = 1.013, time/batch = 0.091\n",
      "Sequence 1985730/3263200 (epoch 60), batch 202436, train_loss = 0.778, time/batch = 0.091\n",
      "Sequence 1986700/3263200 (epoch 60), batch 202536, train_loss = 1.455, time/batch = 0.090\n",
      "Sequence 1987660/3263200 (epoch 60), batch 202636, train_loss = 1.045, time/batch = 0.089\n",
      "Sequence 1988660/3263200 (epoch 60), batch 202736, train_loss = 1.016, time/batch = 0.092\n",
      "Sequence 1989650/3263200 (epoch 60), batch 202836, train_loss = 1.145, time/batch = 0.093\n",
      "Epoch 60 completed, average train loss 1.220446, learning rate 0.0010\n",
      "Sequence 1990642/3263200 (epoch 61), batch 202936, train_loss = 1.312, time/batch = 0.094\n",
      "Shuffling training data...\n",
      "Sequence 1991632/3263200 (epoch 61), batch 203036, train_loss = 1.541, time/batch = 0.093\n",
      "Sequence 1992592/3263200 (epoch 61), batch 203136, train_loss = 1.064, time/batch = 0.094\n",
      "Sequence 1993562/3263200 (epoch 61), batch 203236, train_loss = 1.629, time/batch = 0.092\n",
      "Sequence 1994552/3263200 (epoch 61), batch 203336, train_loss = 1.561, time/batch = 0.094\n",
      "Sequence 1995522/3263200 (epoch 61), batch 203436, train_loss = 1.278, time/batch = 0.093\n",
      "Sequence 1996502/3263200 (epoch 61), batch 203536, train_loss = 0.895, time/batch = 0.091\n",
      "Sequence 1997492/3263200 (epoch 61), batch 203636, train_loss = 1.403, time/batch = 0.092\n",
      "Sequence 1998472/3263200 (epoch 61), batch 203736, train_loss = 1.679, time/batch = 0.092\n",
      "Sequence 1999442/3263200 (epoch 61), batch 203836, train_loss = 1.094, time/batch = 0.092\n",
      "Sequence 2000432/3263200 (epoch 61), batch 203936, train_loss = 0.761, time/batch = 0.091\n",
      "Sequence 2001422/3263200 (epoch 61), batch 204036, train_loss = 1.385, time/batch = 0.094\n",
      "Sequence 2002412/3263200 (epoch 61), batch 204136, train_loss = 1.136, time/batch = 0.093\n",
      "Sequence 2003392/3263200 (epoch 61), batch 204236, train_loss = 1.591, time/batch = 0.092\n",
      "Sequence 2004352/3263200 (epoch 61), batch 204336, train_loss = 0.750, time/batch = 0.093\n",
      "Sequence 2005352/3263200 (epoch 61), batch 204436, train_loss = 1.098, time/batch = 0.094\n",
      "Sequence 2006312/3263200 (epoch 61), batch 204536, train_loss = 1.253, time/batch = 0.094\n",
      "Sequence 2007292/3263200 (epoch 61), batch 204636, train_loss = 1.305, time/batch = 0.093\n",
      "Sequence 2008272/3263200 (epoch 61), batch 204736, train_loss = 1.201, time/batch = 0.093\n",
      "Sequence 2009262/3263200 (epoch 61), batch 204836, train_loss = 1.750, time/batch = 0.093\n",
      "Sequence 2010242/3263200 (epoch 61), batch 204936, train_loss = 0.958, time/batch = 0.093\n",
      "Sequence 2011222/3263200 (epoch 61), batch 205036, train_loss = 1.312, time/batch = 0.092\n",
      "Sequence 2012202/3263200 (epoch 61), batch 205136, train_loss = 1.518, time/batch = 0.094\n",
      "Sequence 2013182/3263200 (epoch 61), batch 205236, train_loss = 0.920, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2014162/3263200 (epoch 61), batch 205336, train_loss = 1.801, time/batch = 0.093\n",
      "Sequence 2015132/3263200 (epoch 61), batch 205436, train_loss = 0.809, time/batch = 0.092\n",
      "Sequence 2016112/3263200 (epoch 61), batch 205536, train_loss = 0.935, time/batch = 0.093\n",
      "Sequence 2017112/3263200 (epoch 61), batch 205636, train_loss = 1.176, time/batch = 0.093\n",
      "Sequence 2018102/3263200 (epoch 61), batch 205736, train_loss = 0.951, time/batch = 0.093\n",
      "Sequence 2019082/3263200 (epoch 61), batch 205836, train_loss = 1.166, time/batch = 0.092\n",
      "Sequence 2020062/3263200 (epoch 61), batch 205936, train_loss = 1.072, time/batch = 0.094\n",
      "Sequence 2021062/3263200 (epoch 61), batch 206036, train_loss = 1.394, time/batch = 0.092\n",
      "Sequence 2022042/3263200 (epoch 61), batch 206136, train_loss = 1.462, time/batch = 0.093\n",
      "Sequence 2023012/3263200 (epoch 61), batch 206236, train_loss = 1.339, time/batch = 0.092\n",
      "Epoch 61 completed, average train loss 1.215879, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2024004/3263200 (epoch 62), batch 206336, train_loss = 1.358, time/batch = 0.093\n",
      "Sequence 2024994/3263200 (epoch 62), batch 206436, train_loss = 2.077, time/batch = 0.092\n",
      "Sequence 2025984/3263200 (epoch 62), batch 206536, train_loss = 1.225, time/batch = 0.092\n",
      "Sequence 2026974/3263200 (epoch 62), batch 206636, train_loss = 1.138, time/batch = 0.093\n",
      "Sequence 2027934/3263200 (epoch 62), batch 206736, train_loss = 0.727, time/batch = 0.093\n",
      "Sequence 2028914/3263200 (epoch 62), batch 206836, train_loss = 1.398, time/batch = 0.093\n",
      "Sequence 2029904/3263200 (epoch 62), batch 206936, train_loss = 1.315, time/batch = 0.092\n",
      "Sequence 2030894/3263200 (epoch 62), batch 207036, train_loss = 1.056, time/batch = 0.093\n",
      "Sequence 2031874/3263200 (epoch 62), batch 207136, train_loss = 1.123, time/batch = 0.093\n",
      "Sequence 2032864/3263200 (epoch 62), batch 207236, train_loss = 1.153, time/batch = 0.092\n",
      "Sequence 2033834/3263200 (epoch 62), batch 207336, train_loss = 1.111, time/batch = 0.093\n",
      "Sequence 2034834/3263200 (epoch 62), batch 207436, train_loss = 1.531, time/batch = 0.093\n",
      "Sequence 2035824/3263200 (epoch 62), batch 207536, train_loss = 0.887, time/batch = 0.092\n",
      "Sequence 2036794/3263200 (epoch 62), batch 207636, train_loss = 0.868, time/batch = 0.092\n",
      "Sequence 2037774/3263200 (epoch 62), batch 207736, train_loss = 0.942, time/batch = 0.092\n",
      "Sequence 2038734/3263200 (epoch 62), batch 207836, train_loss = 1.248, time/batch = 0.092\n",
      "Sequence 2039724/3263200 (epoch 62), batch 207936, train_loss = 1.686, time/batch = 0.091\n",
      "Sequence 2040714/3263200 (epoch 62), batch 208036, train_loss = 1.430, time/batch = 0.090\n",
      "Sequence 2041674/3263200 (epoch 62), batch 208136, train_loss = 1.531, time/batch = 0.092\n",
      "Sequence 2042664/3263200 (epoch 62), batch 208236, train_loss = 0.997, time/batch = 0.094\n",
      "Sequence 2043664/3263200 (epoch 62), batch 208336, train_loss = 1.155, time/batch = 0.092\n",
      "Sequence 2044634/3263200 (epoch 62), batch 208436, train_loss = 1.093, time/batch = 0.092\n",
      "Sequence 2045624/3263200 (epoch 62), batch 208536, train_loss = 0.973, time/batch = 0.093\n",
      "Sequence 2046614/3263200 (epoch 62), batch 208636, train_loss = 1.140, time/batch = 0.093\n",
      "Sequence 2047594/3263200 (epoch 62), batch 208736, train_loss = 1.313, time/batch = 0.092\n",
      "Sequence 2048574/3263200 (epoch 62), batch 208836, train_loss = 0.841, time/batch = 0.091\n",
      "Sequence 2049524/3263200 (epoch 62), batch 208936, train_loss = 1.083, time/batch = 0.093\n",
      "Sequence 2050514/3263200 (epoch 62), batch 209036, train_loss = 1.391, time/batch = 0.092\n",
      "Sequence 2051494/3263200 (epoch 62), batch 209136, train_loss = 0.981, time/batch = 0.093\n",
      "Sequence 2052474/3263200 (epoch 62), batch 209236, train_loss = 1.585, time/batch = 0.093\n",
      "Sequence 2053444/3263200 (epoch 62), batch 209336, train_loss = 1.301, time/batch = 0.093\n",
      "Sequence 2054414/3263200 (epoch 62), batch 209436, train_loss = 1.250, time/batch = 0.092\n",
      "Sequence 2055374/3263200 (epoch 62), batch 209536, train_loss = 1.247, time/batch = 0.093\n",
      "Epoch 62 completed, average train loss 1.214917, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2056366/3263200 (epoch 63), batch 209636, train_loss = 1.048, time/batch = 0.092\n",
      "Sequence 2057326/3263200 (epoch 63), batch 209736, train_loss = 1.181, time/batch = 0.092\n",
      "Sequence 2058316/3263200 (epoch 63), batch 209836, train_loss = 1.022, time/batch = 0.093\n",
      "Sequence 2059276/3263200 (epoch 63), batch 209936, train_loss = 1.394, time/batch = 0.092\n",
      "Sequence 2060246/3263200 (epoch 63), batch 210036, train_loss = 1.232, time/batch = 0.092\n",
      "Sequence 2061236/3263200 (epoch 63), batch 210136, train_loss = 1.148, time/batch = 0.093\n",
      "Sequence 2062216/3263200 (epoch 63), batch 210236, train_loss = 0.993, time/batch = 0.094\n",
      "Sequence 2063206/3263200 (epoch 63), batch 210336, train_loss = 0.900, time/batch = 0.092\n",
      "Sequence 2064176/3263200 (epoch 63), batch 210436, train_loss = 0.974, time/batch = 0.092\n",
      "Sequence 2065166/3263200 (epoch 63), batch 210536, train_loss = 1.435, time/batch = 0.092\n",
      "Sequence 2066166/3263200 (epoch 63), batch 210636, train_loss = 1.325, time/batch = 0.093\n",
      "Sequence 2067146/3263200 (epoch 63), batch 210736, train_loss = 1.066, time/batch = 0.092\n",
      "Sequence 2068136/3263200 (epoch 63), batch 210836, train_loss = 0.869, time/batch = 0.093\n",
      "Sequence 2069116/3263200 (epoch 63), batch 210936, train_loss = 0.923, time/batch = 0.092\n",
      "Sequence 2070076/3263200 (epoch 63), batch 211036, train_loss = 0.857, time/batch = 0.093\n",
      "Sequence 2071056/3263200 (epoch 63), batch 211136, train_loss = 0.968, time/batch = 0.092\n",
      "Sequence 2072036/3263200 (epoch 63), batch 211236, train_loss = 1.078, time/batch = 0.093\n",
      "Sequence 2073016/3263200 (epoch 63), batch 211336, train_loss = 1.129, time/batch = 0.093\n",
      "Sequence 2073986/3263200 (epoch 63), batch 211436, train_loss = 1.097, time/batch = 0.092\n",
      "Sequence 2074966/3263200 (epoch 63), batch 211536, train_loss = 1.160, time/batch = 0.093\n",
      "Sequence 2075966/3263200 (epoch 63), batch 211636, train_loss = 1.502, time/batch = 0.093\n",
      "Sequence 2076936/3263200 (epoch 63), batch 211736, train_loss = 1.201, time/batch = 0.093\n",
      "Sequence 2077926/3263200 (epoch 63), batch 211836, train_loss = 1.306, time/batch = 0.091\n",
      "Sequence 2078916/3263200 (epoch 63), batch 211936, train_loss = 1.216, time/batch = 0.093\n",
      "Sequence 2079916/3263200 (epoch 63), batch 212036, train_loss = 1.284, time/batch = 0.092\n",
      "Sequence 2080906/3263200 (epoch 63), batch 212136, train_loss = 1.347, time/batch = 0.093\n",
      "Sequence 2081876/3263200 (epoch 63), batch 212236, train_loss = 1.302, time/batch = 0.092\n",
      "Sequence 2082826/3263200 (epoch 63), batch 212336, train_loss = 1.233, time/batch = 0.092\n",
      "Sequence 2083796/3263200 (epoch 63), batch 212436, train_loss = 1.635, time/batch = 0.092\n",
      "Sequence 2084776/3263200 (epoch 63), batch 212536, train_loss = 1.700, time/batch = 0.093\n",
      "Sequence 2085756/3263200 (epoch 63), batch 212636, train_loss = 0.769, time/batch = 0.092\n",
      "Sequence 2086746/3263200 (epoch 63), batch 212736, train_loss = 1.135, time/batch = 0.093\n",
      "Sequence 2087746/3263200 (epoch 63), batch 212836, train_loss = 0.785, time/batch = 0.092\n",
      "Epoch 63 completed, average train loss 1.211284, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2088748/3263200 (epoch 64), batch 212936, train_loss = 1.659, time/batch = 0.092\n",
      "Sequence 2089738/3263200 (epoch 64), batch 213036, train_loss = 1.131, time/batch = 0.093\n",
      "Sequence 2090728/3263200 (epoch 64), batch 213136, train_loss = 0.714, time/batch = 0.093\n",
      "Sequence 2091688/3263200 (epoch 64), batch 213236, train_loss = 1.034, time/batch = 0.092\n",
      "Sequence 2092658/3263200 (epoch 64), batch 213336, train_loss = 0.919, time/batch = 0.092\n",
      "Sequence 2093648/3263200 (epoch 64), batch 213436, train_loss = 1.952, time/batch = 0.094\n",
      "Sequence 2094638/3263200 (epoch 64), batch 213536, train_loss = 1.543, time/batch = 0.093\n",
      "Sequence 2095638/3263200 (epoch 64), batch 213636, train_loss = 1.638, time/batch = 0.093\n",
      "Sequence 2096598/3263200 (epoch 64), batch 213736, train_loss = 1.494, time/batch = 0.094\n",
      "Sequence 2097568/3263200 (epoch 64), batch 213836, train_loss = 0.949, time/batch = 0.092\n",
      "Sequence 2098568/3263200 (epoch 64), batch 213936, train_loss = 1.035, time/batch = 0.094\n",
      "Sequence 2099528/3263200 (epoch 64), batch 214036, train_loss = 0.979, time/batch = 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2100518/3263200 (epoch 64), batch 214136, train_loss = 1.110, time/batch = 0.093\n",
      "Sequence 2101498/3263200 (epoch 64), batch 214236, train_loss = 1.120, time/batch = 0.093\n",
      "Sequence 2102458/3263200 (epoch 64), batch 214336, train_loss = 1.708, time/batch = 0.093\n",
      "Sequence 2103448/3263200 (epoch 64), batch 214436, train_loss = 0.906, time/batch = 0.092\n",
      "Sequence 2104438/3263200 (epoch 64), batch 214536, train_loss = 1.368, time/batch = 0.092\n",
      "Sequence 2105418/3263200 (epoch 64), batch 214636, train_loss = 1.195, time/batch = 0.100\n",
      "Sequence 2106388/3263200 (epoch 64), batch 214736, train_loss = 1.149, time/batch = 0.103\n",
      "Sequence 2107368/3263200 (epoch 64), batch 214836, train_loss = 1.525, time/batch = 0.100\n",
      "Sequence 2108328/3263200 (epoch 64), batch 214936, train_loss = 1.585, time/batch = 0.102\n",
      "Sequence 2109288/3263200 (epoch 64), batch 215036, train_loss = 0.745, time/batch = 0.103\n",
      "Sequence 2110288/3263200 (epoch 64), batch 215136, train_loss = 1.250, time/batch = 0.094\n",
      "Sequence 2111248/3263200 (epoch 64), batch 215236, train_loss = 0.986, time/batch = 0.092\n",
      "Sequence 2112228/3263200 (epoch 64), batch 215336, train_loss = 1.480, time/batch = 0.093\n",
      "Sequence 2113208/3263200 (epoch 64), batch 215436, train_loss = 1.005, time/batch = 0.094\n",
      "Sequence 2114188/3263200 (epoch 64), batch 215536, train_loss = 1.117, time/batch = 0.101\n",
      "Sequence 2115168/3263200 (epoch 64), batch 215636, train_loss = 1.552, time/batch = 0.091\n",
      "Sequence 2116158/3263200 (epoch 64), batch 215736, train_loss = 1.014, time/batch = 0.093\n",
      "Sequence 2117148/3263200 (epoch 64), batch 215836, train_loss = 1.176, time/batch = 0.094\n",
      "Sequence 2118138/3263200 (epoch 64), batch 215936, train_loss = 1.069, time/batch = 0.092\n",
      "Sequence 2119128/3263200 (epoch 64), batch 216036, train_loss = 0.995, time/batch = 0.093\n",
      "Sequence 2120128/3263200 (epoch 64), batch 216136, train_loss = 1.004, time/batch = 0.092\n",
      "Epoch 64 completed, average train loss 1.210006, learning rate 0.0010\n",
      "model saved.\n",
      "Sequence 2121110/3263200 (epoch 65), batch 216236, train_loss = 1.255, time/batch = 0.096\n",
      "Shuffling training data...\n",
      "Sequence 2122100/3263200 (epoch 65), batch 216336, train_loss = 1.161, time/batch = 0.093\n",
      "Sequence 2123080/3263200 (epoch 65), batch 216436, train_loss = 1.277, time/batch = 0.093\n",
      "Sequence 2124050/3263200 (epoch 65), batch 216536, train_loss = 1.152, time/batch = 0.093\n",
      "Sequence 2125030/3263200 (epoch 65), batch 216636, train_loss = 0.847, time/batch = 0.092\n",
      "Sequence 2126010/3263200 (epoch 65), batch 216736, train_loss = 1.117, time/batch = 0.092\n",
      "Sequence 2127010/3263200 (epoch 65), batch 216836, train_loss = 1.755, time/batch = 0.092\n",
      "Sequence 2128000/3263200 (epoch 65), batch 216936, train_loss = 1.515, time/batch = 0.093\n",
      "Sequence 2128990/3263200 (epoch 65), batch 217036, train_loss = 1.018, time/batch = 0.092\n",
      "Sequence 2129980/3263200 (epoch 65), batch 217136, train_loss = 0.629, time/batch = 0.093\n",
      "Sequence 2130970/3263200 (epoch 65), batch 217236, train_loss = 1.188, time/batch = 0.093\n",
      "Sequence 2131930/3263200 (epoch 65), batch 217336, train_loss = 0.931, time/batch = 0.092\n",
      "Sequence 2132920/3263200 (epoch 65), batch 217436, train_loss = 0.991, time/batch = 0.092\n",
      "Sequence 2133910/3263200 (epoch 65), batch 217536, train_loss = 1.473, time/batch = 0.092\n",
      "Sequence 2134880/3263200 (epoch 65), batch 217636, train_loss = 0.646, time/batch = 0.093\n",
      "Sequence 2135860/3263200 (epoch 65), batch 217736, train_loss = 1.215, time/batch = 0.093\n",
      "Sequence 2136830/3263200 (epoch 65), batch 217836, train_loss = 0.764, time/batch = 0.092\n",
      "Sequence 2137820/3263200 (epoch 65), batch 217936, train_loss = 0.817, time/batch = 0.093\n",
      "Sequence 2138790/3263200 (epoch 65), batch 218036, train_loss = 1.614, time/batch = 0.092\n",
      "Sequence 2139770/3263200 (epoch 65), batch 218136, train_loss = 1.250, time/batch = 0.093\n",
      "Sequence 2140750/3263200 (epoch 65), batch 218236, train_loss = 1.319, time/batch = 0.093\n",
      "Sequence 2141730/3263200 (epoch 65), batch 218336, train_loss = 1.592, time/batch = 0.093\n",
      "Sequence 2142700/3263200 (epoch 65), batch 218436, train_loss = 0.829, time/batch = 0.092\n",
      "Sequence 2143700/3263200 (epoch 65), batch 218536, train_loss = 1.398, time/batch = 0.093\n",
      "Sequence 2144690/3263200 (epoch 65), batch 218636, train_loss = 1.045, time/batch = 0.093\n",
      "Sequence 2145670/3263200 (epoch 65), batch 218736, train_loss = 1.844, time/batch = 0.091\n",
      "Sequence 2146640/3263200 (epoch 65), batch 218836, train_loss = 1.048, time/batch = 0.093\n",
      "Sequence 2147630/3263200 (epoch 65), batch 218936, train_loss = 1.654, time/batch = 0.094\n",
      "Sequence 2148590/3263200 (epoch 65), batch 219036, train_loss = 0.826, time/batch = 0.094\n",
      "Sequence 2149570/3263200 (epoch 65), batch 219136, train_loss = 0.949, time/batch = 0.092\n",
      "Sequence 2150550/3263200 (epoch 65), batch 219236, train_loss = 2.274, time/batch = 0.093\n",
      "Sequence 2151530/3263200 (epoch 65), batch 219336, train_loss = 0.809, time/batch = 0.094\n",
      "Sequence 2152520/3263200 (epoch 65), batch 219436, train_loss = 1.488, time/batch = 0.093\n",
      "Sequence 2153490/3263200 (epoch 65), batch 219536, train_loss = 1.136, time/batch = 0.093\n",
      "Epoch 65 completed, average train loss 1.206213, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2154472/3263200 (epoch 66), batch 219636, train_loss = 0.867, time/batch = 0.094\n",
      "Sequence 2155462/3263200 (epoch 66), batch 219736, train_loss = 1.753, time/batch = 0.093\n",
      "Sequence 2156432/3263200 (epoch 66), batch 219836, train_loss = 0.958, time/batch = 0.092\n",
      "Sequence 2157402/3263200 (epoch 66), batch 219936, train_loss = 0.865, time/batch = 0.094\n",
      "Sequence 2158382/3263200 (epoch 66), batch 220036, train_loss = 1.194, time/batch = 0.094\n",
      "Sequence 2159362/3263200 (epoch 66), batch 220136, train_loss = 1.586, time/batch = 0.092\n",
      "Sequence 2160342/3263200 (epoch 66), batch 220236, train_loss = 1.132, time/batch = 0.093\n",
      "Sequence 2161322/3263200 (epoch 66), batch 220336, train_loss = 1.224, time/batch = 0.093\n",
      "Sequence 2162312/3263200 (epoch 66), batch 220436, train_loss = 0.953, time/batch = 0.092\n",
      "Sequence 2163302/3263200 (epoch 66), batch 220536, train_loss = 1.374, time/batch = 0.092\n",
      "Sequence 2164292/3263200 (epoch 66), batch 220636, train_loss = 1.736, time/batch = 0.093\n",
      "Sequence 2165282/3263200 (epoch 66), batch 220736, train_loss = 1.207, time/batch = 0.092\n",
      "Sequence 2166232/3263200 (epoch 66), batch 220836, train_loss = 0.847, time/batch = 0.093\n",
      "Sequence 2167212/3263200 (epoch 66), batch 220936, train_loss = 0.933, time/batch = 0.093\n",
      "Sequence 2168172/3263200 (epoch 66), batch 221036, train_loss = 0.903, time/batch = 0.093\n",
      "Sequence 2169162/3263200 (epoch 66), batch 221136, train_loss = 1.518, time/batch = 0.093\n",
      "Sequence 2170152/3263200 (epoch 66), batch 221236, train_loss = 1.214, time/batch = 0.094\n",
      "Sequence 2171152/3263200 (epoch 66), batch 221336, train_loss = 0.976, time/batch = 0.089\n",
      "Sequence 2172132/3263200 (epoch 66), batch 221436, train_loss = 1.379, time/batch = 0.092\n",
      "Sequence 2173102/3263200 (epoch 66), batch 221536, train_loss = 1.767, time/batch = 0.094\n",
      "Sequence 2174082/3263200 (epoch 66), batch 221636, train_loss = 0.922, time/batch = 0.092\n",
      "Sequence 2175072/3263200 (epoch 66), batch 221736, train_loss = 1.366, time/batch = 0.092\n",
      "Sequence 2176032/3263200 (epoch 66), batch 221836, train_loss = 1.156, time/batch = 0.093\n",
      "Sequence 2177022/3263200 (epoch 66), batch 221936, train_loss = 1.148, time/batch = 0.093\n",
      "Sequence 2178002/3263200 (epoch 66), batch 222036, train_loss = 0.992, time/batch = 0.092\n",
      "Sequence 2178982/3263200 (epoch 66), batch 222136, train_loss = 1.375, time/batch = 0.093\n",
      "Sequence 2179972/3263200 (epoch 66), batch 222236, train_loss = 1.700, time/batch = 0.092\n",
      "Sequence 2180962/3263200 (epoch 66), batch 222336, train_loss = 0.993, time/batch = 0.092\n",
      "Sequence 2181942/3263200 (epoch 66), batch 222436, train_loss = 1.650, time/batch = 0.092\n",
      "Sequence 2182912/3263200 (epoch 66), batch 222536, train_loss = 1.186, time/batch = 0.093\n",
      "Sequence 2183892/3263200 (epoch 66), batch 222636, train_loss = 1.805, time/batch = 0.092\n",
      "Sequence 2184862/3263200 (epoch 66), batch 222737, train_loss = 0.435, time/batch = 0.092\n",
      "Sequence 2185852/3263200 (epoch 66), batch 222837, train_loss = 1.249, time/batch = 0.094\n",
      "Epoch 66 completed, average train loss 1.203508, learning rate 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling training data...\n",
      "Sequence 2186844/3263200 (epoch 67), batch 222937, train_loss = 1.033, time/batch = 0.094\n",
      "Sequence 2187834/3263200 (epoch 67), batch 223037, train_loss = 1.276, time/batch = 0.093\n",
      "Sequence 2188814/3263200 (epoch 67), batch 223137, train_loss = 0.696, time/batch = 0.093\n",
      "Sequence 2189814/3263200 (epoch 67), batch 223237, train_loss = 0.749, time/batch = 0.093\n",
      "Sequence 2190794/3263200 (epoch 67), batch 223337, train_loss = 1.037, time/batch = 0.094\n",
      "Sequence 2191774/3263200 (epoch 67), batch 223437, train_loss = 1.000, time/batch = 0.089\n",
      "Sequence 2192754/3263200 (epoch 67), batch 223537, train_loss = 0.832, time/batch = 0.088\n",
      "Sequence 2193754/3263200 (epoch 67), batch 223637, train_loss = 1.618, time/batch = 0.093\n",
      "Sequence 2194694/3263200 (epoch 67), batch 223737, train_loss = 1.026, time/batch = 0.093\n",
      "Sequence 2195634/3263200 (epoch 67), batch 223837, train_loss = 1.243, time/batch = 0.092\n",
      "Sequence 2196604/3263200 (epoch 67), batch 223937, train_loss = 1.017, time/batch = 0.092\n",
      "Sequence 2197584/3263200 (epoch 67), batch 224037, train_loss = 1.053, time/batch = 0.091\n",
      "Sequence 2198564/3263200 (epoch 67), batch 224137, train_loss = 1.390, time/batch = 0.092\n",
      "Sequence 2199554/3263200 (epoch 67), batch 224237, train_loss = 1.633, time/batch = 0.094\n",
      "Sequence 2200554/3263200 (epoch 67), batch 224337, train_loss = 1.199, time/batch = 0.094\n",
      "Sequence 2201534/3263200 (epoch 67), batch 224437, train_loss = 0.899, time/batch = 0.093\n",
      "Sequence 2202504/3263200 (epoch 67), batch 224537, train_loss = 0.995, time/batch = 0.092\n",
      "Sequence 2203494/3263200 (epoch 67), batch 224637, train_loss = 2.001, time/batch = 0.092\n",
      "Sequence 2204454/3263200 (epoch 67), batch 224737, train_loss = 1.494, time/batch = 0.093\n",
      "Sequence 2205424/3263200 (epoch 67), batch 224837, train_loss = 0.967, time/batch = 0.093\n",
      "Sequence 2206414/3263200 (epoch 67), batch 224937, train_loss = 1.276, time/batch = 0.093\n",
      "Sequence 2207374/3263200 (epoch 67), batch 225037, train_loss = 1.056, time/batch = 0.092\n",
      "Sequence 2208364/3263200 (epoch 67), batch 225137, train_loss = 1.359, time/batch = 0.095\n",
      "Sequence 2209344/3263200 (epoch 67), batch 225237, train_loss = 1.245, time/batch = 0.093\n",
      "Sequence 2210334/3263200 (epoch 67), batch 225337, train_loss = 1.392, time/batch = 0.092\n",
      "Sequence 2211324/3263200 (epoch 67), batch 225437, train_loss = 0.581, time/batch = 0.093\n",
      "Sequence 2212274/3263200 (epoch 67), batch 225537, train_loss = 0.950, time/batch = 0.093\n",
      "Sequence 2213264/3263200 (epoch 67), batch 225637, train_loss = 1.629, time/batch = 0.093\n",
      "Sequence 2214264/3263200 (epoch 67), batch 225737, train_loss = 0.789, time/batch = 0.093\n",
      "Sequence 2215244/3263200 (epoch 67), batch 225837, train_loss = 1.437, time/batch = 0.093\n",
      "Sequence 2216214/3263200 (epoch 67), batch 225937, train_loss = 0.785, time/batch = 0.092\n",
      "Sequence 2217204/3263200 (epoch 67), batch 226037, train_loss = 0.967, time/batch = 0.094\n",
      "Sequence 2218204/3263200 (epoch 67), batch 226137, train_loss = 1.047, time/batch = 0.093\n",
      "Epoch 67 completed, average train loss 1.202032, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2219196/3263200 (epoch 68), batch 226237, train_loss = 1.312, time/batch = 0.092\n",
      "Sequence 2220176/3263200 (epoch 68), batch 226337, train_loss = 1.000, time/batch = 0.093\n",
      "Sequence 2221166/3263200 (epoch 68), batch 226437, train_loss = 0.879, time/batch = 0.093\n",
      "Sequence 2222136/3263200 (epoch 68), batch 226537, train_loss = 1.304, time/batch = 0.093\n",
      "Sequence 2223126/3263200 (epoch 68), batch 226637, train_loss = 0.957, time/batch = 0.092\n",
      "Sequence 2224096/3263200 (epoch 68), batch 226737, train_loss = 1.608, time/batch = 0.093\n",
      "Sequence 2225076/3263200 (epoch 68), batch 226837, train_loss = 1.364, time/batch = 0.092\n",
      "Sequence 2226066/3263200 (epoch 68), batch 226937, train_loss = 0.959, time/batch = 0.093\n",
      "Sequence 2227066/3263200 (epoch 68), batch 227037, train_loss = 1.350, time/batch = 0.093\n",
      "Sequence 2228026/3263200 (epoch 68), batch 227137, train_loss = 1.310, time/batch = 0.093\n",
      "Sequence 2229016/3263200 (epoch 68), batch 227237, train_loss = 1.721, time/batch = 0.092\n",
      "Sequence 2229986/3263200 (epoch 68), batch 227337, train_loss = 1.015, time/batch = 0.093\n",
      "Sequence 2230986/3263200 (epoch 68), batch 227437, train_loss = 1.042, time/batch = 0.093\n",
      "Sequence 2231976/3263200 (epoch 68), batch 227537, train_loss = 1.255, time/batch = 0.093\n",
      "Sequence 2232976/3263200 (epoch 68), batch 227637, train_loss = 1.324, time/batch = 0.092\n",
      "Sequence 2233936/3263200 (epoch 68), batch 227737, train_loss = 1.408, time/batch = 0.091\n",
      "Sequence 2234926/3263200 (epoch 68), batch 227837, train_loss = 0.908, time/batch = 0.094\n",
      "Sequence 2235916/3263200 (epoch 68), batch 227937, train_loss = 1.035, time/batch = 0.092\n",
      "Sequence 2236896/3263200 (epoch 68), batch 228037, train_loss = 1.282, time/batch = 0.094\n",
      "Sequence 2237896/3263200 (epoch 68), batch 228137, train_loss = 1.325, time/batch = 0.093\n",
      "Sequence 2238886/3263200 (epoch 68), batch 228237, train_loss = 1.360, time/batch = 0.094\n",
      "Sequence 2239856/3263200 (epoch 68), batch 228337, train_loss = 1.155, time/batch = 0.093\n",
      "Sequence 2240816/3263200 (epoch 68), batch 228437, train_loss = 0.837, time/batch = 0.093\n",
      "Sequence 2241806/3263200 (epoch 68), batch 228537, train_loss = 0.962, time/batch = 0.093\n",
      "Sequence 2242766/3263200 (epoch 68), batch 228637, train_loss = 2.161, time/batch = 0.092\n",
      "Sequence 2243766/3263200 (epoch 68), batch 228737, train_loss = 1.530, time/batch = 0.092\n",
      "Sequence 2244756/3263200 (epoch 68), batch 228837, train_loss = 1.739, time/batch = 0.093\n",
      "Sequence 2245716/3263200 (epoch 68), batch 228937, train_loss = 1.134, time/batch = 0.094\n",
      "Sequence 2246706/3263200 (epoch 68), batch 229037, train_loss = 1.291, time/batch = 0.093\n",
      "Sequence 2247686/3263200 (epoch 68), batch 229137, train_loss = 1.214, time/batch = 0.094\n",
      "Sequence 2248656/3263200 (epoch 68), batch 229237, train_loss = 1.420, time/batch = 0.094\n",
      "Sequence 2249636/3263200 (epoch 68), batch 229337, train_loss = 1.343, time/batch = 0.092\n",
      "Sequence 2250606/3263200 (epoch 68), batch 229437, train_loss = 1.504, time/batch = 0.092\n",
      "Sequence 2251586/3263200 (epoch 68), batch 229537, train_loss = 1.780, time/batch = 0.093\n",
      "Epoch 68 completed, average train loss 1.199169, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2252578/3263200 (epoch 69), batch 229637, train_loss = 1.139, time/batch = 0.093\n",
      "Sequence 2253558/3263200 (epoch 69), batch 229737, train_loss = 1.340, time/batch = 0.092\n",
      "Sequence 2254548/3263200 (epoch 69), batch 229837, train_loss = 1.585, time/batch = 0.093\n",
      "Sequence 2255528/3263200 (epoch 69), batch 229937, train_loss = 1.103, time/batch = 0.092\n",
      "Sequence 2256518/3263200 (epoch 69), batch 230037, train_loss = 1.090, time/batch = 0.093\n",
      "Sequence 2257508/3263200 (epoch 69), batch 230137, train_loss = 1.211, time/batch = 0.093\n",
      "Sequence 2258498/3263200 (epoch 69), batch 230237, train_loss = 1.543, time/batch = 0.093\n",
      "Sequence 2259488/3263200 (epoch 69), batch 230337, train_loss = 0.953, time/batch = 0.092\n",
      "Sequence 2260468/3263200 (epoch 69), batch 230438, train_loss = 0.790, time/batch = 0.093\n",
      "Sequence 2261448/3263200 (epoch 69), batch 230538, train_loss = 1.110, time/batch = 0.091\n",
      "Sequence 2262408/3263200 (epoch 69), batch 230638, train_loss = 1.001, time/batch = 0.093\n",
      "Sequence 2263378/3263200 (epoch 69), batch 230738, train_loss = 1.228, time/batch = 0.093\n",
      "Sequence 2264378/3263200 (epoch 69), batch 230838, train_loss = 1.082, time/batch = 0.093\n",
      "Sequence 2265338/3263200 (epoch 69), batch 230938, train_loss = 1.828, time/batch = 0.105\n",
      "Sequence 2266298/3263200 (epoch 69), batch 231038, train_loss = 1.231, time/batch = 0.100\n",
      "Sequence 2267268/3263200 (epoch 69), batch 231138, train_loss = 1.025, time/batch = 0.097\n",
      "Sequence 2268248/3263200 (epoch 69), batch 231238, train_loss = 1.170, time/batch = 0.093\n",
      "Sequence 2269218/3263200 (epoch 69), batch 231338, train_loss = 0.908, time/batch = 0.093\n",
      "Sequence 2270208/3263200 (epoch 69), batch 231438, train_loss = 1.150, time/batch = 0.093\n",
      "Sequence 2271178/3263200 (epoch 69), batch 231538, train_loss = 0.828, time/batch = 0.094\n",
      "Sequence 2272178/3263200 (epoch 69), batch 231638, train_loss = 0.908, time/batch = 0.094\n",
      "Sequence 2273148/3263200 (epoch 69), batch 231738, train_loss = 1.455, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2274138/3263200 (epoch 69), batch 231838, train_loss = 1.094, time/batch = 0.092\n",
      "Sequence 2275098/3263200 (epoch 69), batch 231938, train_loss = 1.327, time/batch = 0.093\n",
      "Sequence 2276068/3263200 (epoch 69), batch 232038, train_loss = 0.908, time/batch = 0.092\n",
      "Sequence 2277058/3263200 (epoch 69), batch 232138, train_loss = 1.067, time/batch = 0.091\n",
      "Sequence 2278038/3263200 (epoch 69), batch 232238, train_loss = 0.840, time/batch = 0.103\n",
      "Sequence 2278998/3263200 (epoch 69), batch 232338, train_loss = 0.855, time/batch = 0.102\n",
      "Sequence 2279998/3263200 (epoch 69), batch 232438, train_loss = 1.833, time/batch = 0.096\n",
      "Sequence 2280988/3263200 (epoch 69), batch 232538, train_loss = 1.646, time/batch = 0.093\n",
      "Sequence 2281978/3263200 (epoch 69), batch 232638, train_loss = 0.950, time/batch = 0.094\n",
      "Sequence 2282958/3263200 (epoch 69), batch 232738, train_loss = 1.463, time/batch = 0.092\n",
      "Sequence 2283958/3263200 (epoch 69), batch 232838, train_loss = 0.904, time/batch = 0.093\n",
      "Epoch 69 completed, average train loss 1.195721, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 2284940/3263200 (epoch 70), batch 232938, train_loss = 0.810, time/batch = 0.095\n",
      "Sequence 2285940/3263200 (epoch 70), batch 233038, train_loss = 0.936, time/batch = 0.093\n",
      "Sequence 2286920/3263200 (epoch 70), batch 233138, train_loss = 1.121, time/batch = 0.093\n",
      "Sequence 2287890/3263200 (epoch 70), batch 233238, train_loss = 0.840, time/batch = 0.092\n",
      "Sequence 2288880/3263200 (epoch 70), batch 233338, train_loss = 1.326, time/batch = 0.094\n",
      "Sequence 2289850/3263200 (epoch 70), batch 233438, train_loss = 0.873, time/batch = 0.092\n",
      "Sequence 2290840/3263200 (epoch 70), batch 233538, train_loss = 0.804, time/batch = 0.092\n",
      "Sequence 2291840/3263200 (epoch 70), batch 233638, train_loss = 1.433, time/batch = 0.092\n",
      "Sequence 2292840/3263200 (epoch 70), batch 233738, train_loss = 1.701, time/batch = 0.092\n",
      "Sequence 2293800/3263200 (epoch 70), batch 233838, train_loss = 1.396, time/batch = 0.092\n",
      "Sequence 2294780/3263200 (epoch 70), batch 233938, train_loss = 1.316, time/batch = 0.091\n",
      "Sequence 2295770/3263200 (epoch 70), batch 234038, train_loss = 0.837, time/batch = 0.093\n",
      "Sequence 2296760/3263200 (epoch 70), batch 234138, train_loss = 1.482, time/batch = 0.093\n",
      "Sequence 2297740/3263200 (epoch 70), batch 234238, train_loss = 0.919, time/batch = 0.092\n",
      "Sequence 2298720/3263200 (epoch 70), batch 234338, train_loss = 1.426, time/batch = 0.093\n",
      "Sequence 2299710/3263200 (epoch 70), batch 234438, train_loss = 1.397, time/batch = 0.092\n",
      "Sequence 2300700/3263200 (epoch 70), batch 234538, train_loss = 0.975, time/batch = 0.093\n",
      "Sequence 2301700/3263200 (epoch 70), batch 234638, train_loss = 0.924, time/batch = 0.104\n",
      "Sequence 2302700/3263200 (epoch 70), batch 234738, train_loss = 1.397, time/batch = 0.104\n",
      "Sequence 2303660/3263200 (epoch 70), batch 234838, train_loss = 0.815, time/batch = 0.097\n",
      "Sequence 2304620/3263200 (epoch 70), batch 234938, train_loss = 1.312, time/batch = 0.093\n",
      "Sequence 2305610/3263200 (epoch 70), batch 235038, train_loss = 1.594, time/batch = 0.093\n",
      "Sequence 2306600/3263200 (epoch 70), batch 235138, train_loss = 0.709, time/batch = 0.090\n",
      "Sequence 2307570/3263200 (epoch 70), batch 235238, train_loss = 1.084, time/batch = 0.094\n",
      "Sequence 2308540/3263200 (epoch 70), batch 235338, train_loss = 0.854, time/batch = 0.092\n",
      "Sequence 2309520/3263200 (epoch 70), batch 235438, train_loss = 0.746, time/batch = 0.093\n",
      "Sequence 2310510/3263200 (epoch 70), batch 235538, train_loss = 1.138, time/batch = 0.093\n",
      "Sequence 2311470/3263200 (epoch 70), batch 235638, train_loss = 1.679, time/batch = 0.092\n",
      "Sequence 2312440/3263200 (epoch 70), batch 235738, train_loss = 0.814, time/batch = 0.092\n",
      "Sequence 2313420/3263200 (epoch 70), batch 235838, train_loss = 1.301, time/batch = 0.092\n",
      "Sequence 2314400/3263200 (epoch 70), batch 235938, train_loss = 1.121, time/batch = 0.093\n",
      "Sequence 2315360/3263200 (epoch 70), batch 236038, train_loss = 0.948, time/batch = 0.094\n",
      "Sequence 2316340/3263200 (epoch 70), batch 236138, train_loss = 1.549, time/batch = 0.093\n",
      "Epoch 70 completed, average train loss 1.191980, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2317322/3263200 (epoch 71), batch 236238, train_loss = 1.869, time/batch = 0.094\n",
      "Sequence 2318292/3263200 (epoch 71), batch 236338, train_loss = 0.648, time/batch = 0.093\n",
      "Sequence 2319282/3263200 (epoch 71), batch 236438, train_loss = 1.055, time/batch = 0.091\n",
      "Sequence 2320272/3263200 (epoch 71), batch 236538, train_loss = 1.243, time/batch = 0.093\n",
      "Sequence 2321232/3263200 (epoch 71), batch 236638, train_loss = 1.338, time/batch = 0.093\n",
      "Sequence 2322222/3263200 (epoch 71), batch 236738, train_loss = 0.715, time/batch = 0.093\n",
      "Sequence 2323192/3263200 (epoch 71), batch 236838, train_loss = 1.623, time/batch = 0.093\n",
      "Sequence 2324162/3263200 (epoch 71), batch 236938, train_loss = 1.572, time/batch = 0.094\n",
      "Sequence 2325142/3263200 (epoch 71), batch 237038, train_loss = 1.765, time/batch = 0.092\n",
      "Sequence 2326132/3263200 (epoch 71), batch 237138, train_loss = 1.480, time/batch = 0.093\n",
      "Sequence 2327122/3263200 (epoch 71), batch 237238, train_loss = 0.917, time/batch = 0.094\n",
      "Sequence 2328122/3263200 (epoch 71), batch 237338, train_loss = 1.033, time/batch = 0.093\n",
      "Sequence 2329092/3263200 (epoch 71), batch 237438, train_loss = 0.933, time/batch = 0.092\n",
      "Sequence 2330062/3263200 (epoch 71), batch 237538, train_loss = 1.324, time/batch = 0.090\n",
      "Sequence 2331052/3263200 (epoch 71), batch 237638, train_loss = 1.214, time/batch = 0.093\n",
      "Sequence 2332052/3263200 (epoch 71), batch 237738, train_loss = 0.782, time/batch = 0.092\n",
      "Sequence 2333032/3263200 (epoch 71), batch 237838, train_loss = 1.149, time/batch = 0.093\n",
      "Sequence 2334022/3263200 (epoch 71), batch 237938, train_loss = 0.771, time/batch = 0.092\n",
      "Sequence 2335002/3263200 (epoch 71), batch 238038, train_loss = 0.864, time/batch = 0.094\n",
      "Sequence 2335982/3263200 (epoch 71), batch 238138, train_loss = 0.931, time/batch = 0.093\n",
      "Sequence 2336942/3263200 (epoch 71), batch 238238, train_loss = 1.281, time/batch = 0.094\n",
      "Sequence 2337922/3263200 (epoch 71), batch 238338, train_loss = 1.677, time/batch = 0.093\n",
      "Sequence 2338912/3263200 (epoch 71), batch 238438, train_loss = 1.349, time/batch = 0.093\n",
      "Sequence 2339892/3263200 (epoch 71), batch 238538, train_loss = 0.779, time/batch = 0.092\n",
      "Sequence 2340862/3263200 (epoch 71), batch 238638, train_loss = 1.424, time/batch = 0.092\n",
      "Sequence 2341842/3263200 (epoch 71), batch 238738, train_loss = 0.959, time/batch = 0.092\n",
      "Sequence 2342832/3263200 (epoch 71), batch 238838, train_loss = 1.304, time/batch = 0.091\n",
      "Sequence 2343802/3263200 (epoch 71), batch 238938, train_loss = 1.057, time/batch = 0.092\n",
      "Sequence 2344752/3263200 (epoch 71), batch 239038, train_loss = 1.287, time/batch = 0.092\n",
      "Sequence 2345742/3263200 (epoch 71), batch 239139, train_loss = 0.743, time/batch = 0.093\n",
      "Sequence 2346732/3263200 (epoch 71), batch 239239, train_loss = 1.213, time/batch = 0.093\n",
      "Sequence 2347722/3263200 (epoch 71), batch 239339, train_loss = 1.030, time/batch = 0.094\n",
      "Sequence 2348702/3263200 (epoch 71), batch 239439, train_loss = 1.490, time/batch = 0.092\n",
      "Epoch 71 completed, average train loss 1.191004, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2349694/3263200 (epoch 72), batch 239539, train_loss = 0.981, time/batch = 0.094\n",
      "Sequence 2350664/3263200 (epoch 72), batch 239639, train_loss = 1.049, time/batch = 0.094\n",
      "Sequence 2351634/3263200 (epoch 72), batch 239739, train_loss = 1.062, time/batch = 0.092\n",
      "Sequence 2352624/3263200 (epoch 72), batch 239839, train_loss = 1.006, time/batch = 0.092\n",
      "Sequence 2353614/3263200 (epoch 72), batch 239939, train_loss = 1.094, time/batch = 0.093\n",
      "Sequence 2354574/3263200 (epoch 72), batch 240039, train_loss = 1.185, time/batch = 0.091\n",
      "Sequence 2355554/3263200 (epoch 72), batch 240139, train_loss = 0.430, time/batch = 0.093\n",
      "Sequence 2356524/3263200 (epoch 72), batch 240239, train_loss = 1.661, time/batch = 0.094\n",
      "Sequence 2357504/3263200 (epoch 72), batch 240339, train_loss = 0.980, time/batch = 0.093\n",
      "Sequence 2358484/3263200 (epoch 72), batch 240439, train_loss = 1.450, time/batch = 0.094\n",
      "Sequence 2359424/3263200 (epoch 72), batch 240539, train_loss = 1.248, time/batch = 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2360424/3263200 (epoch 72), batch 240639, train_loss = 2.058, time/batch = 0.093\n",
      "Sequence 2361394/3263200 (epoch 72), batch 240739, train_loss = 0.975, time/batch = 0.094\n",
      "Sequence 2362384/3263200 (epoch 72), batch 240839, train_loss = 0.911, time/batch = 0.092\n",
      "Sequence 2363374/3263200 (epoch 72), batch 240940, train_loss = 0.876, time/batch = 0.095\n",
      "Sequence 2364344/3263200 (epoch 72), batch 241040, train_loss = 0.988, time/batch = 0.093\n",
      "Sequence 2365334/3263200 (epoch 72), batch 241140, train_loss = 0.933, time/batch = 0.092\n",
      "Sequence 2366314/3263200 (epoch 72), batch 241240, train_loss = 1.388, time/batch = 0.093\n",
      "Sequence 2367274/3263200 (epoch 72), batch 241340, train_loss = 0.790, time/batch = 0.092\n",
      "Sequence 2368264/3263200 (epoch 72), batch 241440, train_loss = 1.227, time/batch = 0.093\n",
      "Sequence 2369264/3263200 (epoch 72), batch 241540, train_loss = 1.203, time/batch = 0.092\n",
      "Sequence 2370244/3263200 (epoch 72), batch 241640, train_loss = 0.765, time/batch = 0.093\n",
      "Sequence 2371234/3263200 (epoch 72), batch 241740, train_loss = 1.254, time/batch = 0.093\n",
      "Sequence 2372234/3263200 (epoch 72), batch 241840, train_loss = 2.022, time/batch = 0.092\n",
      "Sequence 2373224/3263200 (epoch 72), batch 241940, train_loss = 1.002, time/batch = 0.092\n",
      "Sequence 2374204/3263200 (epoch 72), batch 242040, train_loss = 0.883, time/batch = 0.093\n",
      "Sequence 2375194/3263200 (epoch 72), batch 242140, train_loss = 1.401, time/batch = 0.094\n",
      "Sequence 2376174/3263200 (epoch 72), batch 242240, train_loss = 1.049, time/batch = 0.093\n",
      "Sequence 2377164/3263200 (epoch 72), batch 242340, train_loss = 0.945, time/batch = 0.092\n",
      "Sequence 2378134/3263200 (epoch 72), batch 242440, train_loss = 1.035, time/batch = 0.093\n",
      "Sequence 2379114/3263200 (epoch 72), batch 242540, train_loss = 0.906, time/batch = 0.092\n",
      "Sequence 2380074/3263200 (epoch 72), batch 242640, train_loss = 0.766, time/batch = 0.094\n",
      "Sequence 2381074/3263200 (epoch 72), batch 242740, train_loss = 0.984, time/batch = 0.091\n",
      "Sequence 2382064/3263200 (epoch 72), batch 242840, train_loss = 1.176, time/batch = 0.092\n",
      "Epoch 72 completed, average train loss 1.189332, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2383036/3263200 (epoch 73), batch 242940, train_loss = 0.634, time/batch = 0.093\n",
      "Sequence 2384026/3263200 (epoch 73), batch 243040, train_loss = 0.946, time/batch = 0.091\n",
      "Sequence 2384996/3263200 (epoch 73), batch 243140, train_loss = 0.983, time/batch = 0.092\n",
      "Sequence 2385986/3263200 (epoch 73), batch 243240, train_loss = 1.781, time/batch = 0.093\n",
      "Sequence 2386986/3263200 (epoch 73), batch 243340, train_loss = 1.011, time/batch = 0.092\n",
      "Sequence 2387976/3263200 (epoch 73), batch 243440, train_loss = 1.301, time/batch = 0.093\n",
      "Sequence 2388966/3263200 (epoch 73), batch 243540, train_loss = 0.861, time/batch = 0.092\n",
      "Sequence 2389956/3263200 (epoch 73), batch 243640, train_loss = 1.462, time/batch = 0.093\n",
      "Sequence 2390896/3263200 (epoch 73), batch 243740, train_loss = 0.958, time/batch = 0.093\n",
      "Sequence 2391876/3263200 (epoch 73), batch 243840, train_loss = 1.455, time/batch = 0.093\n",
      "Sequence 2392876/3263200 (epoch 73), batch 243940, train_loss = 1.385, time/batch = 0.091\n",
      "Sequence 2393866/3263200 (epoch 73), batch 244040, train_loss = 0.736, time/batch = 0.092\n",
      "Sequence 2394846/3263200 (epoch 73), batch 244140, train_loss = 1.049, time/batch = 0.093\n",
      "Sequence 2395806/3263200 (epoch 73), batch 244240, train_loss = 1.406, time/batch = 0.092\n",
      "Sequence 2396786/3263200 (epoch 73), batch 244340, train_loss = 0.746, time/batch = 0.094\n",
      "Sequence 2397776/3263200 (epoch 73), batch 244440, train_loss = 0.912, time/batch = 0.093\n",
      "Sequence 2398746/3263200 (epoch 73), batch 244540, train_loss = 1.436, time/batch = 0.093\n",
      "Sequence 2399736/3263200 (epoch 73), batch 244640, train_loss = 1.125, time/batch = 0.092\n",
      "Sequence 2400726/3263200 (epoch 73), batch 244740, train_loss = 1.407, time/batch = 0.092\n",
      "Sequence 2401706/3263200 (epoch 73), batch 244840, train_loss = 1.744, time/batch = 0.092\n",
      "Sequence 2402706/3263200 (epoch 73), batch 244940, train_loss = 1.312, time/batch = 0.092\n",
      "Sequence 2403666/3263200 (epoch 73), batch 245040, train_loss = 0.511, time/batch = 0.093\n",
      "Sequence 2404646/3263200 (epoch 73), batch 245140, train_loss = 1.512, time/batch = 0.093\n",
      "Sequence 2405636/3263200 (epoch 73), batch 245240, train_loss = 1.179, time/batch = 0.094\n",
      "Sequence 2406616/3263200 (epoch 73), batch 245340, train_loss = 0.744, time/batch = 0.104\n",
      "Sequence 2407586/3263200 (epoch 73), batch 245440, train_loss = 0.829, time/batch = 0.104\n",
      "Sequence 2408566/3263200 (epoch 73), batch 245540, train_loss = 0.861, time/batch = 0.102\n",
      "Sequence 2409556/3263200 (epoch 73), batch 245640, train_loss = 0.944, time/batch = 0.101\n",
      "Sequence 2410506/3263200 (epoch 73), batch 245740, train_loss = 1.406, time/batch = 0.105\n",
      "Sequence 2411506/3263200 (epoch 73), batch 245840, train_loss = 1.037, time/batch = 0.104\n",
      "Sequence 2412486/3263200 (epoch 73), batch 245940, train_loss = 0.644, time/batch = 0.093\n",
      "Sequence 2413476/3263200 (epoch 73), batch 246040, train_loss = 1.147, time/batch = 0.093\n",
      "Sequence 2414456/3263200 (epoch 73), batch 246140, train_loss = 0.869, time/batch = 0.093\n",
      "Epoch 73 completed, average train loss 1.185707, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2415428/3263200 (epoch 74), batch 246240, train_loss = 1.061, time/batch = 0.093\n",
      "Sequence 2416398/3263200 (epoch 74), batch 246340, train_loss = 1.038, time/batch = 0.092\n",
      "Sequence 2417378/3263200 (epoch 74), batch 246440, train_loss = 1.240, time/batch = 0.092\n",
      "Sequence 2418368/3263200 (epoch 74), batch 246540, train_loss = 0.854, time/batch = 0.092\n",
      "Sequence 2419368/3263200 (epoch 74), batch 246640, train_loss = 1.446, time/batch = 0.093\n",
      "Sequence 2420358/3263200 (epoch 74), batch 246740, train_loss = 1.277, time/batch = 0.093\n",
      "Sequence 2421338/3263200 (epoch 74), batch 246840, train_loss = 1.693, time/batch = 0.094\n",
      "Sequence 2422308/3263200 (epoch 74), batch 246940, train_loss = 1.628, time/batch = 0.092\n",
      "Sequence 2423308/3263200 (epoch 74), batch 247040, train_loss = 1.456, time/batch = 0.091\n",
      "Sequence 2424288/3263200 (epoch 74), batch 247140, train_loss = 1.600, time/batch = 0.094\n",
      "Sequence 2425278/3263200 (epoch 74), batch 247240, train_loss = 1.250, time/batch = 0.093\n",
      "Sequence 2426248/3263200 (epoch 74), batch 247340, train_loss = 1.759, time/batch = 0.093\n",
      "Sequence 2427238/3263200 (epoch 74), batch 247440, train_loss = 1.406, time/batch = 0.093\n",
      "Sequence 2428218/3263200 (epoch 74), batch 247540, train_loss = 0.574, time/batch = 0.092\n",
      "Sequence 2429178/3263200 (epoch 74), batch 247640, train_loss = 1.110, time/batch = 0.091\n",
      "Sequence 2430178/3263200 (epoch 74), batch 247740, train_loss = 1.228, time/batch = 0.091\n",
      "Sequence 2431158/3263200 (epoch 74), batch 247840, train_loss = 1.390, time/batch = 0.092\n",
      "Sequence 2432138/3263200 (epoch 74), batch 247940, train_loss = 0.873, time/batch = 0.093\n",
      "Sequence 2433128/3263200 (epoch 74), batch 248040, train_loss = 1.532, time/batch = 0.093\n",
      "Sequence 2434118/3263200 (epoch 74), batch 248140, train_loss = 0.985, time/batch = 0.093\n",
      "Sequence 2435068/3263200 (epoch 74), batch 248240, train_loss = 0.938, time/batch = 0.093\n",
      "Sequence 2436048/3263200 (epoch 74), batch 248340, train_loss = 1.426, time/batch = 0.093\n",
      "Sequence 2437048/3263200 (epoch 74), batch 248440, train_loss = 1.919, time/batch = 0.093\n",
      "Sequence 2438018/3263200 (epoch 74), batch 248540, train_loss = 1.154, time/batch = 0.092\n",
      "Sequence 2439008/3263200 (epoch 74), batch 248640, train_loss = 1.438, time/batch = 0.091\n",
      "Sequence 2439998/3263200 (epoch 74), batch 248740, train_loss = 1.009, time/batch = 0.092\n",
      "Sequence 2440958/3263200 (epoch 74), batch 248840, train_loss = 1.069, time/batch = 0.088\n",
      "Sequence 2441908/3263200 (epoch 74), batch 248940, train_loss = 1.241, time/batch = 0.092\n",
      "Sequence 2442878/3263200 (epoch 74), batch 249040, train_loss = 1.254, time/batch = 0.092\n",
      "Sequence 2443858/3263200 (epoch 74), batch 249140, train_loss = 1.858, time/batch = 0.093\n",
      "Sequence 2444818/3263200 (epoch 74), batch 249240, train_loss = 2.129, time/batch = 0.093\n",
      "Sequence 2445808/3263200 (epoch 74), batch 249340, train_loss = 1.615, time/batch = 0.092\n",
      "Sequence 2446788/3263200 (epoch 74), batch 249440, train_loss = 0.898, time/batch = 0.091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 completed, average train loss 1.183562, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 2447790/3263200 (epoch 75), batch 249540, train_loss = 1.428, time/batch = 0.094\n",
      "Sequence 2448750/3263200 (epoch 75), batch 249640, train_loss = 1.073, time/batch = 0.092\n",
      "Sequence 2449740/3263200 (epoch 75), batch 249740, train_loss = 1.087, time/batch = 0.092\n",
      "Sequence 2450740/3263200 (epoch 75), batch 249840, train_loss = 0.894, time/batch = 0.092\n",
      "Sequence 2451720/3263200 (epoch 75), batch 249940, train_loss = 1.195, time/batch = 0.093\n",
      "Sequence 2452710/3263200 (epoch 75), batch 250040, train_loss = 1.755, time/batch = 0.092\n",
      "Sequence 2453700/3263200 (epoch 75), batch 250140, train_loss = 1.388, time/batch = 0.094\n",
      "Sequence 2454680/3263200 (epoch 75), batch 250240, train_loss = 1.569, time/batch = 0.093\n",
      "Sequence 2455650/3263200 (epoch 75), batch 250340, train_loss = 1.161, time/batch = 0.092\n",
      "Sequence 2456640/3263200 (epoch 75), batch 250440, train_loss = 1.668, time/batch = 0.092\n",
      "Sequence 2457630/3263200 (epoch 75), batch 250540, train_loss = 1.535, time/batch = 0.092\n",
      "Sequence 2458620/3263200 (epoch 75), batch 250640, train_loss = 1.528, time/batch = 0.093\n",
      "Sequence 2459620/3263200 (epoch 75), batch 250740, train_loss = 1.219, time/batch = 0.093\n",
      "Sequence 2460580/3263200 (epoch 75), batch 250840, train_loss = 1.209, time/batch = 0.093\n",
      "Sequence 2461560/3263200 (epoch 75), batch 250940, train_loss = 1.425, time/batch = 0.091\n",
      "Sequence 2462560/3263200 (epoch 75), batch 251040, train_loss = 1.078, time/batch = 0.091\n",
      "Sequence 2463520/3263200 (epoch 75), batch 251140, train_loss = 1.186, time/batch = 0.092\n",
      "Sequence 2464490/3263200 (epoch 75), batch 251240, train_loss = 1.396, time/batch = 0.093\n",
      "Sequence 2465460/3263200 (epoch 75), batch 251340, train_loss = 1.162, time/batch = 0.094\n",
      "Sequence 2466450/3263200 (epoch 75), batch 251441, train_loss = 0.516, time/batch = 0.093\n",
      "Sequence 2467420/3263200 (epoch 75), batch 251541, train_loss = 1.225, time/batch = 0.092\n",
      "Sequence 2468380/3263200 (epoch 75), batch 251641, train_loss = 0.778, time/batch = 0.091\n",
      "Sequence 2469360/3263200 (epoch 75), batch 251741, train_loss = 0.939, time/batch = 0.093\n",
      "Sequence 2470350/3263200 (epoch 75), batch 251841, train_loss = 1.425, time/batch = 0.092\n",
      "Sequence 2471350/3263200 (epoch 75), batch 251941, train_loss = 0.837, time/batch = 0.092\n",
      "Sequence 2472320/3263200 (epoch 75), batch 252041, train_loss = 1.301, time/batch = 0.092\n",
      "Sequence 2473270/3263200 (epoch 75), batch 252141, train_loss = 1.162, time/batch = 0.092\n",
      "Sequence 2474260/3263200 (epoch 75), batch 252241, train_loss = 1.272, time/batch = 0.092\n",
      "Sequence 2475230/3263200 (epoch 75), batch 252341, train_loss = 0.724, time/batch = 0.093\n",
      "Sequence 2476220/3263200 (epoch 75), batch 252441, train_loss = 0.921, time/batch = 0.093\n",
      "Sequence 2477200/3263200 (epoch 75), batch 252541, train_loss = 1.325, time/batch = 0.093\n",
      "Sequence 2478180/3263200 (epoch 75), batch 252641, train_loss = 1.223, time/batch = 0.094\n",
      "Sequence 2479180/3263200 (epoch 75), batch 252741, train_loss = 0.817, time/batch = 0.092\n",
      "Epoch 75 completed, average train loss 1.182795, learning rate 0.0010\n",
      "Sequence 2480162/3263200 (epoch 76), batch 252841, train_loss = 1.023, time/batch = 0.094\n",
      "Shuffling training data...\n",
      "Sequence 2481152/3263200 (epoch 76), batch 252941, train_loss = 1.946, time/batch = 0.093\n",
      "Sequence 2482122/3263200 (epoch 76), batch 253041, train_loss = 1.251, time/batch = 0.093\n",
      "Sequence 2483122/3263200 (epoch 76), batch 253141, train_loss = 1.037, time/batch = 0.092\n",
      "Sequence 2484102/3263200 (epoch 76), batch 253241, train_loss = 1.050, time/batch = 0.091\n",
      "Sequence 2485092/3263200 (epoch 76), batch 253341, train_loss = 1.473, time/batch = 0.092\n",
      "Sequence 2486072/3263200 (epoch 76), batch 253441, train_loss = 1.277, time/batch = 0.091\n",
      "Sequence 2487052/3263200 (epoch 76), batch 253541, train_loss = 0.993, time/batch = 0.092\n",
      "Sequence 2488022/3263200 (epoch 76), batch 253641, train_loss = 1.118, time/batch = 0.094\n",
      "Sequence 2489002/3263200 (epoch 76), batch 253741, train_loss = 0.832, time/batch = 0.093\n",
      "Sequence 2490002/3263200 (epoch 76), batch 253841, train_loss = 0.985, time/batch = 0.092\n",
      "Sequence 2491002/3263200 (epoch 76), batch 253941, train_loss = 1.217, time/batch = 0.093\n",
      "Sequence 2491982/3263200 (epoch 76), batch 254041, train_loss = 0.847, time/batch = 0.092\n",
      "Sequence 2492942/3263200 (epoch 76), batch 254141, train_loss = 0.885, time/batch = 0.092\n",
      "Sequence 2493882/3263200 (epoch 76), batch 254241, train_loss = 1.060, time/batch = 0.093\n",
      "Sequence 2494842/3263200 (epoch 76), batch 254341, train_loss = 1.226, time/batch = 0.092\n",
      "Sequence 2495822/3263200 (epoch 76), batch 254441, train_loss = 0.974, time/batch = 0.092\n",
      "Sequence 2496812/3263200 (epoch 76), batch 254541, train_loss = 1.373, time/batch = 0.091\n",
      "Sequence 2497792/3263200 (epoch 76), batch 254641, train_loss = 1.396, time/batch = 0.091\n",
      "Sequence 2498752/3263200 (epoch 76), batch 254741, train_loss = 1.187, time/batch = 0.092\n",
      "Sequence 2499752/3263200 (epoch 76), batch 254841, train_loss = 1.532, time/batch = 0.092\n",
      "Sequence 2500722/3263200 (epoch 76), batch 254941, train_loss = 0.802, time/batch = 0.092\n",
      "Sequence 2501692/3263200 (epoch 76), batch 255041, train_loss = 0.685, time/batch = 0.092\n",
      "Sequence 2502672/3263200 (epoch 76), batch 255141, train_loss = 0.540, time/batch = 0.093\n",
      "Sequence 2503662/3263200 (epoch 76), batch 255241, train_loss = 1.357, time/batch = 0.092\n",
      "Sequence 2504652/3263200 (epoch 76), batch 255341, train_loss = 0.914, time/batch = 0.093\n",
      "Sequence 2505622/3263200 (epoch 76), batch 255441, train_loss = 1.148, time/batch = 0.090\n",
      "Sequence 2506612/3263200 (epoch 76), batch 255541, train_loss = 1.985, time/batch = 0.096\n",
      "Sequence 2507592/3263200 (epoch 76), batch 255641, train_loss = 1.238, time/batch = 0.103\n",
      "Sequence 2508572/3263200 (epoch 76), batch 255741, train_loss = 0.912, time/batch = 0.093\n",
      "Sequence 2509562/3263200 (epoch 76), batch 255841, train_loss = 1.618, time/batch = 0.093\n",
      "Sequence 2510562/3263200 (epoch 76), batch 255941, train_loss = 1.897, time/batch = 0.092\n",
      "Sequence 2511532/3263200 (epoch 76), batch 256041, train_loss = 1.468, time/batch = 0.093\n",
      "Sequence 2512522/3263200 (epoch 76), batch 256141, train_loss = 1.051, time/batch = 0.092\n",
      "Epoch 76 completed, average train loss 1.179799, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2513504/3263200 (epoch 77), batch 256241, train_loss = 0.979, time/batch = 0.093\n",
      "Sequence 2514484/3263200 (epoch 77), batch 256341, train_loss = 0.884, time/batch = 0.092\n",
      "Sequence 2515464/3263200 (epoch 77), batch 256441, train_loss = 1.372, time/batch = 0.093\n",
      "Sequence 2516464/3263200 (epoch 77), batch 256541, train_loss = 1.186, time/batch = 0.092\n",
      "Sequence 2517464/3263200 (epoch 77), batch 256641, train_loss = 0.838, time/batch = 0.093\n",
      "Sequence 2518454/3263200 (epoch 77), batch 256741, train_loss = 1.229, time/batch = 0.103\n",
      "Sequence 2519424/3263200 (epoch 77), batch 256841, train_loss = 1.196, time/batch = 0.101\n",
      "Sequence 2520414/3263200 (epoch 77), batch 256941, train_loss = 1.594, time/batch = 0.103\n",
      "Sequence 2521404/3263200 (epoch 77), batch 257041, train_loss = 2.141, time/batch = 0.101\n",
      "Sequence 2522384/3263200 (epoch 77), batch 257141, train_loss = 0.925, time/batch = 0.096\n",
      "Sequence 2523364/3263200 (epoch 77), batch 257241, train_loss = 0.547, time/batch = 0.093\n",
      "Sequence 2524354/3263200 (epoch 77), batch 257341, train_loss = 1.322, time/batch = 0.093\n",
      "Sequence 2525344/3263200 (epoch 77), batch 257441, train_loss = 1.108, time/batch = 0.093\n",
      "Sequence 2526344/3263200 (epoch 77), batch 257541, train_loss = 1.120, time/batch = 0.093\n",
      "Sequence 2527334/3263200 (epoch 77), batch 257641, train_loss = 1.351, time/batch = 0.092\n",
      "Sequence 2528304/3263200 (epoch 77), batch 257741, train_loss = 1.340, time/batch = 0.093\n",
      "Sequence 2529274/3263200 (epoch 77), batch 257841, train_loss = 0.680, time/batch = 0.093\n",
      "Sequence 2530274/3263200 (epoch 77), batch 257941, train_loss = 1.238, time/batch = 0.093\n",
      "Sequence 2531224/3263200 (epoch 77), batch 258041, train_loss = 0.820, time/batch = 0.092\n",
      "Sequence 2532174/3263200 (epoch 77), batch 258141, train_loss = 1.094, time/batch = 0.092\n",
      "Sequence 2533164/3263200 (epoch 77), batch 258241, train_loss = 1.304, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2534154/3263200 (epoch 77), batch 258341, train_loss = 1.094, time/batch = 0.094\n",
      "Sequence 2535124/3263200 (epoch 77), batch 258441, train_loss = 1.127, time/batch = 0.093\n",
      "Sequence 2536124/3263200 (epoch 77), batch 258541, train_loss = 1.615, time/batch = 0.092\n",
      "Sequence 2537064/3263200 (epoch 77), batch 258641, train_loss = 1.128, time/batch = 0.093\n",
      "Sequence 2538054/3263200 (epoch 77), batch 258741, train_loss = 0.873, time/batch = 0.092\n",
      "Sequence 2539024/3263200 (epoch 77), batch 258841, train_loss = 0.986, time/batch = 0.092\n",
      "Sequence 2540014/3263200 (epoch 77), batch 258941, train_loss = 1.294, time/batch = 0.092\n",
      "Sequence 2540974/3263200 (epoch 77), batch 259041, train_loss = 1.679, time/batch = 0.093\n",
      "Sequence 2541964/3263200 (epoch 77), batch 259141, train_loss = 1.374, time/batch = 0.092\n",
      "Sequence 2542934/3263200 (epoch 77), batch 259241, train_loss = 1.265, time/batch = 0.091\n",
      "Sequence 2543904/3263200 (epoch 77), batch 259341, train_loss = 1.131, time/batch = 0.092\n",
      "Sequence 2544884/3263200 (epoch 77), batch 259441, train_loss = 1.342, time/batch = 0.092\n",
      "Epoch 77 completed, average train loss 1.177120, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2545886/3263200 (epoch 78), batch 259541, train_loss = 0.932, time/batch = 0.093\n",
      "Sequence 2546886/3263200 (epoch 78), batch 259641, train_loss = 0.728, time/batch = 0.092\n",
      "Sequence 2547846/3263200 (epoch 78), batch 259741, train_loss = 0.991, time/batch = 0.092\n",
      "Sequence 2548826/3263200 (epoch 78), batch 259841, train_loss = 1.248, time/batch = 0.093\n",
      "Sequence 2549816/3263200 (epoch 78), batch 259941, train_loss = 1.291, time/batch = 0.093\n",
      "Sequence 2550806/3263200 (epoch 78), batch 260041, train_loss = 1.688, time/batch = 0.092\n",
      "Sequence 2551766/3263200 (epoch 78), batch 260141, train_loss = 1.102, time/batch = 0.091\n",
      "Sequence 2552746/3263200 (epoch 78), batch 260241, train_loss = 1.066, time/batch = 0.092\n",
      "Sequence 2553746/3263200 (epoch 78), batch 260341, train_loss = 1.141, time/batch = 0.092\n",
      "Sequence 2554726/3263200 (epoch 78), batch 260441, train_loss = 1.414, time/batch = 0.093\n",
      "Sequence 2555706/3263200 (epoch 78), batch 260541, train_loss = 0.870, time/batch = 0.093\n",
      "Sequence 2556696/3263200 (epoch 78), batch 260641, train_loss = 2.274, time/batch = 0.091\n",
      "Sequence 2557676/3263200 (epoch 78), batch 260741, train_loss = 1.167, time/batch = 0.092\n",
      "Sequence 2558676/3263200 (epoch 78), batch 260841, train_loss = 1.492, time/batch = 0.094\n",
      "Sequence 2559666/3263200 (epoch 78), batch 260941, train_loss = 0.960, time/batch = 0.093\n",
      "Sequence 2560636/3263200 (epoch 78), batch 261041, train_loss = 0.896, time/batch = 0.092\n",
      "Sequence 2561626/3263200 (epoch 78), batch 261141, train_loss = 0.966, time/batch = 0.094\n",
      "Sequence 2562616/3263200 (epoch 78), batch 261241, train_loss = 1.333, time/batch = 0.093\n",
      "Sequence 2563586/3263200 (epoch 78), batch 261341, train_loss = 1.292, time/batch = 0.094\n",
      "Sequence 2564586/3263200 (epoch 78), batch 261441, train_loss = 1.310, time/batch = 0.092\n",
      "Sequence 2565576/3263200 (epoch 78), batch 261541, train_loss = 1.149, time/batch = 0.092\n",
      "Sequence 2566566/3263200 (epoch 78), batch 261641, train_loss = 1.098, time/batch = 0.092\n",
      "Sequence 2567546/3263200 (epoch 78), batch 261741, train_loss = 1.099, time/batch = 0.093\n",
      "Sequence 2568516/3263200 (epoch 78), batch 261841, train_loss = 0.702, time/batch = 0.092\n",
      "Sequence 2569476/3263200 (epoch 78), batch 261941, train_loss = 1.313, time/batch = 0.092\n",
      "Sequence 2570456/3263200 (epoch 78), batch 262041, train_loss = 1.934, time/batch = 0.092\n",
      "Sequence 2571416/3263200 (epoch 78), batch 262141, train_loss = 1.365, time/batch = 0.093\n",
      "Sequence 2572396/3263200 (epoch 78), batch 262241, train_loss = 0.906, time/batch = 0.091\n",
      "Sequence 2573366/3263200 (epoch 78), batch 262341, train_loss = 0.993, time/batch = 0.093\n",
      "Sequence 2574356/3263200 (epoch 78), batch 262441, train_loss = 1.272, time/batch = 0.093\n",
      "Sequence 2575316/3263200 (epoch 78), batch 262541, train_loss = 1.200, time/batch = 0.092\n",
      "Sequence 2576296/3263200 (epoch 78), batch 262641, train_loss = 1.320, time/batch = 0.092\n",
      "Sequence 2577276/3263200 (epoch 78), batch 262741, train_loss = 1.035, time/batch = 0.093\n",
      "Epoch 78 completed, average train loss 1.175927, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2578248/3263200 (epoch 79), batch 262841, train_loss = 1.350, time/batch = 0.093\n",
      "Sequence 2579228/3263200 (epoch 79), batch 262941, train_loss = 1.855, time/batch = 0.093\n",
      "Sequence 2580178/3263200 (epoch 79), batch 263041, train_loss = 1.407, time/batch = 0.093\n",
      "Sequence 2581178/3263200 (epoch 79), batch 263141, train_loss = 0.960, time/batch = 0.092\n",
      "Sequence 2582138/3263200 (epoch 79), batch 263242, train_loss = 0.646, time/batch = 0.092\n",
      "Sequence 2583108/3263200 (epoch 79), batch 263342, train_loss = 2.285, time/batch = 0.093\n",
      "Sequence 2584098/3263200 (epoch 79), batch 263442, train_loss = 1.119, time/batch = 0.093\n",
      "Sequence 2585088/3263200 (epoch 79), batch 263542, train_loss = 1.084, time/batch = 0.092\n",
      "Sequence 2586048/3263200 (epoch 79), batch 263642, train_loss = 1.071, time/batch = 0.092\n",
      "Sequence 2587038/3263200 (epoch 79), batch 263742, train_loss = 1.144, time/batch = 0.093\n",
      "Sequence 2588018/3263200 (epoch 79), batch 263842, train_loss = 0.916, time/batch = 0.091\n",
      "Sequence 2589018/3263200 (epoch 79), batch 263942, train_loss = 1.164, time/batch = 0.093\n",
      "Sequence 2590008/3263200 (epoch 79), batch 264042, train_loss = 1.605, time/batch = 0.093\n",
      "Sequence 2590988/3263200 (epoch 79), batch 264142, train_loss = 2.015, time/batch = 0.094\n",
      "Sequence 2591928/3263200 (epoch 79), batch 264242, train_loss = 0.688, time/batch = 0.093\n",
      "Sequence 2592908/3263200 (epoch 79), batch 264342, train_loss = 1.678, time/batch = 0.092\n",
      "Sequence 2593908/3263200 (epoch 79), batch 264442, train_loss = 0.871, time/batch = 0.092\n",
      "Sequence 2594878/3263200 (epoch 79), batch 264542, train_loss = 1.254, time/batch = 0.092\n",
      "Sequence 2595858/3263200 (epoch 79), batch 264642, train_loss = 1.559, time/batch = 0.092\n",
      "Sequence 2596828/3263200 (epoch 79), batch 264742, train_loss = 1.077, time/batch = 0.093\n",
      "Sequence 2597808/3263200 (epoch 79), batch 264842, train_loss = 0.983, time/batch = 0.093\n",
      "Sequence 2598808/3263200 (epoch 79), batch 264942, train_loss = 1.287, time/batch = 0.093\n",
      "Sequence 2599788/3263200 (epoch 79), batch 265042, train_loss = 1.373, time/batch = 0.093\n",
      "Sequence 2600788/3263200 (epoch 79), batch 265142, train_loss = 0.720, time/batch = 0.094\n",
      "Sequence 2601768/3263200 (epoch 79), batch 265242, train_loss = 0.960, time/batch = 0.092\n",
      "Sequence 2602768/3263200 (epoch 79), batch 265342, train_loss = 0.889, time/batch = 0.092\n",
      "Sequence 2603748/3263200 (epoch 79), batch 265442, train_loss = 1.359, time/batch = 0.092\n",
      "Sequence 2604718/3263200 (epoch 79), batch 265542, train_loss = 1.803, time/batch = 0.093\n",
      "Sequence 2605708/3263200 (epoch 79), batch 265642, train_loss = 1.505, time/batch = 0.092\n",
      "Sequence 2606698/3263200 (epoch 79), batch 265742, train_loss = 0.843, time/batch = 0.093\n",
      "Sequence 2607678/3263200 (epoch 79), batch 265842, train_loss = 1.190, time/batch = 0.092\n",
      "Sequence 2608648/3263200 (epoch 79), batch 265942, train_loss = 0.721, time/batch = 0.089\n",
      "Sequence 2609638/3263200 (epoch 79), batch 266042, train_loss = 1.109, time/batch = 0.092\n",
      "Epoch 79 completed, average train loss 1.173440, learning rate 0.0010\n",
      "model saved.\n",
      "Sequence 2610620/3263200 (epoch 80), batch 266142, train_loss = 0.936, time/batch = 0.093\n",
      "Shuffling training data...\n",
      "Sequence 2611620/3263200 (epoch 80), batch 266243, train_loss = 0.657, time/batch = 0.093\n",
      "Sequence 2612610/3263200 (epoch 80), batch 266343, train_loss = 1.090, time/batch = 0.092\n",
      "Sequence 2613600/3263200 (epoch 80), batch 266443, train_loss = 0.869, time/batch = 0.092\n",
      "Sequence 2614560/3263200 (epoch 80), batch 266543, train_loss = 1.018, time/batch = 0.092\n",
      "Sequence 2615540/3263200 (epoch 80), batch 266643, train_loss = 1.000, time/batch = 0.093\n",
      "Sequence 2616510/3263200 (epoch 80), batch 266743, train_loss = 1.311, time/batch = 0.092\n",
      "Sequence 2617490/3263200 (epoch 80), batch 266843, train_loss = 0.980, time/batch = 0.093\n",
      "Sequence 2618480/3263200 (epoch 80), batch 266943, train_loss = 1.031, time/batch = 0.093\n",
      "Sequence 2619450/3263200 (epoch 80), batch 267043, train_loss = 1.904, time/batch = 0.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2620430/3263200 (epoch 80), batch 267143, train_loss = 1.346, time/batch = 0.093\n",
      "Sequence 2621400/3263200 (epoch 80), batch 267243, train_loss = 0.890, time/batch = 0.094\n",
      "Sequence 2622390/3263200 (epoch 80), batch 267343, train_loss = 1.356, time/batch = 0.093\n",
      "Sequence 2623370/3263200 (epoch 80), batch 267443, train_loss = 1.085, time/batch = 0.092\n",
      "Sequence 2624360/3263200 (epoch 80), batch 267543, train_loss = 1.869, time/batch = 0.093\n",
      "Sequence 2625350/3263200 (epoch 80), batch 267643, train_loss = 1.189, time/batch = 0.092\n",
      "Sequence 2626310/3263200 (epoch 80), batch 267743, train_loss = 0.533, time/batch = 0.091\n",
      "Sequence 2627310/3263200 (epoch 80), batch 267843, train_loss = 1.044, time/batch = 0.092\n",
      "Sequence 2628300/3263200 (epoch 80), batch 267943, train_loss = 1.068, time/batch = 0.094\n",
      "Sequence 2629300/3263200 (epoch 80), batch 268043, train_loss = 0.787, time/batch = 0.097\n",
      "Sequence 2630290/3263200 (epoch 80), batch 268143, train_loss = 1.865, time/batch = 0.096\n",
      "Sequence 2631280/3263200 (epoch 80), batch 268243, train_loss = 1.131, time/batch = 0.093\n",
      "Sequence 2632280/3263200 (epoch 80), batch 268343, train_loss = 1.031, time/batch = 0.092\n",
      "Sequence 2633250/3263200 (epoch 80), batch 268443, train_loss = 1.442, time/batch = 0.093\n",
      "Sequence 2634220/3263200 (epoch 80), batch 268543, train_loss = 1.425, time/batch = 0.092\n",
      "Sequence 2635190/3263200 (epoch 80), batch 268643, train_loss = 0.811, time/batch = 0.093\n",
      "Sequence 2636150/3263200 (epoch 80), batch 268743, train_loss = 0.878, time/batch = 0.093\n",
      "Sequence 2637120/3263200 (epoch 80), batch 268843, train_loss = 0.887, time/batch = 0.092\n",
      "Sequence 2638100/3263200 (epoch 80), batch 268943, train_loss = 1.086, time/batch = 0.092\n",
      "Sequence 2639090/3263200 (epoch 80), batch 269043, train_loss = 0.537, time/batch = 0.093\n",
      "Sequence 2640050/3263200 (epoch 80), batch 269143, train_loss = 0.717, time/batch = 0.093\n",
      "Sequence 2641050/3263200 (epoch 80), batch 269243, train_loss = 1.028, time/batch = 0.092\n",
      "Sequence 2642010/3263200 (epoch 80), batch 269343, train_loss = 0.912, time/batch = 0.094\n",
      "Sequence 2642990/3263200 (epoch 80), batch 269443, train_loss = 0.908, time/batch = 0.093\n",
      "Epoch 80 completed, average train loss 1.171929, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2643992/3263200 (epoch 81), batch 269543, train_loss = 1.229, time/batch = 0.093\n",
      "Sequence 2644992/3263200 (epoch 81), batch 269643, train_loss = 1.311, time/batch = 0.093\n",
      "Sequence 2645982/3263200 (epoch 81), batch 269743, train_loss = 1.066, time/batch = 0.092\n",
      "Sequence 2646982/3263200 (epoch 81), batch 269843, train_loss = 1.348, time/batch = 0.092\n",
      "Sequence 2647972/3263200 (epoch 81), batch 269943, train_loss = 1.789, time/batch = 0.093\n",
      "Sequence 2648952/3263200 (epoch 81), batch 270043, train_loss = 0.908, time/batch = 0.092\n",
      "Sequence 2649922/3263200 (epoch 81), batch 270143, train_loss = 1.559, time/batch = 0.094\n",
      "Sequence 2650912/3263200 (epoch 81), batch 270243, train_loss = 1.056, time/batch = 0.092\n",
      "Sequence 2651872/3263200 (epoch 81), batch 270343, train_loss = 0.992, time/batch = 0.093\n",
      "Sequence 2652842/3263200 (epoch 81), batch 270443, train_loss = 1.008, time/batch = 0.091\n",
      "Sequence 2653792/3263200 (epoch 81), batch 270543, train_loss = 1.713, time/batch = 0.092\n",
      "Sequence 2654772/3263200 (epoch 81), batch 270643, train_loss = 1.197, time/batch = 0.092\n",
      "Sequence 2655752/3263200 (epoch 81), batch 270743, train_loss = 1.453, time/batch = 0.094\n",
      "Sequence 2656732/3263200 (epoch 81), batch 270843, train_loss = 0.791, time/batch = 0.093\n",
      "Sequence 2657732/3263200 (epoch 81), batch 270943, train_loss = 1.736, time/batch = 0.092\n",
      "Sequence 2658692/3263200 (epoch 81), batch 271043, train_loss = 0.885, time/batch = 0.093\n",
      "Sequence 2659662/3263200 (epoch 81), batch 271143, train_loss = 1.511, time/batch = 0.092\n",
      "Sequence 2660652/3263200 (epoch 81), batch 271243, train_loss = 1.686, time/batch = 0.092\n",
      "Sequence 2661642/3263200 (epoch 81), batch 271343, train_loss = 1.512, time/batch = 0.093\n",
      "Sequence 2662632/3263200 (epoch 81), batch 271443, train_loss = 1.021, time/batch = 0.093\n",
      "Sequence 2663622/3263200 (epoch 81), batch 271543, train_loss = 1.520, time/batch = 0.093\n",
      "Sequence 2664572/3263200 (epoch 81), batch 271643, train_loss = 1.528, time/batch = 0.092\n",
      "Sequence 2665562/3263200 (epoch 81), batch 271743, train_loss = 1.259, time/batch = 0.094\n",
      "Sequence 2666492/3263200 (epoch 81), batch 271843, train_loss = 1.331, time/batch = 0.093\n",
      "Sequence 2667472/3263200 (epoch 81), batch 271943, train_loss = 1.283, time/batch = 0.092\n",
      "Sequence 2668452/3263200 (epoch 81), batch 272043, train_loss = 1.142, time/batch = 0.091\n",
      "Sequence 2669442/3263200 (epoch 81), batch 272143, train_loss = 1.626, time/batch = 0.093\n",
      "Sequence 2670432/3263200 (epoch 81), batch 272243, train_loss = 0.977, time/batch = 0.091\n",
      "Sequence 2671432/3263200 (epoch 81), batch 272343, train_loss = 1.552, time/batch = 0.093\n",
      "Sequence 2672422/3263200 (epoch 81), batch 272443, train_loss = 1.149, time/batch = 0.093\n",
      "Sequence 2673402/3263200 (epoch 81), batch 272543, train_loss = 1.342, time/batch = 0.093\n",
      "Sequence 2674392/3263200 (epoch 81), batch 272643, train_loss = 1.197, time/batch = 0.094\n",
      "Sequence 2675392/3263200 (epoch 81), batch 272743, train_loss = 0.776, time/batch = 0.092\n",
      "Epoch 81 completed, average train loss 1.168006, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2676354/3263200 (epoch 82), batch 272843, train_loss = 2.014, time/batch = 0.094\n",
      "Sequence 2677344/3263200 (epoch 82), batch 272943, train_loss = 2.124, time/batch = 0.092\n",
      "Sequence 2678314/3263200 (epoch 82), batch 273043, train_loss = 1.146, time/batch = 0.094\n",
      "Sequence 2679304/3263200 (epoch 82), batch 273143, train_loss = 1.010, time/batch = 0.093\n",
      "Sequence 2680304/3263200 (epoch 82), batch 273243, train_loss = 1.214, time/batch = 0.093\n",
      "Sequence 2681274/3263200 (epoch 82), batch 273343, train_loss = 0.944, time/batch = 0.094\n",
      "Sequence 2682264/3263200 (epoch 82), batch 273443, train_loss = 1.924, time/batch = 0.093\n",
      "Sequence 2683254/3263200 (epoch 82), batch 273543, train_loss = 1.137, time/batch = 0.092\n",
      "Sequence 2684254/3263200 (epoch 82), batch 273643, train_loss = 1.615, time/batch = 0.093\n",
      "Sequence 2685224/3263200 (epoch 82), batch 273743, train_loss = 1.134, time/batch = 0.093\n",
      "Sequence 2686224/3263200 (epoch 82), batch 273843, train_loss = 0.972, time/batch = 0.092\n",
      "Sequence 2687194/3263200 (epoch 82), batch 273943, train_loss = 0.917, time/batch = 0.093\n",
      "Sequence 2688154/3263200 (epoch 82), batch 274043, train_loss = 1.065, time/batch = 0.092\n",
      "Sequence 2689134/3263200 (epoch 82), batch 274143, train_loss = 1.092, time/batch = 0.093\n",
      "Sequence 2690094/3263200 (epoch 82), batch 274243, train_loss = 1.003, time/batch = 0.092\n",
      "Sequence 2691064/3263200 (epoch 82), batch 274343, train_loss = 1.703, time/batch = 0.092\n",
      "Sequence 2692064/3263200 (epoch 82), batch 274443, train_loss = 1.297, time/batch = 0.092\n",
      "Sequence 2693034/3263200 (epoch 82), batch 274543, train_loss = 1.918, time/batch = 0.092\n",
      "Sequence 2693984/3263200 (epoch 82), batch 274643, train_loss = 0.775, time/batch = 0.094\n",
      "Sequence 2694964/3263200 (epoch 82), batch 274743, train_loss = 0.922, time/batch = 0.091\n",
      "Sequence 2695954/3263200 (epoch 82), batch 274843, train_loss = 1.124, time/batch = 0.092\n",
      "Sequence 2696954/3263200 (epoch 82), batch 274943, train_loss = 0.686, time/batch = 0.092\n",
      "Sequence 2697914/3263200 (epoch 82), batch 275043, train_loss = 1.432, time/batch = 0.092\n",
      "Sequence 2698904/3263200 (epoch 82), batch 275143, train_loss = 1.496, time/batch = 0.093\n",
      "Sequence 2699904/3263200 (epoch 82), batch 275243, train_loss = 0.956, time/batch = 0.093\n",
      "Sequence 2700904/3263200 (epoch 82), batch 275343, train_loss = 1.273, time/batch = 0.093\n",
      "Sequence 2701894/3263200 (epoch 82), batch 275443, train_loss = 1.451, time/batch = 0.093\n",
      "Sequence 2702884/3263200 (epoch 82), batch 275543, train_loss = 0.877, time/batch = 0.092\n",
      "Sequence 2703844/3263200 (epoch 82), batch 275643, train_loss = 1.303, time/batch = 0.093\n",
      "Sequence 2704814/3263200 (epoch 82), batch 275743, train_loss = 1.442, time/batch = 0.092\n",
      "Sequence 2705784/3263200 (epoch 82), batch 275843, train_loss = 0.820, time/batch = 0.092\n",
      "Sequence 2706784/3263200 (epoch 82), batch 275943, train_loss = 0.791, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2707754/3263200 (epoch 82), batch 276043, train_loss = 0.850, time/batch = 0.092\n",
      "Epoch 82 completed, average train loss 1.167154, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2708746/3263200 (epoch 83), batch 276143, train_loss = 1.068, time/batch = 0.094\n",
      "Sequence 2709736/3263200 (epoch 83), batch 276243, train_loss = 1.170, time/batch = 0.093\n",
      "Sequence 2710726/3263200 (epoch 83), batch 276343, train_loss = 1.303, time/batch = 0.093\n",
      "Sequence 2711706/3263200 (epoch 83), batch 276443, train_loss = 0.912, time/batch = 0.092\n",
      "Sequence 2712686/3263200 (epoch 83), batch 276543, train_loss = 1.368, time/batch = 0.093\n",
      "Sequence 2713686/3263200 (epoch 83), batch 276643, train_loss = 1.138, time/batch = 0.093\n",
      "Sequence 2714676/3263200 (epoch 83), batch 276743, train_loss = 1.213, time/batch = 0.093\n",
      "Sequence 2715656/3263200 (epoch 83), batch 276843, train_loss = 0.968, time/batch = 0.094\n",
      "Sequence 2716656/3263200 (epoch 83), batch 276943, train_loss = 1.466, time/batch = 0.091\n",
      "Sequence 2717636/3263200 (epoch 83), batch 277043, train_loss = 0.867, time/batch = 0.092\n",
      "Sequence 2718596/3263200 (epoch 83), batch 277144, train_loss = 0.579, time/batch = 0.093\n",
      "Sequence 2719576/3263200 (epoch 83), batch 277244, train_loss = 1.637, time/batch = 0.093\n",
      "Sequence 2720536/3263200 (epoch 83), batch 277344, train_loss = 2.087, time/batch = 0.093\n",
      "Sequence 2721516/3263200 (epoch 83), batch 277444, train_loss = 1.322, time/batch = 0.093\n",
      "Sequence 2722466/3263200 (epoch 83), batch 277544, train_loss = 0.888, time/batch = 0.092\n",
      "Sequence 2723456/3263200 (epoch 83), batch 277644, train_loss = 1.069, time/batch = 0.092\n",
      "Sequence 2724446/3263200 (epoch 83), batch 277744, train_loss = 1.922, time/batch = 0.093\n",
      "Sequence 2725426/3263200 (epoch 83), batch 277844, train_loss = 1.572, time/batch = 0.092\n",
      "Sequence 2726396/3263200 (epoch 83), batch 277944, train_loss = 1.536, time/batch = 0.093\n",
      "Sequence 2727386/3263200 (epoch 83), batch 278044, train_loss = 1.307, time/batch = 0.094\n",
      "Sequence 2728346/3263200 (epoch 83), batch 278144, train_loss = 1.540, time/batch = 0.092\n",
      "Sequence 2729306/3263200 (epoch 83), batch 278244, train_loss = 1.116, time/batch = 0.094\n",
      "Sequence 2730306/3263200 (epoch 83), batch 278344, train_loss = 1.102, time/batch = 0.093\n",
      "Sequence 2731276/3263200 (epoch 83), batch 278444, train_loss = 1.564, time/batch = 0.093\n",
      "Sequence 2732246/3263200 (epoch 83), batch 278544, train_loss = 0.921, time/batch = 0.092\n",
      "Sequence 2733236/3263200 (epoch 83), batch 278644, train_loss = 1.171, time/batch = 0.093\n",
      "Sequence 2734206/3263200 (epoch 83), batch 278745, train_loss = 0.537, time/batch = 0.092\n",
      "Sequence 2735196/3263200 (epoch 83), batch 278845, train_loss = 1.946, time/batch = 0.094\n",
      "Sequence 2736196/3263200 (epoch 83), batch 278945, train_loss = 0.673, time/batch = 0.092\n",
      "Sequence 2737156/3263200 (epoch 83), batch 279045, train_loss = 1.089, time/batch = 0.092\n",
      "Sequence 2738156/3263200 (epoch 83), batch 279145, train_loss = 1.131, time/batch = 0.093\n",
      "Sequence 2739156/3263200 (epoch 83), batch 279245, train_loss = 1.283, time/batch = 0.094\n",
      "Sequence 2740136/3263200 (epoch 83), batch 279345, train_loss = 1.112, time/batch = 0.093\n",
      "Epoch 83 completed, average train loss 1.165288, learning rate 0.0010\n",
      "Sequence 2741128/3263200 (epoch 84), batch 279445, train_loss = 1.017, time/batch = 0.094\n",
      "Shuffling training data...\n",
      "Sequence 2742098/3263200 (epoch 84), batch 279545, train_loss = 0.951, time/batch = 0.093\n",
      "Sequence 2743098/3263200 (epoch 84), batch 279645, train_loss = 1.082, time/batch = 0.094\n",
      "Sequence 2744088/3263200 (epoch 84), batch 279745, train_loss = 0.539, time/batch = 0.094\n",
      "Sequence 2745058/3263200 (epoch 84), batch 279845, train_loss = 1.178, time/batch = 0.094\n",
      "Sequence 2746038/3263200 (epoch 84), batch 279945, train_loss = 1.410, time/batch = 0.093\n",
      "Sequence 2747018/3263200 (epoch 84), batch 280045, train_loss = 1.186, time/batch = 0.093\n",
      "Sequence 2748018/3263200 (epoch 84), batch 280145, train_loss = 1.422, time/batch = 0.092\n",
      "Sequence 2748988/3263200 (epoch 84), batch 280245, train_loss = 1.190, time/batch = 0.093\n",
      "Sequence 2749958/3263200 (epoch 84), batch 280345, train_loss = 1.722, time/batch = 0.093\n",
      "Sequence 2750938/3263200 (epoch 84), batch 280445, train_loss = 1.236, time/batch = 0.092\n",
      "Sequence 2751898/3263200 (epoch 84), batch 280545, train_loss = 1.158, time/batch = 0.092\n",
      "Sequence 2752898/3263200 (epoch 84), batch 280645, train_loss = 1.131, time/batch = 0.092\n",
      "Sequence 2753878/3263200 (epoch 84), batch 280745, train_loss = 0.704, time/batch = 0.093\n",
      "Sequence 2754868/3263200 (epoch 84), batch 280845, train_loss = 1.613, time/batch = 0.093\n",
      "Sequence 2755858/3263200 (epoch 84), batch 280945, train_loss = 1.003, time/batch = 0.092\n",
      "Sequence 2756838/3263200 (epoch 84), batch 281045, train_loss = 0.858, time/batch = 0.093\n",
      "Sequence 2757798/3263200 (epoch 84), batch 281145, train_loss = 1.156, time/batch = 0.093\n",
      "Sequence 2758778/3263200 (epoch 84), batch 281245, train_loss = 1.186, time/batch = 0.094\n",
      "Sequence 2759758/3263200 (epoch 84), batch 281345, train_loss = 1.128, time/batch = 0.093\n",
      "Sequence 2760738/3263200 (epoch 84), batch 281445, train_loss = 1.213, time/batch = 0.093\n",
      "Sequence 2761718/3263200 (epoch 84), batch 281545, train_loss = 1.985, time/batch = 0.094\n",
      "Sequence 2762708/3263200 (epoch 84), batch 281645, train_loss = 1.844, time/batch = 0.092\n",
      "Sequence 2763688/3263200 (epoch 84), batch 281745, train_loss = 0.721, time/batch = 0.092\n",
      "Sequence 2764668/3263200 (epoch 84), batch 281845, train_loss = 1.323, time/batch = 0.094\n",
      "Sequence 2765658/3263200 (epoch 84), batch 281945, train_loss = 1.009, time/batch = 0.092\n",
      "Sequence 2766638/3263200 (epoch 84), batch 282045, train_loss = 0.897, time/batch = 0.092\n",
      "Sequence 2767608/3263200 (epoch 84), batch 282145, train_loss = 0.891, time/batch = 0.092\n",
      "Sequence 2768608/3263200 (epoch 84), batch 282245, train_loss = 1.246, time/batch = 0.092\n",
      "Sequence 2769588/3263200 (epoch 84), batch 282345, train_loss = 1.256, time/batch = 0.092\n",
      "Sequence 2770578/3263200 (epoch 84), batch 282445, train_loss = 0.809, time/batch = 0.094\n",
      "Sequence 2771558/3263200 (epoch 84), batch 282545, train_loss = 0.966, time/batch = 0.094\n",
      "Sequence 2772548/3263200 (epoch 84), batch 282645, train_loss = 1.019, time/batch = 0.092\n",
      "Sequence 2773518/3263200 (epoch 84), batch 282745, train_loss = 1.556, time/batch = 0.092\n",
      "Epoch 84 completed, average train loss 1.163925, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 2774490/3263200 (epoch 85), batch 282845, train_loss = 0.694, time/batch = 0.096\n",
      "Sequence 2775460/3263200 (epoch 85), batch 282945, train_loss = 1.393, time/batch = 0.092\n",
      "Sequence 2776460/3263200 (epoch 85), batch 283045, train_loss = 0.951, time/batch = 0.092\n",
      "Sequence 2777430/3263200 (epoch 85), batch 283145, train_loss = 1.592, time/batch = 0.094\n",
      "Sequence 2778400/3263200 (epoch 85), batch 283245, train_loss = 1.114, time/batch = 0.092\n",
      "Sequence 2779380/3263200 (epoch 85), batch 283345, train_loss = 1.061, time/batch = 0.093\n",
      "Sequence 2780360/3263200 (epoch 85), batch 283445, train_loss = 1.259, time/batch = 0.092\n",
      "Sequence 2781320/3263200 (epoch 85), batch 283545, train_loss = 1.145, time/batch = 0.093\n",
      "Sequence 2782300/3263200 (epoch 85), batch 283645, train_loss = 0.974, time/batch = 0.092\n",
      "Sequence 2783290/3263200 (epoch 85), batch 283745, train_loss = 1.122, time/batch = 0.093\n",
      "Sequence 2784260/3263200 (epoch 85), batch 283845, train_loss = 0.889, time/batch = 0.094\n",
      "Sequence 2785240/3263200 (epoch 85), batch 283945, train_loss = 1.286, time/batch = 0.092\n",
      "Sequence 2786230/3263200 (epoch 85), batch 284045, train_loss = 1.510, time/batch = 0.091\n",
      "Sequence 2787230/3263200 (epoch 85), batch 284145, train_loss = 1.294, time/batch = 0.093\n",
      "Sequence 2788220/3263200 (epoch 85), batch 284245, train_loss = 1.493, time/batch = 0.092\n",
      "Sequence 2789200/3263200 (epoch 85), batch 284345, train_loss = 1.213, time/batch = 0.093\n",
      "Sequence 2790170/3263200 (epoch 85), batch 284445, train_loss = 0.951, time/batch = 0.092\n",
      "Sequence 2791150/3263200 (epoch 85), batch 284545, train_loss = 1.131, time/batch = 0.093\n",
      "Sequence 2792140/3263200 (epoch 85), batch 284645, train_loss = 0.766, time/batch = 0.092\n",
      "Sequence 2793120/3263200 (epoch 85), batch 284745, train_loss = 1.246, time/batch = 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2794100/3263200 (epoch 85), batch 284845, train_loss = 1.024, time/batch = 0.092\n",
      "Sequence 2795090/3263200 (epoch 85), batch 284945, train_loss = 1.532, time/batch = 0.093\n",
      "Sequence 2796060/3263200 (epoch 85), batch 285045, train_loss = 0.518, time/batch = 0.092\n",
      "Sequence 2797040/3263200 (epoch 85), batch 285145, train_loss = 1.142, time/batch = 0.092\n",
      "Sequence 2798020/3263200 (epoch 85), batch 285245, train_loss = 0.736, time/batch = 0.092\n",
      "Sequence 2799020/3263200 (epoch 85), batch 285345, train_loss = 1.376, time/batch = 0.093\n",
      "Sequence 2799990/3263200 (epoch 85), batch 285445, train_loss = 0.985, time/batch = 0.093\n",
      "Sequence 2800990/3263200 (epoch 85), batch 285545, train_loss = 0.957, time/batch = 0.092\n",
      "Sequence 2801940/3263200 (epoch 85), batch 285645, train_loss = 1.040, time/batch = 0.093\n",
      "Sequence 2802940/3263200 (epoch 85), batch 285745, train_loss = 1.289, time/batch = 0.093\n",
      "Sequence 2803940/3263200 (epoch 85), batch 285845, train_loss = 1.138, time/batch = 0.093\n",
      "Sequence 2804930/3263200 (epoch 85), batch 285945, train_loss = 0.848, time/batch = 0.092\n",
      "Sequence 2805920/3263200 (epoch 85), batch 286045, train_loss = 0.798, time/batch = 0.095\n",
      "Epoch 85 completed, average train loss 1.160785, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2806882/3263200 (epoch 86), batch 286145, train_loss = 1.005, time/batch = 0.093\n",
      "Sequence 2807862/3263200 (epoch 86), batch 286245, train_loss = 0.573, time/batch = 0.092\n",
      "Sequence 2808832/3263200 (epoch 86), batch 286345, train_loss = 1.229, time/batch = 0.093\n",
      "Sequence 2809822/3263200 (epoch 86), batch 286445, train_loss = 1.321, time/batch = 0.093\n",
      "Sequence 2810802/3263200 (epoch 86), batch 286545, train_loss = 1.215, time/batch = 0.092\n",
      "Sequence 2811782/3263200 (epoch 86), batch 286645, train_loss = 1.047, time/batch = 0.093\n",
      "Sequence 2812772/3263200 (epoch 86), batch 286745, train_loss = 0.861, time/batch = 0.092\n",
      "Sequence 2813772/3263200 (epoch 86), batch 286845, train_loss = 1.212, time/batch = 0.093\n",
      "Sequence 2814742/3263200 (epoch 86), batch 286945, train_loss = 0.972, time/batch = 0.093\n",
      "Sequence 2815722/3263200 (epoch 86), batch 287045, train_loss = 0.883, time/batch = 0.093\n",
      "Sequence 2816712/3263200 (epoch 86), batch 287145, train_loss = 0.829, time/batch = 0.094\n",
      "Sequence 2817672/3263200 (epoch 86), batch 287245, train_loss = 1.336, time/batch = 0.092\n",
      "Sequence 2818662/3263200 (epoch 86), batch 287345, train_loss = 1.190, time/batch = 0.093\n",
      "Sequence 2819652/3263200 (epoch 86), batch 287445, train_loss = 0.767, time/batch = 0.093\n",
      "Sequence 2820632/3263200 (epoch 86), batch 287545, train_loss = 0.939, time/batch = 0.093\n",
      "Sequence 2821612/3263200 (epoch 86), batch 287645, train_loss = 1.091, time/batch = 0.094\n",
      "Sequence 2822592/3263200 (epoch 86), batch 287745, train_loss = 0.978, time/batch = 0.091\n",
      "Sequence 2823572/3263200 (epoch 86), batch 287845, train_loss = 1.342, time/batch = 0.091\n",
      "Sequence 2824542/3263200 (epoch 86), batch 287945, train_loss = 0.903, time/batch = 0.092\n",
      "Sequence 2825512/3263200 (epoch 86), batch 288045, train_loss = 0.910, time/batch = 0.092\n",
      "Sequence 2826502/3263200 (epoch 86), batch 288145, train_loss = 0.812, time/batch = 0.092\n",
      "Sequence 2827482/3263200 (epoch 86), batch 288245, train_loss = 1.306, time/batch = 0.092\n",
      "Sequence 2828482/3263200 (epoch 86), batch 288345, train_loss = 1.175, time/batch = 0.092\n",
      "Sequence 2829452/3263200 (epoch 86), batch 288445, train_loss = 1.334, time/batch = 0.093\n",
      "Sequence 2830412/3263200 (epoch 86), batch 288545, train_loss = 1.632, time/batch = 0.092\n",
      "Sequence 2831382/3263200 (epoch 86), batch 288645, train_loss = 1.912, time/batch = 0.093\n",
      "Sequence 2832382/3263200 (epoch 86), batch 288745, train_loss = 0.914, time/batch = 0.092\n",
      "Sequence 2833372/3263200 (epoch 86), batch 288845, train_loss = 1.213, time/batch = 0.093\n",
      "Sequence 2834352/3263200 (epoch 86), batch 288945, train_loss = 1.539, time/batch = 0.093\n",
      "Sequence 2835352/3263200 (epoch 86), batch 289045, train_loss = 1.037, time/batch = 0.093\n",
      "Sequence 2836342/3263200 (epoch 86), batch 289145, train_loss = 1.347, time/batch = 0.093\n",
      "Sequence 2837322/3263200 (epoch 86), batch 289245, train_loss = 1.076, time/batch = 0.092\n",
      "Sequence 2838292/3263200 (epoch 86), batch 289345, train_loss = 1.125, time/batch = 0.091\n",
      "Epoch 86 completed, average train loss 1.160665, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2839264/3263200 (epoch 87), batch 289445, train_loss = 1.183, time/batch = 0.093\n",
      "Sequence 2840264/3263200 (epoch 87), batch 289545, train_loss = 1.556, time/batch = 0.092\n",
      "Sequence 2841254/3263200 (epoch 87), batch 289645, train_loss = 1.501, time/batch = 0.092\n",
      "Sequence 2842224/3263200 (epoch 87), batch 289745, train_loss = 0.860, time/batch = 0.092\n",
      "Sequence 2843204/3263200 (epoch 87), batch 289845, train_loss = 1.180, time/batch = 0.092\n",
      "Sequence 2844194/3263200 (epoch 87), batch 289945, train_loss = 1.338, time/batch = 0.092\n",
      "Sequence 2845194/3263200 (epoch 87), batch 290045, train_loss = 1.284, time/batch = 0.092\n",
      "Sequence 2846134/3263200 (epoch 87), batch 290145, train_loss = 1.445, time/batch = 0.093\n",
      "Sequence 2847114/3263200 (epoch 87), batch 290245, train_loss = 0.866, time/batch = 0.093\n",
      "Sequence 2848104/3263200 (epoch 87), batch 290345, train_loss = 1.198, time/batch = 0.093\n",
      "Sequence 2849084/3263200 (epoch 87), batch 290445, train_loss = 1.252, time/batch = 0.093\n",
      "Sequence 2850054/3263200 (epoch 87), batch 290545, train_loss = 1.228, time/batch = 0.093\n",
      "Sequence 2851034/3263200 (epoch 87), batch 290645, train_loss = 0.931, time/batch = 0.093\n",
      "Sequence 2852034/3263200 (epoch 87), batch 290745, train_loss = 1.054, time/batch = 0.093\n",
      "Sequence 2853014/3263200 (epoch 87), batch 290845, train_loss = 1.163, time/batch = 0.092\n",
      "Sequence 2854004/3263200 (epoch 87), batch 290945, train_loss = 1.332, time/batch = 0.094\n",
      "Sequence 2854984/3263200 (epoch 87), batch 291045, train_loss = 1.060, time/batch = 0.093\n",
      "Sequence 2855964/3263200 (epoch 87), batch 291145, train_loss = 1.705, time/batch = 0.092\n",
      "Sequence 2856944/3263200 (epoch 87), batch 291245, train_loss = 1.001, time/batch = 0.093\n",
      "Sequence 2857924/3263200 (epoch 87), batch 291345, train_loss = 0.839, time/batch = 0.092\n",
      "Sequence 2858914/3263200 (epoch 87), batch 291445, train_loss = 1.202, time/batch = 0.091\n",
      "Sequence 2859884/3263200 (epoch 87), batch 291545, train_loss = 1.039, time/batch = 0.092\n",
      "Sequence 2860874/3263200 (epoch 87), batch 291645, train_loss = 0.824, time/batch = 0.091\n",
      "Sequence 2861844/3263200 (epoch 87), batch 291745, train_loss = 1.029, time/batch = 0.093\n",
      "Sequence 2862834/3263200 (epoch 87), batch 291845, train_loss = 1.324, time/batch = 0.092\n",
      "Sequence 2863814/3263200 (epoch 87), batch 291945, train_loss = 1.270, time/batch = 0.092\n",
      "Sequence 2864804/3263200 (epoch 87), batch 292045, train_loss = 1.474, time/batch = 0.093\n",
      "Sequence 2865784/3263200 (epoch 87), batch 292145, train_loss = 1.108, time/batch = 0.093\n",
      "Sequence 2866744/3263200 (epoch 87), batch 292245, train_loss = 1.289, time/batch = 0.094\n",
      "Sequence 2867724/3263200 (epoch 87), batch 292345, train_loss = 1.127, time/batch = 0.092\n",
      "Sequence 2868704/3263200 (epoch 87), batch 292445, train_loss = 0.883, time/batch = 0.093\n",
      "Sequence 2869684/3263200 (epoch 87), batch 292545, train_loss = 1.168, time/batch = 0.092\n",
      "Sequence 2870674/3263200 (epoch 87), batch 292645, train_loss = 0.952, time/batch = 0.092\n",
      "Epoch 87 completed, average train loss 1.158682, learning rate 0.0010\n",
      "Sequence 2871656/3263200 (epoch 88), batch 292746, train_loss = 0.405, time/batch = 0.092\n",
      "Shuffling training data...\n",
      "Sequence 2872616/3263200 (epoch 88), batch 292846, train_loss = 1.096, time/batch = 0.094\n",
      "Sequence 2873576/3263200 (epoch 88), batch 292946, train_loss = 2.387, time/batch = 0.093\n",
      "Sequence 2874566/3263200 (epoch 88), batch 293046, train_loss = 1.134, time/batch = 0.089\n",
      "Sequence 2875566/3263200 (epoch 88), batch 293146, train_loss = 0.960, time/batch = 0.093\n",
      "Sequence 2876546/3263200 (epoch 88), batch 293246, train_loss = 1.745, time/batch = 0.094\n",
      "Sequence 2877546/3263200 (epoch 88), batch 293346, train_loss = 0.881, time/batch = 0.093\n",
      "Sequence 2878526/3263200 (epoch 88), batch 293446, train_loss = 0.811, time/batch = 0.092\n",
      "Sequence 2879526/3263200 (epoch 88), batch 293546, train_loss = 0.727, time/batch = 0.094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2880526/3263200 (epoch 88), batch 293646, train_loss = 1.084, time/batch = 0.092\n",
      "Sequence 2881516/3263200 (epoch 88), batch 293746, train_loss = 1.207, time/batch = 0.093\n",
      "Sequence 2882496/3263200 (epoch 88), batch 293846, train_loss = 0.972, time/batch = 0.092\n",
      "Sequence 2883486/3263200 (epoch 88), batch 293946, train_loss = 1.230, time/batch = 0.095\n",
      "Sequence 2884476/3263200 (epoch 88), batch 294046, train_loss = 1.348, time/batch = 0.092\n",
      "Sequence 2885456/3263200 (epoch 88), batch 294146, train_loss = 0.985, time/batch = 0.092\n",
      "Sequence 2886456/3263200 (epoch 88), batch 294246, train_loss = 1.193, time/batch = 0.093\n",
      "Sequence 2887446/3263200 (epoch 88), batch 294346, train_loss = 0.873, time/batch = 0.094\n",
      "Sequence 2888406/3263200 (epoch 88), batch 294446, train_loss = 1.312, time/batch = 0.093\n",
      "Sequence 2889386/3263200 (epoch 88), batch 294546, train_loss = 1.709, time/batch = 0.093\n",
      "Sequence 2890386/3263200 (epoch 88), batch 294647, train_loss = 0.634, time/batch = 0.092\n",
      "Sequence 2891336/3263200 (epoch 88), batch 294747, train_loss = 1.142, time/batch = 0.091\n",
      "Sequence 2892326/3263200 (epoch 88), batch 294847, train_loss = 1.143, time/batch = 0.092\n",
      "Sequence 2893286/3263200 (epoch 88), batch 294947, train_loss = 1.322, time/batch = 0.093\n",
      "Sequence 2894246/3263200 (epoch 88), batch 295047, train_loss = 1.165, time/batch = 0.092\n",
      "Sequence 2895216/3263200 (epoch 88), batch 295147, train_loss = 1.491, time/batch = 0.093\n",
      "Sequence 2896186/3263200 (epoch 88), batch 295247, train_loss = 1.129, time/batch = 0.092\n",
      "Sequence 2897166/3263200 (epoch 88), batch 295347, train_loss = 1.174, time/batch = 0.093\n",
      "Sequence 2898136/3263200 (epoch 88), batch 295447, train_loss = 1.005, time/batch = 0.094\n",
      "Sequence 2899126/3263200 (epoch 88), batch 295547, train_loss = 1.093, time/batch = 0.092\n",
      "Sequence 2900116/3263200 (epoch 88), batch 295647, train_loss = 0.941, time/batch = 0.093\n",
      "Sequence 2901116/3263200 (epoch 88), batch 295747, train_loss = 0.875, time/batch = 0.092\n",
      "Sequence 2902086/3263200 (epoch 88), batch 295847, train_loss = 1.794, time/batch = 0.092\n",
      "Sequence 2903046/3263200 (epoch 88), batch 295947, train_loss = 1.424, time/batch = 0.092\n",
      "Sequence 2904006/3263200 (epoch 88), batch 296047, train_loss = 1.341, time/batch = 0.092\n",
      "Epoch 88 completed, average train loss 1.155544, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2904998/3263200 (epoch 89), batch 296147, train_loss = 1.208, time/batch = 0.093\n",
      "Sequence 2905978/3263200 (epoch 89), batch 296247, train_loss = 1.374, time/batch = 0.093\n",
      "Sequence 2906958/3263200 (epoch 89), batch 296347, train_loss = 1.513, time/batch = 0.094\n",
      "Sequence 2907928/3263200 (epoch 89), batch 296447, train_loss = 0.716, time/batch = 0.093\n",
      "Sequence 2908908/3263200 (epoch 89), batch 296547, train_loss = 1.174, time/batch = 0.093\n",
      "Sequence 2909868/3263200 (epoch 89), batch 296647, train_loss = 1.617, time/batch = 0.093\n",
      "Sequence 2910858/3263200 (epoch 89), batch 296747, train_loss = 1.483, time/batch = 0.093\n",
      "Sequence 2911848/3263200 (epoch 89), batch 296847, train_loss = 0.968, time/batch = 0.093\n",
      "Sequence 2912818/3263200 (epoch 89), batch 296947, train_loss = 1.013, time/batch = 0.093\n",
      "Sequence 2913788/3263200 (epoch 89), batch 297047, train_loss = 1.377, time/batch = 0.093\n",
      "Sequence 2914778/3263200 (epoch 89), batch 297147, train_loss = 0.873, time/batch = 0.092\n",
      "Sequence 2915748/3263200 (epoch 89), batch 297247, train_loss = 1.139, time/batch = 0.094\n",
      "Sequence 2916718/3263200 (epoch 89), batch 297347, train_loss = 1.330, time/batch = 0.092\n",
      "Sequence 2917668/3263200 (epoch 89), batch 297447, train_loss = 1.377, time/batch = 0.092\n",
      "Sequence 2918658/3263200 (epoch 89), batch 297547, train_loss = 1.110, time/batch = 0.092\n",
      "Sequence 2919648/3263200 (epoch 89), batch 297647, train_loss = 1.836, time/batch = 0.092\n",
      "Sequence 2920638/3263200 (epoch 89), batch 297747, train_loss = 0.957, time/batch = 0.093\n",
      "Sequence 2921628/3263200 (epoch 89), batch 297847, train_loss = 0.994, time/batch = 0.094\n",
      "Sequence 2922598/3263200 (epoch 89), batch 297947, train_loss = 0.993, time/batch = 0.093\n",
      "Sequence 2923578/3263200 (epoch 89), batch 298047, train_loss = 0.733, time/batch = 0.092\n",
      "Sequence 2924578/3263200 (epoch 89), batch 298147, train_loss = 0.988, time/batch = 0.092\n",
      "Sequence 2925558/3263200 (epoch 89), batch 298247, train_loss = 0.796, time/batch = 0.093\n",
      "Sequence 2926558/3263200 (epoch 89), batch 298347, train_loss = 1.316, time/batch = 0.093\n",
      "Sequence 2927538/3263200 (epoch 89), batch 298447, train_loss = 0.983, time/batch = 0.094\n",
      "Sequence 2928528/3263200 (epoch 89), batch 298547, train_loss = 1.183, time/batch = 0.092\n",
      "Sequence 2929478/3263200 (epoch 89), batch 298647, train_loss = 1.484, time/batch = 0.094\n",
      "Sequence 2930458/3263200 (epoch 89), batch 298747, train_loss = 1.119, time/batch = 0.093\n",
      "Sequence 2931458/3263200 (epoch 89), batch 298847, train_loss = 0.870, time/batch = 0.091\n",
      "Sequence 2932428/3263200 (epoch 89), batch 298947, train_loss = 1.216, time/batch = 0.094\n",
      "Sequence 2933418/3263200 (epoch 89), batch 299047, train_loss = 0.893, time/batch = 0.093\n",
      "Sequence 2934398/3263200 (epoch 89), batch 299147, train_loss = 1.302, time/batch = 0.092\n",
      "Sequence 2935388/3263200 (epoch 89), batch 299247, train_loss = 1.460, time/batch = 0.092\n",
      "Sequence 2936378/3263200 (epoch 89), batch 299347, train_loss = 1.298, time/batch = 0.093\n",
      "Epoch 89 completed, average train loss 1.154275, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 2937370/3263200 (epoch 90), batch 299447, train_loss = 1.389, time/batch = 0.096\n",
      "Sequence 2938350/3263200 (epoch 90), batch 299547, train_loss = 1.154, time/batch = 0.092\n",
      "Sequence 2939340/3263200 (epoch 90), batch 299647, train_loss = 1.377, time/batch = 0.093\n",
      "Sequence 2940310/3263200 (epoch 90), batch 299747, train_loss = 1.754, time/batch = 0.094\n",
      "Sequence 2941290/3263200 (epoch 90), batch 299847, train_loss = 1.164, time/batch = 0.093\n",
      "Sequence 2942250/3263200 (epoch 90), batch 299947, train_loss = 1.645, time/batch = 0.092\n",
      "Sequence 2943230/3263200 (epoch 90), batch 300047, train_loss = 1.158, time/batch = 0.093\n",
      "Sequence 2944210/3263200 (epoch 90), batch 300147, train_loss = 1.345, time/batch = 0.092\n",
      "Sequence 2945200/3263200 (epoch 90), batch 300247, train_loss = 0.788, time/batch = 0.093\n",
      "Sequence 2946160/3263200 (epoch 90), batch 300347, train_loss = 0.986, time/batch = 0.093\n",
      "Sequence 2947150/3263200 (epoch 90), batch 300447, train_loss = 1.148, time/batch = 0.093\n",
      "Sequence 2948140/3263200 (epoch 90), batch 300547, train_loss = 1.331, time/batch = 0.092\n",
      "Sequence 2949140/3263200 (epoch 90), batch 300647, train_loss = 1.030, time/batch = 0.101\n",
      "Sequence 2950100/3263200 (epoch 90), batch 300747, train_loss = 0.700, time/batch = 0.103\n",
      "Sequence 2951070/3263200 (epoch 90), batch 300847, train_loss = 0.881, time/batch = 0.103\n",
      "Sequence 2952050/3263200 (epoch 90), batch 300947, train_loss = 1.476, time/batch = 0.102\n",
      "Sequence 2953030/3263200 (epoch 90), batch 301047, train_loss = 1.106, time/batch = 0.100\n",
      "Sequence 2953990/3263200 (epoch 90), batch 301147, train_loss = 1.384, time/batch = 0.093\n",
      "Sequence 2954970/3263200 (epoch 90), batch 301247, train_loss = 1.762, time/batch = 0.093\n",
      "Sequence 2955950/3263200 (epoch 90), batch 301347, train_loss = 1.593, time/batch = 0.093\n",
      "Sequence 2956950/3263200 (epoch 90), batch 301447, train_loss = 1.128, time/batch = 0.093\n",
      "Sequence 2957920/3263200 (epoch 90), batch 301547, train_loss = 0.799, time/batch = 0.093\n",
      "Sequence 2958900/3263200 (epoch 90), batch 301647, train_loss = 1.019, time/batch = 0.092\n",
      "Sequence 2959880/3263200 (epoch 90), batch 301747, train_loss = 1.500, time/batch = 0.092\n",
      "Sequence 2960870/3263200 (epoch 90), batch 301848, train_loss = 0.638, time/batch = 0.093\n",
      "Sequence 2961860/3263200 (epoch 90), batch 301948, train_loss = 1.113, time/batch = 0.092\n",
      "Sequence 2962860/3263200 (epoch 90), batch 302048, train_loss = 0.738, time/batch = 0.092\n",
      "Sequence 2963850/3263200 (epoch 90), batch 302148, train_loss = 1.178, time/batch = 0.093\n",
      "Sequence 2964830/3263200 (epoch 90), batch 302248, train_loss = 1.047, time/batch = 0.092\n",
      "Sequence 2965810/3263200 (epoch 90), batch 302348, train_loss = 1.091, time/batch = 0.092\n",
      "Sequence 2966800/3263200 (epoch 90), batch 302448, train_loss = 0.817, time/batch = 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 2967790/3263200 (epoch 90), batch 302548, train_loss = 0.972, time/batch = 0.092\n",
      "Sequence 2968750/3263200 (epoch 90), batch 302648, train_loss = 0.953, time/batch = 0.094\n",
      "Epoch 90 completed, average train loss 1.151222, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 2969742/3263200 (epoch 91), batch 302748, train_loss = 0.901, time/batch = 0.091\n",
      "Sequence 2970742/3263200 (epoch 91), batch 302848, train_loss = 0.964, time/batch = 0.092\n",
      "Sequence 2971732/3263200 (epoch 91), batch 302948, train_loss = 0.546, time/batch = 0.092\n",
      "Sequence 2972712/3263200 (epoch 91), batch 303048, train_loss = 0.915, time/batch = 0.093\n",
      "Sequence 2973692/3263200 (epoch 91), batch 303148, train_loss = 1.316, time/batch = 0.092\n",
      "Sequence 2974682/3263200 (epoch 91), batch 303248, train_loss = 0.904, time/batch = 0.093\n",
      "Sequence 2975632/3263200 (epoch 91), batch 303348, train_loss = 1.056, time/batch = 0.091\n",
      "Sequence 2976622/3263200 (epoch 91), batch 303448, train_loss = 1.367, time/batch = 0.094\n",
      "Sequence 2977602/3263200 (epoch 91), batch 303548, train_loss = 1.023, time/batch = 0.093\n",
      "Sequence 2978602/3263200 (epoch 91), batch 303648, train_loss = 0.910, time/batch = 0.093\n",
      "Sequence 2979592/3263200 (epoch 91), batch 303748, train_loss = 0.887, time/batch = 0.092\n",
      "Sequence 2980562/3263200 (epoch 91), batch 303848, train_loss = 1.695, time/batch = 0.093\n",
      "Sequence 2981522/3263200 (epoch 91), batch 303948, train_loss = 0.964, time/batch = 0.093\n",
      "Sequence 2982512/3263200 (epoch 91), batch 304048, train_loss = 1.518, time/batch = 0.093\n",
      "Sequence 2983472/3263200 (epoch 91), batch 304148, train_loss = 0.912, time/batch = 0.093\n",
      "Sequence 2984442/3263200 (epoch 91), batch 304248, train_loss = 1.197, time/batch = 0.092\n",
      "Sequence 2985412/3263200 (epoch 91), batch 304348, train_loss = 1.257, time/batch = 0.094\n",
      "Sequence 2986392/3263200 (epoch 91), batch 304448, train_loss = 1.212, time/batch = 0.093\n",
      "Sequence 2987352/3263200 (epoch 91), batch 304548, train_loss = 0.961, time/batch = 0.092\n",
      "Sequence 2988352/3263200 (epoch 91), batch 304648, train_loss = 1.097, time/batch = 0.093\n",
      "Sequence 2989342/3263200 (epoch 91), batch 304748, train_loss = 0.809, time/batch = 0.096\n",
      "Sequence 2990312/3263200 (epoch 91), batch 304848, train_loss = 0.981, time/batch = 0.092\n",
      "Sequence 2991292/3263200 (epoch 91), batch 304948, train_loss = 0.891, time/batch = 0.092\n",
      "Sequence 2992272/3263200 (epoch 91), batch 305049, train_loss = 0.859, time/batch = 0.093\n",
      "Sequence 2993262/3263200 (epoch 91), batch 305149, train_loss = 0.908, time/batch = 0.093\n",
      "Sequence 2994262/3263200 (epoch 91), batch 305249, train_loss = 0.790, time/batch = 0.093\n",
      "Sequence 2995232/3263200 (epoch 91), batch 305349, train_loss = 1.148, time/batch = 0.093\n",
      "Sequence 2996202/3263200 (epoch 91), batch 305449, train_loss = 1.669, time/batch = 0.092\n",
      "Sequence 2997192/3263200 (epoch 91), batch 305549, train_loss = 1.377, time/batch = 0.094\n",
      "Sequence 2998172/3263200 (epoch 91), batch 305649, train_loss = 1.065, time/batch = 0.093\n",
      "Sequence 2999172/3263200 (epoch 91), batch 305749, train_loss = 1.084, time/batch = 0.092\n",
      "Sequence 3000152/3263200 (epoch 91), batch 305849, train_loss = 1.014, time/batch = 0.092\n",
      "Sequence 3001142/3263200 (epoch 91), batch 305949, train_loss = 1.361, time/batch = 0.092\n",
      "Epoch 91 completed, average train loss 1.151350, learning rate 0.0010\n",
      "Sequence 3002144/3263200 (epoch 92), batch 306049, train_loss = 1.213, time/batch = 0.093\n",
      "Shuffling training data...\n",
      "Sequence 3003094/3263200 (epoch 92), batch 306149, train_loss = 1.253, time/batch = 0.093\n",
      "Sequence 3004064/3263200 (epoch 92), batch 306249, train_loss = 1.516, time/batch = 0.093\n",
      "Sequence 3005024/3263200 (epoch 92), batch 306349, train_loss = 1.102, time/batch = 0.094\n",
      "Sequence 3006024/3263200 (epoch 92), batch 306449, train_loss = 0.885, time/batch = 0.092\n",
      "Sequence 3007004/3263200 (epoch 92), batch 306549, train_loss = 0.779, time/batch = 0.092\n",
      "Sequence 3008004/3263200 (epoch 92), batch 306649, train_loss = 0.634, time/batch = 0.093\n",
      "Sequence 3008994/3263200 (epoch 92), batch 306749, train_loss = 1.383, time/batch = 0.094\n",
      "Sequence 3009984/3263200 (epoch 92), batch 306849, train_loss = 1.404, time/batch = 0.093\n",
      "Sequence 3010964/3263200 (epoch 92), batch 306949, train_loss = 2.250, time/batch = 0.093\n",
      "Sequence 3011944/3263200 (epoch 92), batch 307049, train_loss = 1.165, time/batch = 0.092\n",
      "Sequence 3012934/3263200 (epoch 92), batch 307149, train_loss = 0.979, time/batch = 0.094\n",
      "Sequence 3013914/3263200 (epoch 92), batch 307249, train_loss = 1.482, time/batch = 0.093\n",
      "Sequence 3014884/3263200 (epoch 92), batch 307349, train_loss = 0.883, time/batch = 0.093\n",
      "Sequence 3015834/3263200 (epoch 92), batch 307449, train_loss = 0.809, time/batch = 0.093\n",
      "Sequence 3016804/3263200 (epoch 92), batch 307549, train_loss = 1.120, time/batch = 0.094\n",
      "Sequence 3017784/3263200 (epoch 92), batch 307649, train_loss = 1.916, time/batch = 0.092\n",
      "Sequence 3018774/3263200 (epoch 92), batch 307749, train_loss = 1.691, time/batch = 0.092\n",
      "Sequence 3019774/3263200 (epoch 92), batch 307849, train_loss = 1.153, time/batch = 0.092\n",
      "Sequence 3020754/3263200 (epoch 92), batch 307949, train_loss = 1.419, time/batch = 0.093\n",
      "Sequence 3021744/3263200 (epoch 92), batch 308049, train_loss = 0.906, time/batch = 0.092\n",
      "Sequence 3022734/3263200 (epoch 92), batch 308149, train_loss = 1.165, time/batch = 0.092\n",
      "Sequence 3023724/3263200 (epoch 92), batch 308249, train_loss = 1.203, time/batch = 0.093\n",
      "Sequence 3024714/3263200 (epoch 92), batch 308349, train_loss = 1.361, time/batch = 0.092\n",
      "Sequence 3025674/3263200 (epoch 92), batch 308449, train_loss = 1.741, time/batch = 0.092\n",
      "Sequence 3026654/3263200 (epoch 92), batch 308549, train_loss = 0.952, time/batch = 0.093\n",
      "Sequence 3027634/3263200 (epoch 92), batch 308649, train_loss = 1.622, time/batch = 0.091\n",
      "Sequence 3028614/3263200 (epoch 92), batch 308749, train_loss = 1.702, time/batch = 0.093\n",
      "Sequence 3029584/3263200 (epoch 92), batch 308849, train_loss = 0.898, time/batch = 0.094\n",
      "Sequence 3030554/3263200 (epoch 92), batch 308949, train_loss = 1.123, time/batch = 0.092\n",
      "Sequence 3031534/3263200 (epoch 92), batch 309049, train_loss = 1.348, time/batch = 0.091\n",
      "Sequence 3032514/3263200 (epoch 92), batch 309149, train_loss = 1.276, time/batch = 0.091\n",
      "Sequence 3033494/3263200 (epoch 92), batch 309249, train_loss = 0.765, time/batch = 0.091\n",
      "Sequence 3034494/3263200 (epoch 92), batch 309349, train_loss = 1.124, time/batch = 0.088\n",
      "Epoch 92 completed, average train loss 1.147235, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 3035466/3263200 (epoch 93), batch 309449, train_loss = 1.077, time/batch = 0.092\n",
      "Sequence 3036456/3263200 (epoch 93), batch 309550, train_loss = 0.517, time/batch = 0.092\n",
      "Sequence 3037436/3263200 (epoch 93), batch 309650, train_loss = 1.020, time/batch = 0.093\n",
      "Sequence 3038436/3263200 (epoch 93), batch 309750, train_loss = 1.292, time/batch = 0.093\n",
      "Sequence 3039426/3263200 (epoch 93), batch 309850, train_loss = 1.202, time/batch = 0.093\n",
      "Sequence 3040416/3263200 (epoch 93), batch 309950, train_loss = 1.309, time/batch = 0.092\n",
      "Sequence 3041406/3263200 (epoch 93), batch 310050, train_loss = 1.491, time/batch = 0.093\n",
      "Sequence 3042386/3263200 (epoch 93), batch 310150, train_loss = 0.909, time/batch = 0.092\n",
      "Sequence 3043366/3263200 (epoch 93), batch 310250, train_loss = 1.526, time/batch = 0.093\n",
      "Sequence 3044326/3263200 (epoch 93), batch 310350, train_loss = 0.981, time/batch = 0.094\n",
      "Sequence 3045296/3263200 (epoch 93), batch 310450, train_loss = 1.167, time/batch = 0.092\n",
      "Sequence 3046276/3263200 (epoch 93), batch 310550, train_loss = 1.103, time/batch = 0.091\n",
      "Sequence 3047266/3263200 (epoch 93), batch 310650, train_loss = 1.052, time/batch = 0.095\n",
      "Sequence 3048256/3263200 (epoch 93), batch 310750, train_loss = 1.075, time/batch = 0.105\n",
      "Sequence 3049236/3263200 (epoch 93), batch 310850, train_loss = 0.922, time/batch = 0.104\n",
      "Sequence 3050216/3263200 (epoch 93), batch 310950, train_loss = 1.096, time/batch = 0.104\n",
      "Sequence 3051186/3263200 (epoch 93), batch 311050, train_loss = 1.099, time/batch = 0.102\n",
      "Sequence 3052186/3263200 (epoch 93), batch 311150, train_loss = 0.755, time/batch = 0.094\n",
      "Sequence 3053176/3263200 (epoch 93), batch 311250, train_loss = 1.794, time/batch = 0.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 3054166/3263200 (epoch 93), batch 311350, train_loss = 1.566, time/batch = 0.094\n",
      "Sequence 3055136/3263200 (epoch 93), batch 311450, train_loss = 0.973, time/batch = 0.093\n",
      "Sequence 3056116/3263200 (epoch 93), batch 311550, train_loss = 1.133, time/batch = 0.093\n",
      "Sequence 3057096/3263200 (epoch 93), batch 311650, train_loss = 0.959, time/batch = 0.094\n",
      "Sequence 3058066/3263200 (epoch 93), batch 311750, train_loss = 0.800, time/batch = 0.092\n",
      "Sequence 3059056/3263200 (epoch 93), batch 311850, train_loss = 2.476, time/batch = 0.093\n",
      "Sequence 3060036/3263200 (epoch 93), batch 311950, train_loss = 1.191, time/batch = 0.092\n",
      "Sequence 3061016/3263200 (epoch 93), batch 312050, train_loss = 1.280, time/batch = 0.093\n",
      "Sequence 3061986/3263200 (epoch 93), batch 312150, train_loss = 1.142, time/batch = 0.092\n",
      "Sequence 3062956/3263200 (epoch 93), batch 312250, train_loss = 1.399, time/batch = 0.092\n",
      "Sequence 3063926/3263200 (epoch 93), batch 312350, train_loss = 0.997, time/batch = 0.092\n",
      "Sequence 3064916/3263200 (epoch 93), batch 312450, train_loss = 1.128, time/batch = 0.092\n",
      "Sequence 3065896/3263200 (epoch 93), batch 312550, train_loss = 1.036, time/batch = 0.094\n",
      "Sequence 3066876/3263200 (epoch 93), batch 312650, train_loss = 1.167, time/batch = 0.093\n",
      "Epoch 93 completed, average train loss 1.147794, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 3067868/3263200 (epoch 94), batch 312750, train_loss = 0.944, time/batch = 0.095\n",
      "Sequence 3068838/3263200 (epoch 94), batch 312850, train_loss = 1.069, time/batch = 0.092\n",
      "Sequence 3069818/3263200 (epoch 94), batch 312950, train_loss = 1.293, time/batch = 0.094\n",
      "Sequence 3070808/3263200 (epoch 94), batch 313050, train_loss = 1.085, time/batch = 0.094\n",
      "Sequence 3071798/3263200 (epoch 94), batch 313150, train_loss = 1.026, time/batch = 0.094\n",
      "Sequence 3072758/3263200 (epoch 94), batch 313250, train_loss = 1.373, time/batch = 0.093\n",
      "Sequence 3073748/3263200 (epoch 94), batch 313350, train_loss = 1.005, time/batch = 0.093\n",
      "Sequence 3074748/3263200 (epoch 94), batch 313450, train_loss = 1.139, time/batch = 0.093\n",
      "Sequence 3075728/3263200 (epoch 94), batch 313550, train_loss = 1.169, time/batch = 0.092\n",
      "Sequence 3076718/3263200 (epoch 94), batch 313650, train_loss = 0.899, time/batch = 0.093\n",
      "Sequence 3077708/3263200 (epoch 94), batch 313750, train_loss = 1.206, time/batch = 0.093\n",
      "Sequence 3078678/3263200 (epoch 94), batch 313850, train_loss = 0.779, time/batch = 0.092\n",
      "Sequence 3079668/3263200 (epoch 94), batch 313950, train_loss = 0.902, time/batch = 0.093\n",
      "Sequence 3080648/3263200 (epoch 94), batch 314050, train_loss = 1.396, time/batch = 0.091\n",
      "Sequence 3081648/3263200 (epoch 94), batch 314150, train_loss = 0.873, time/batch = 0.092\n",
      "Sequence 3082648/3263200 (epoch 94), batch 314250, train_loss = 0.796, time/batch = 0.093\n",
      "Sequence 3083608/3263200 (epoch 94), batch 314350, train_loss = 1.196, time/batch = 0.094\n",
      "Sequence 3084608/3263200 (epoch 94), batch 314450, train_loss = 0.894, time/batch = 0.093\n",
      "Sequence 3085608/3263200 (epoch 94), batch 314550, train_loss = 1.357, time/batch = 0.093\n",
      "Sequence 3086588/3263200 (epoch 94), batch 314650, train_loss = 1.491, time/batch = 0.093\n",
      "Sequence 3087538/3263200 (epoch 94), batch 314750, train_loss = 1.237, time/batch = 0.093\n",
      "Sequence 3088498/3263200 (epoch 94), batch 314850, train_loss = 1.292, time/batch = 0.092\n",
      "Sequence 3089488/3263200 (epoch 94), batch 314950, train_loss = 1.030, time/batch = 0.091\n",
      "Sequence 3090478/3263200 (epoch 94), batch 315050, train_loss = 1.336, time/batch = 0.094\n",
      "Sequence 3091448/3263200 (epoch 94), batch 315150, train_loss = 0.956, time/batch = 0.093\n",
      "Sequence 3092438/3263200 (epoch 94), batch 315250, train_loss = 1.376, time/batch = 0.093\n",
      "Sequence 3093398/3263200 (epoch 94), batch 315350, train_loss = 1.781, time/batch = 0.094\n",
      "Sequence 3094368/3263200 (epoch 94), batch 315450, train_loss = 0.977, time/batch = 0.091\n",
      "Sequence 3095338/3263200 (epoch 94), batch 315550, train_loss = 1.341, time/batch = 0.093\n",
      "Sequence 3096288/3263200 (epoch 94), batch 315650, train_loss = 0.932, time/batch = 0.093\n",
      "Sequence 3097278/3263200 (epoch 94), batch 315750, train_loss = 1.399, time/batch = 0.092\n",
      "Sequence 3098248/3263200 (epoch 94), batch 315850, train_loss = 1.099, time/batch = 0.091\n",
      "Sequence 3099238/3263200 (epoch 94), batch 315950, train_loss = 0.854, time/batch = 0.093\n",
      "Epoch 94 completed, average train loss 1.146145, learning rate 0.0010\n",
      "model saved.\n",
      "Shuffling training data...\n",
      "Sequence 3100230/3263200 (epoch 95), batch 316050, train_loss = 1.148, time/batch = 0.096\n",
      "Sequence 3101220/3263200 (epoch 95), batch 316150, train_loss = 0.882, time/batch = 0.093\n",
      "Sequence 3102200/3263200 (epoch 95), batch 316250, train_loss = 1.634, time/batch = 0.093\n",
      "Sequence 3103180/3263200 (epoch 95), batch 316350, train_loss = 0.980, time/batch = 0.093\n",
      "Sequence 3104180/3263200 (epoch 95), batch 316450, train_loss = 1.529, time/batch = 0.093\n",
      "Sequence 3105180/3263200 (epoch 95), batch 316550, train_loss = 1.126, time/batch = 0.093\n",
      "Sequence 3106170/3263200 (epoch 95), batch 316650, train_loss = 1.506, time/batch = 0.092\n",
      "Sequence 3107160/3263200 (epoch 95), batch 316750, train_loss = 0.979, time/batch = 0.092\n",
      "Sequence 3108130/3263200 (epoch 95), batch 316850, train_loss = 0.761, time/batch = 0.093\n",
      "Sequence 3109100/3263200 (epoch 95), batch 316950, train_loss = 1.279, time/batch = 0.094\n",
      "Sequence 3110090/3263200 (epoch 95), batch 317050, train_loss = 1.193, time/batch = 0.093\n",
      "Sequence 3111060/3263200 (epoch 95), batch 317150, train_loss = 1.162, time/batch = 0.092\n",
      "Sequence 3112060/3263200 (epoch 95), batch 317250, train_loss = 1.190, time/batch = 0.093\n",
      "Sequence 3113040/3263200 (epoch 95), batch 317350, train_loss = 1.343, time/batch = 0.091\n",
      "Sequence 3114030/3263200 (epoch 95), batch 317450, train_loss = 1.444, time/batch = 0.092\n",
      "Sequence 3115010/3263200 (epoch 95), batch 317550, train_loss = 1.279, time/batch = 0.089\n",
      "Sequence 3116000/3263200 (epoch 95), batch 317650, train_loss = 0.657, time/batch = 0.092\n",
      "Sequence 3116990/3263200 (epoch 95), batch 317750, train_loss = 1.449, time/batch = 0.093\n",
      "Sequence 3117930/3263200 (epoch 95), batch 317850, train_loss = 0.569, time/batch = 0.092\n",
      "Sequence 3118890/3263200 (epoch 95), batch 317950, train_loss = 1.439, time/batch = 0.094\n",
      "Sequence 3119870/3263200 (epoch 95), batch 318050, train_loss = 1.049, time/batch = 0.092\n",
      "Sequence 3120840/3263200 (epoch 95), batch 318150, train_loss = 0.881, time/batch = 0.093\n",
      "Sequence 3121810/3263200 (epoch 95), batch 318250, train_loss = 1.362, time/batch = 0.093\n",
      "Sequence 3122800/3263200 (epoch 95), batch 318350, train_loss = 1.908, time/batch = 0.093\n",
      "Sequence 3123780/3263200 (epoch 95), batch 318450, train_loss = 0.907, time/batch = 0.092\n",
      "Sequence 3124760/3263200 (epoch 95), batch 318550, train_loss = 0.949, time/batch = 0.094\n",
      "Sequence 3125740/3263200 (epoch 95), batch 318650, train_loss = 1.519, time/batch = 0.093\n",
      "Sequence 3126720/3263200 (epoch 95), batch 318750, train_loss = 0.890, time/batch = 0.093\n",
      "Sequence 3127690/3263200 (epoch 95), batch 318850, train_loss = 1.679, time/batch = 0.092\n",
      "Sequence 3128660/3263200 (epoch 95), batch 318951, train_loss = 0.995, time/batch = 0.093\n",
      "Sequence 3129630/3263200 (epoch 95), batch 319051, train_loss = 0.881, time/batch = 0.093\n",
      "Sequence 3130620/3263200 (epoch 95), batch 319151, train_loss = 1.088, time/batch = 0.093\n",
      "Sequence 3131620/3263200 (epoch 95), batch 319251, train_loss = 1.312, time/batch = 0.093\n",
      "Sequence 3132610/3263200 (epoch 95), batch 319351, train_loss = 0.954, time/batch = 0.093\n",
      "Epoch 95 completed, average train loss 1.142872, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 3133572/3263200 (epoch 96), batch 319451, train_loss = 1.177, time/batch = 0.093\n",
      "Sequence 3134572/3263200 (epoch 96), batch 319551, train_loss = 0.917, time/batch = 0.094\n",
      "Sequence 3135562/3263200 (epoch 96), batch 319651, train_loss = 2.120, time/batch = 0.093\n",
      "Sequence 3136542/3263200 (epoch 96), batch 319751, train_loss = 0.985, time/batch = 0.093\n",
      "Sequence 3137522/3263200 (epoch 96), batch 319851, train_loss = 0.835, time/batch = 0.093\n",
      "Sequence 3138502/3263200 (epoch 96), batch 319951, train_loss = 1.350, time/batch = 0.092\n",
      "Sequence 3139462/3263200 (epoch 96), batch 320051, train_loss = 1.246, time/batch = 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 3140452/3263200 (epoch 96), batch 320151, train_loss = 1.488, time/batch = 0.091\n",
      "Sequence 3141432/3263200 (epoch 96), batch 320251, train_loss = 1.251, time/batch = 0.091\n",
      "Sequence 3142412/3263200 (epoch 96), batch 320351, train_loss = 1.232, time/batch = 0.092\n",
      "Sequence 3143382/3263200 (epoch 96), batch 320451, train_loss = 1.118, time/batch = 0.093\n",
      "Sequence 3144352/3263200 (epoch 96), batch 320551, train_loss = 1.445, time/batch = 0.091\n",
      "Sequence 3145332/3263200 (epoch 96), batch 320651, train_loss = 0.992, time/batch = 0.092\n",
      "Sequence 3146322/3263200 (epoch 96), batch 320751, train_loss = 1.177, time/batch = 0.093\n",
      "Sequence 3147312/3263200 (epoch 96), batch 320851, train_loss = 1.482, time/batch = 0.093\n",
      "Sequence 3148302/3263200 (epoch 96), batch 320951, train_loss = 1.233, time/batch = 0.092\n",
      "Sequence 3149302/3263200 (epoch 96), batch 321051, train_loss = 0.860, time/batch = 0.092\n",
      "Sequence 3150282/3263200 (epoch 96), batch 321151, train_loss = 0.965, time/batch = 0.093\n",
      "Sequence 3151252/3263200 (epoch 96), batch 321251, train_loss = 1.014, time/batch = 0.093\n",
      "Sequence 3152232/3263200 (epoch 96), batch 321351, train_loss = 0.986, time/batch = 0.093\n",
      "Sequence 3153222/3263200 (epoch 96), batch 321451, train_loss = 1.134, time/batch = 0.095\n",
      "Sequence 3154182/3263200 (epoch 96), batch 321551, train_loss = 1.381, time/batch = 0.092\n",
      "Sequence 3155152/3263200 (epoch 96), batch 321651, train_loss = 0.953, time/batch = 0.093\n",
      "Sequence 3156122/3263200 (epoch 96), batch 321751, train_loss = 1.304, time/batch = 0.092\n",
      "Sequence 3157092/3263200 (epoch 96), batch 321851, train_loss = 1.163, time/batch = 0.092\n",
      "Sequence 3158072/3263200 (epoch 96), batch 321951, train_loss = 0.910, time/batch = 0.093\n",
      "Sequence 3159062/3263200 (epoch 96), batch 322051, train_loss = 0.930, time/batch = 0.093\n",
      "Sequence 3160042/3263200 (epoch 96), batch 322151, train_loss = 0.867, time/batch = 0.092\n",
      "Sequence 3161032/3263200 (epoch 96), batch 322251, train_loss = 1.641, time/batch = 0.093\n",
      "Sequence 3162002/3263200 (epoch 96), batch 322351, train_loss = 2.078, time/batch = 0.092\n",
      "Sequence 3162992/3263200 (epoch 96), batch 322451, train_loss = 1.199, time/batch = 0.093\n",
      "Sequence 3163972/3263200 (epoch 96), batch 322551, train_loss = 0.812, time/batch = 0.093\n",
      "Sequence 3164962/3263200 (epoch 96), batch 322651, train_loss = 1.310, time/batch = 0.093\n",
      "Epoch 96 completed, average train loss 1.141591, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 3165954/3263200 (epoch 97), batch 322751, train_loss = 1.065, time/batch = 0.093\n",
      "Sequence 3166934/3263200 (epoch 97), batch 322851, train_loss = 1.280, time/batch = 0.092\n",
      "Sequence 3167904/3263200 (epoch 97), batch 322951, train_loss = 1.350, time/batch = 0.093\n",
      "Sequence 3168874/3263200 (epoch 97), batch 323051, train_loss = 1.233, time/batch = 0.093\n",
      "Sequence 3169844/3263200 (epoch 97), batch 323152, train_loss = 0.906, time/batch = 0.093\n",
      "Sequence 3170814/3263200 (epoch 97), batch 323252, train_loss = 0.725, time/batch = 0.093\n",
      "Sequence 3171784/3263200 (epoch 97), batch 323352, train_loss = 1.035, time/batch = 0.093\n",
      "Sequence 3172744/3263200 (epoch 97), batch 323452, train_loss = 0.991, time/batch = 0.093\n",
      "Sequence 3173744/3263200 (epoch 97), batch 323552, train_loss = 1.618, time/batch = 0.092\n",
      "Sequence 3174744/3263200 (epoch 97), batch 323652, train_loss = 0.965, time/batch = 0.091\n",
      "Sequence 3175724/3263200 (epoch 97), batch 323752, train_loss = 1.207, time/batch = 0.092\n",
      "Sequence 3176704/3263200 (epoch 97), batch 323852, train_loss = 0.935, time/batch = 0.093\n",
      "Sequence 3177694/3263200 (epoch 97), batch 323952, train_loss = 1.359, time/batch = 0.094\n",
      "Sequence 3178654/3263200 (epoch 97), batch 324052, train_loss = 0.919, time/batch = 0.093\n",
      "Sequence 3179624/3263200 (epoch 97), batch 324152, train_loss = 0.953, time/batch = 0.093\n",
      "Sequence 3180604/3263200 (epoch 97), batch 324252, train_loss = 1.231, time/batch = 0.093\n",
      "Sequence 3181584/3263200 (epoch 97), batch 324352, train_loss = 1.002, time/batch = 0.092\n",
      "Sequence 3182584/3263200 (epoch 97), batch 324452, train_loss = 1.054, time/batch = 0.092\n",
      "Sequence 3183574/3263200 (epoch 97), batch 324552, train_loss = 1.115, time/batch = 0.093\n",
      "Sequence 3184554/3263200 (epoch 97), batch 324652, train_loss = 1.199, time/batch = 0.092\n",
      "Sequence 3185544/3263200 (epoch 97), batch 324752, train_loss = 1.299, time/batch = 0.092\n",
      "Sequence 3186534/3263200 (epoch 97), batch 324852, train_loss = 1.118, time/batch = 0.092\n",
      "Sequence 3187494/3263200 (epoch 97), batch 324952, train_loss = 1.073, time/batch = 0.094\n",
      "Sequence 3188484/3263200 (epoch 97), batch 325052, train_loss = 2.069, time/batch = 0.093\n",
      "Sequence 3189464/3263200 (epoch 97), batch 325152, train_loss = 0.882, time/batch = 0.093\n",
      "Sequence 3190394/3263200 (epoch 97), batch 325252, train_loss = 1.348, time/batch = 0.092\n",
      "Sequence 3191374/3263200 (epoch 97), batch 325352, train_loss = 0.938, time/batch = 0.092\n",
      "Sequence 3192364/3263200 (epoch 97), batch 325452, train_loss = 0.961, time/batch = 0.092\n",
      "Sequence 3193364/3263200 (epoch 97), batch 325552, train_loss = 0.838, time/batch = 0.094\n",
      "Sequence 3194364/3263200 (epoch 97), batch 325652, train_loss = 1.077, time/batch = 0.093\n",
      "Sequence 3195354/3263200 (epoch 97), batch 325752, train_loss = 1.827, time/batch = 0.092\n",
      "Sequence 3196344/3263200 (epoch 97), batch 325852, train_loss = 1.024, time/batch = 0.092\n",
      "Sequence 3197344/3263200 (epoch 97), batch 325952, train_loss = 1.459, time/batch = 0.093\n",
      "Epoch 97 completed, average train loss 1.141410, learning rate 0.0010\n",
      "Shuffling training data...\n",
      "Sequence 3198326/3263200 (epoch 98), batch 326052, train_loss = 1.274, time/batch = 0.095\n",
      "Sequence 3199306/3263200 (epoch 98), batch 326152, train_loss = 1.042, time/batch = 0.092\n",
      "Sequence 3200286/3263200 (epoch 98), batch 326252, train_loss = 1.481, time/batch = 0.094\n",
      "Sequence 3201266/3263200 (epoch 98), batch 326352, train_loss = 1.205, time/batch = 0.092\n",
      "Sequence 3202236/3263200 (epoch 98), batch 326452, train_loss = 1.222, time/batch = 0.094\n",
      "Sequence 3203196/3263200 (epoch 98), batch 326552, train_loss = 0.489, time/batch = 0.094\n",
      "Sequence 3204136/3263200 (epoch 98), batch 326652, train_loss = 1.150, time/batch = 0.092\n",
      "Sequence 3205116/3263200 (epoch 98), batch 326752, train_loss = 1.006, time/batch = 0.093\n",
      "Sequence 3206096/3263200 (epoch 98), batch 326852, train_loss = 0.992, time/batch = 0.092\n",
      "Sequence 3207066/3263200 (epoch 98), batch 326952, train_loss = 0.992, time/batch = 0.092\n",
      "Sequence 3208046/3263200 (epoch 98), batch 327052, train_loss = 0.800, time/batch = 0.092\n",
      "Sequence 3208996/3263200 (epoch 98), batch 327152, train_loss = 0.856, time/batch = 0.093\n",
      "Sequence 3209976/3263200 (epoch 98), batch 327252, train_loss = 1.751, time/batch = 0.096\n",
      "Sequence 3210956/3263200 (epoch 98), batch 327352, train_loss = 1.295, time/batch = 0.093\n",
      "Sequence 3211956/3263200 (epoch 98), batch 327452, train_loss = 1.741, time/batch = 0.093\n",
      "Sequence 3212946/3263200 (epoch 98), batch 327552, train_loss = 1.750, time/batch = 0.092\n",
      "Sequence 3213946/3263200 (epoch 98), batch 327652, train_loss = 0.972, time/batch = 0.094\n",
      "Sequence 3214946/3263200 (epoch 98), batch 327752, train_loss = 1.049, time/batch = 0.093\n",
      "Sequence 3215916/3263200 (epoch 98), batch 327852, train_loss = 1.426, time/batch = 0.093\n",
      "Sequence 3216906/3263200 (epoch 98), batch 327952, train_loss = 0.940, time/batch = 0.091\n",
      "Sequence 3217896/3263200 (epoch 98), batch 328052, train_loss = 1.345, time/batch = 0.093\n",
      "Sequence 3218876/3263200 (epoch 98), batch 328152, train_loss = 0.750, time/batch = 0.093\n",
      "Sequence 3219866/3263200 (epoch 98), batch 328252, train_loss = 1.033, time/batch = 0.092\n",
      "Sequence 3220846/3263200 (epoch 98), batch 328352, train_loss = 0.994, time/batch = 0.093\n",
      "Sequence 3221826/3263200 (epoch 98), batch 328452, train_loss = 1.187, time/batch = 0.092\n",
      "Sequence 3222806/3263200 (epoch 98), batch 328553, train_loss = 0.591, time/batch = 0.093\n",
      "Sequence 3223786/3263200 (epoch 98), batch 328653, train_loss = 1.724, time/batch = 0.092\n",
      "Sequence 3224746/3263200 (epoch 98), batch 328753, train_loss = 0.905, time/batch = 0.093\n",
      "Sequence 3225736/3263200 (epoch 98), batch 328853, train_loss = 1.432, time/batch = 0.093\n",
      "Sequence 3226736/3263200 (epoch 98), batch 328953, train_loss = 1.086, time/batch = 0.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 3227726/3263200 (epoch 98), batch 329053, train_loss = 0.987, time/batch = 0.092\n",
      "Sequence 3228716/3263200 (epoch 98), batch 329153, train_loss = 0.897, time/batch = 0.093\n",
      "Sequence 3229706/3263200 (epoch 98), batch 329253, train_loss = 1.545, time/batch = 0.093\n",
      "Epoch 98 completed, average train loss 1.140088, learning rate 0.0010\n",
      "Sequence 3230708/3263200 (epoch 99), batch 329353, train_loss = 1.225, time/batch = 0.093\n",
      "Shuffling training data...\n",
      "Sequence 3231688/3263200 (epoch 99), batch 329453, train_loss = 0.893, time/batch = 0.093\n",
      "Sequence 3232668/3263200 (epoch 99), batch 329553, train_loss = 1.452, time/batch = 0.092\n",
      "Sequence 3233638/3263200 (epoch 99), batch 329653, train_loss = 1.260, time/batch = 0.092\n",
      "Sequence 3234618/3263200 (epoch 99), batch 329753, train_loss = 1.327, time/batch = 0.093\n",
      "Sequence 3235618/3263200 (epoch 99), batch 329853, train_loss = 0.889, time/batch = 0.094\n",
      "Sequence 3236578/3263200 (epoch 99), batch 329953, train_loss = 1.272, time/batch = 0.093\n",
      "Sequence 3237548/3263200 (epoch 99), batch 330053, train_loss = 1.276, time/batch = 0.093\n",
      "Sequence 3238518/3263200 (epoch 99), batch 330153, train_loss = 1.090, time/batch = 0.094\n",
      "Sequence 3239498/3263200 (epoch 99), batch 330253, train_loss = 0.941, time/batch = 0.093\n",
      "Sequence 3240488/3263200 (epoch 99), batch 330353, train_loss = 0.851, time/batch = 0.093\n",
      "Sequence 3241458/3263200 (epoch 99), batch 330453, train_loss = 1.061, time/batch = 0.094\n",
      "Sequence 3242438/3263200 (epoch 99), batch 330553, train_loss = 1.172, time/batch = 0.091\n",
      "Sequence 3243408/3263200 (epoch 99), batch 330653, train_loss = 0.931, time/batch = 0.093\n",
      "Sequence 3244388/3263200 (epoch 99), batch 330753, train_loss = 1.110, time/batch = 0.093\n",
      "Sequence 3245358/3263200 (epoch 99), batch 330853, train_loss = 1.135, time/batch = 0.093\n",
      "Sequence 3246338/3263200 (epoch 99), batch 330953, train_loss = 0.948, time/batch = 0.092\n",
      "Sequence 3247328/3263200 (epoch 99), batch 331053, train_loss = 0.739, time/batch = 0.093\n",
      "Sequence 3248308/3263200 (epoch 99), batch 331153, train_loss = 0.729, time/batch = 0.092\n",
      "Sequence 3249298/3263200 (epoch 99), batch 331253, train_loss = 1.387, time/batch = 0.093\n",
      "Sequence 3250288/3263200 (epoch 99), batch 331353, train_loss = 0.937, time/batch = 0.092\n",
      "Sequence 3251278/3263200 (epoch 99), batch 331453, train_loss = 0.812, time/batch = 0.093\n",
      "Sequence 3252258/3263200 (epoch 99), batch 331553, train_loss = 0.883, time/batch = 0.090\n",
      "Sequence 3253238/3263200 (epoch 99), batch 331653, train_loss = 1.248, time/batch = 0.091\n",
      "Sequence 3254218/3263200 (epoch 99), batch 331753, train_loss = 0.984, time/batch = 0.092\n",
      "Sequence 3255198/3263200 (epoch 99), batch 331853, train_loss = 1.673, time/batch = 0.093\n",
      "Sequence 3256188/3263200 (epoch 99), batch 331953, train_loss = 1.172, time/batch = 0.091\n",
      "Sequence 3257158/3263200 (epoch 99), batch 332053, train_loss = 1.626, time/batch = 0.091\n",
      "Sequence 3258128/3263200 (epoch 99), batch 332153, train_loss = 1.165, time/batch = 0.093\n",
      "Sequence 3259108/3263200 (epoch 99), batch 332253, train_loss = 1.069, time/batch = 0.093\n",
      "Sequence 3260098/3263200 (epoch 99), batch 332353, train_loss = 0.966, time/batch = 0.093\n",
      "Sequence 3261088/3263200 (epoch 99), batch 332453, train_loss = 0.996, time/batch = 0.092\n",
      "Sequence 3262088/3263200 (epoch 99), batch 332553, train_loss = 1.524, time/batch = 0.092\n",
      "Sequence 3263088/3263200 (epoch 99), batch 332653, train_loss = 0.807, time/batch = 0.092\n",
      "Epoch 99 completed, average train loss 1.137166, learning rate 0.0010\n",
      "model saved.\n"
     ]
    }
   ],
   "source": [
    "#args.num_epochs = 4\n",
    "#args.save_every = 2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  #with tf.device('/device:GPU:2'):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------#\n",
    "#  print training loss   #\n",
    "#------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Alternative training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dim_rec', type=int, default=128,\n",
    "                     help='size of RNN hidden state')\n",
    "parser.add_argument('--num_layers', type=int, default=2,\n",
    "                     help='number of layers in the RNN. ')\n",
    "parser.add_argument('--batch_size', type=int, default=10,\n",
    "                     help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=200,\n",
    "                     help='number of epochs')\n",
    "parser.add_argument('--save_every', type=int, default=10,\n",
    "                     help='save frequency by epoches')\n",
    "parser.add_argument('--model_dir', type=str, default='checkpoints',\n",
    "                     help='directory to save model to')\n",
    "parser.add_argument('--summary_dir', type=str, default='summary',\n",
    "                     help='directory to save tensorboard info')\n",
    "parser.add_argument('--max_grad_norm', type=float, default=1.,\n",
    "                     help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001,\n",
    "                     help='learning rate')\n",
    "parser.add_argument('--decay_rate', type=float, default=1.0,\n",
    "                     help='decay rate for the optimizer')\n",
    "parser.add_argument('--num_mixture', type=int, default=2,\n",
    "                     help='number of gaussian mixtures')\n",
    "parser.add_argument('--data_scale', type=float, default=1000,\n",
    "                     help='factor to scale raw data down by')\n",
    "parser.add_argument('--load_model', type=str, default=None,\n",
    "                     help='Reload a model checkpoint and restore training.' )\n",
    "parser.add_argument('--bptt_length', type=int, default=120,\n",
    "                     help='How many steps should the gradients pass back.' )\n",
    "parser.add_argument('--loss_form', type=str, default='mse',\n",
    "                     help='mse / gmm' )\n",
    "parser.add_argument('--constraint_factor', type=float, default=0.,\n",
    "                     help='the weight for constraint term in the cost function.' )\n",
    "  \n",
    "args = parser.parse_args(['--num_epochs','200'])\n",
    "\n",
    "args.num_epochs = 4\n",
    "args.save_every = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python36.zip', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/lib-dynload', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/Sphinx-1.5.6-py3.6.egg', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/svgwrite-1.1.6-py3.6.egg', '/home/mye/tools/anacondas3/envs/tensorflow/lib/python3.6/site-packages/IPython/extensions', '/home/mye/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#reload(sys)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal length of the training data is 169\n",
      "Training data case distribution: [5348, 5108, 4904, 1208, 1260, 1224, 916, 1180, 1216, 1044, 876, 456, 1264, 992, 1080, 932, 1008, 420, 1160, 1036]\n",
      "Validation data case distribution: [70, 67, 65, 15, 17, 16, 12, 15, 16, 14, 12, 6, 17, 13, 14, 13, 14, 5, 15, 13]\n",
      "Shuffling training data...\n",
      "Shuffling training data...\n",
      "('i:', 0, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3, ' x.shape:', (10, 120, 6))\n",
      "('i:', 4, ' x.shape:', (10, 120, 6))\n",
      "('i:', 5, ' x.shape:', (10, 120, 6))\n",
      "('i:', 6, ' x.shape:', (10, 120, 6))\n",
      "('i:', 7, ' x.shape:', (10, 120, 6))\n",
      "('i:', 8, ' x.shape:', (10, 120, 6))\n",
      "('i:', 9, ' x.shape:', (10, 120, 6))\n",
      "('i:', 10, ' x.shape:', (10, 120, 6))\n",
      "('i:', 11, ' x.shape:', (10, 120, 6))\n",
      "('i:', 12, ' x.shape:', (10, 120, 6))\n",
      "('i:', 13, ' x.shape:', (10, 120, 6))\n",
      "('i:', 14, ' x.shape:', (10, 120, 6))\n",
      "('i:', 15, ' x.shape:', (10, 120, 6))\n",
      "('i:', 16, ' x.shape:', (10, 120, 6))\n",
      "('i:', 17, ' x.shape:', (10, 120, 6))\n",
      "('i:', 18, ' x.shape:', (10, 120, 6))\n",
      "('i:', 19, ' x.shape:', (10, 120, 6))\n",
      "('i:', 20, ' x.shape:', (10, 120, 6))\n",
      "('i:', 21, ' x.shape:', (10, 120, 6))\n",
      "('i:', 22, ' x.shape:', (10, 120, 6))\n",
      "('i:', 23, ' x.shape:', (10, 120, 6))\n",
      "('i:', 24, ' x.shape:', (10, 120, 6))\n",
      "('i:', 25, ' x.shape:', (10, 120, 6))\n",
      "('i:', 26, ' x.shape:', (10, 240, 6))\n",
      "('i:', 27, ' x.shape:', (10, 120, 6))\n",
      "('i:', 28, ' x.shape:', (10, 120, 6))\n",
      "('i:', 29, ' x.shape:', (10, 120, 6))\n",
      "('i:', 30, ' x.shape:', (10, 120, 6))\n",
      "('i:', 31, ' x.shape:', (10, 120, 6))\n",
      "('i:', 32, ' x.shape:', (10, 120, 6))\n",
      "('i:', 33, ' x.shape:', (10, 120, 6))\n",
      "('i:', 34, ' x.shape:', (10, 120, 6))\n",
      "('i:', 35, ' x.shape:', (10, 120, 6))\n",
      "('i:', 36, ' x.shape:', (10, 120, 6))\n",
      "('i:', 37, ' x.shape:', (10, 120, 6))\n",
      "('i:', 38, ' x.shape:', (10, 120, 6))\n",
      "('i:', 39, ' x.shape:', (10, 120, 6))\n",
      "('i:', 40, ' x.shape:', (10, 120, 6))\n",
      "('i:', 41, ' x.shape:', (10, 120, 6))\n",
      "('i:', 42, ' x.shape:', (10, 120, 6))\n",
      "('i:', 43, ' x.shape:', (10, 120, 6))\n",
      "('i:', 44, ' x.shape:', (10, 120, 6))\n",
      "('i:', 45, ' x.shape:', (10, 120, 6))\n",
      "('i:', 46, ' x.shape:', (10, 120, 6))\n",
      "('i:', 47, ' x.shape:', (10, 120, 6))\n",
      "('i:', 48, ' x.shape:', (10, 120, 6))\n",
      "('i:', 49, ' x.shape:', (10, 120, 6))\n",
      "('i:', 50, ' x.shape:', (10, 120, 6))\n",
      "('i:', 51, ' x.shape:', (10, 120, 6))\n",
      "('i:', 52, ' x.shape:', (10, 120, 6))\n",
      "('i:', 53, ' x.shape:', (10, 120, 6))\n",
      "('i:', 54, ' x.shape:', (10, 120, 6))\n",
      "('i:', 55, ' x.shape:', (10, 120, 6))\n",
      "('i:', 56, ' x.shape:', (10, 120, 6))\n",
      "('i:', 57, ' x.shape:', (10, 120, 6))\n",
      "('i:', 58, ' x.shape:', (10, 120, 6))\n",
      "('i:', 59, ' x.shape:', (10, 120, 6))\n",
      "('i:', 60, ' x.shape:', (10, 120, 6))\n",
      "('i:', 61, ' x.shape:', (10, 120, 6))\n",
      "('i:', 62, ' x.shape:', (10, 120, 6))\n",
      "('i:', 63, ' x.shape:', (10, 120, 6))\n",
      "('i:', 64, ' x.shape:', (10, 120, 6))\n",
      "('i:', 65, ' x.shape:', (10, 120, 6))\n",
      "('i:', 66, ' x.shape:', (10, 120, 6))\n",
      "('i:', 67, ' x.shape:', (10, 120, 6))\n",
      "('i:', 68, ' x.shape:', (10, 120, 6))\n",
      "('i:', 69, ' x.shape:', (10, 120, 6))\n",
      "('i:', 70, ' x.shape:', (10, 120, 6))\n",
      "('i:', 71, ' x.shape:', (10, 120, 6))\n",
      "('i:', 72, ' x.shape:', (10, 120, 6))\n",
      "('i:', 73, ' x.shape:', (10, 120, 6))\n",
      "('i:', 74, ' x.shape:', (10, 120, 6))\n",
      "('i:', 75, ' x.shape:', (10, 120, 6))\n",
      "('i:', 76, ' x.shape:', (10, 120, 6))\n",
      "('i:', 77, ' x.shape:', (10, 120, 6))\n",
      "('i:', 78, ' x.shape:', (10, 240, 6))\n",
      "('i:', 79, ' x.shape:', (10, 120, 6))\n",
      "('i:', 80, ' x.shape:', (10, 120, 6))\n",
      "('i:', 81, ' x.shape:', (10, 120, 6))\n",
      "('i:', 82, ' x.shape:', (10, 120, 6))\n",
      "('i:', 83, ' x.shape:', (10, 120, 6))\n",
      "('i:', 84, ' x.shape:', (10, 120, 6))\n",
      "('i:', 85, ' x.shape:', (10, 120, 6))\n",
      "('i:', 86, ' x.shape:', (10, 120, 6))\n",
      "('i:', 87, ' x.shape:', (10, 120, 6))\n",
      "('i:', 88, ' x.shape:', (10, 120, 6))\n",
      "('i:', 89, ' x.shape:', (10, 120, 6))\n",
      "('i:', 90, ' x.shape:', (10, 120, 6))\n",
      "('i:', 91, ' x.shape:', (10, 120, 6))\n",
      "('i:', 92, ' x.shape:', (10, 120, 6))\n",
      "('i:', 93, ' x.shape:', (10, 120, 6))\n",
      "('i:', 94, ' x.shape:', (10, 120, 6))\n",
      "('i:', 95, ' x.shape:', (10, 120, 6))\n",
      "('i:', 96, ' x.shape:', (10, 120, 6))\n",
      "('i:', 97, ' x.shape:', (10, 120, 6))\n",
      "('i:', 98, ' x.shape:', (10, 120, 6))\n",
      "('i:', 99, ' x.shape:', (10, 120, 6))\n",
      "('i:', 100, ' x.shape:', (10, 120, 6))\n",
      "('i:', 101, ' x.shape:', (10, 120, 6))\n",
      "('i:', 102, ' x.shape:', (10, 120, 6))\n",
      "('i:', 103, ' x.shape:', (10, 120, 6))\n",
      "('i:', 104, ' x.shape:', (10, 120, 6))\n",
      "('i:', 105, ' x.shape:', (10, 120, 6))\n",
      "('i:', 106, ' x.shape:', (10, 120, 6))\n",
      "('i:', 107, ' x.shape:', (10, 120, 6))\n",
      "('i:', 108, ' x.shape:', (10, 120, 6))\n",
      "('i:', 109, ' x.shape:', (10, 120, 6))\n",
      "('i:', 110, ' x.shape:', (10, 120, 6))\n",
      "('i:', 111, ' x.shape:', (10, 120, 6))\n",
      "('i:', 112, ' x.shape:', (10, 120, 6))\n",
      "('i:', 113, ' x.shape:', (10, 120, 6))\n",
      "('i:', 114, ' x.shape:', (10, 120, 6))\n",
      "('i:', 115, ' x.shape:', (10, 120, 6))\n",
      "('i:', 116, ' x.shape:', (10, 120, 6))\n",
      "('i:', 117, ' x.shape:', (10, 120, 6))\n",
      "('i:', 118, ' x.shape:', (10, 120, 6))\n",
      "('i:', 119, ' x.shape:', (10, 120, 6))\n",
      "('i:', 120, ' x.shape:', (10, 120, 6))\n",
      "('i:', 121, ' x.shape:', (10, 120, 6))\n",
      "('i:', 122, ' x.shape:', (10, 120, 6))\n",
      "('i:', 123, ' x.shape:', (10, 120, 6))\n",
      "('i:', 124, ' x.shape:', (10, 120, 6))\n",
      "('i:', 125, ' x.shape:', (10, 120, 6))\n",
      "('i:', 126, ' x.shape:', (10, 120, 6))\n",
      "('i:', 127, ' x.shape:', (10, 120, 6))\n",
      "('i:', 128, ' x.shape:', (10, 120, 6))\n",
      "('i:', 129, ' x.shape:', (10, 120, 6))\n",
      "('i:', 130, ' x.shape:', (10, 120, 6))\n",
      "('i:', 131, ' x.shape:', (10, 120, 6))\n",
      "('i:', 132, ' x.shape:', (10, 120, 6))\n",
      "('i:', 133, ' x.shape:', (10, 120, 6))\n",
      "('i:', 134, ' x.shape:', (10, 120, 6))\n",
      "('i:', 135, ' x.shape:', (10, 120, 6))\n",
      "('i:', 136, ' x.shape:', (10, 120, 6))\n",
      "('i:', 137, ' x.shape:', (10, 120, 6))\n",
      "('i:', 138, ' x.shape:', (10, 120, 6))\n",
      "('i:', 139, ' x.shape:', (10, 120, 6))\n",
      "('i:', 140, ' x.shape:', (10, 120, 6))\n",
      "('i:', 141, ' x.shape:', (10, 120, 6))\n",
      "('i:', 142, ' x.shape:', (10, 120, 6))\n",
      "('i:', 143, ' x.shape:', (10, 120, 6))\n",
      "('i:', 144, ' x.shape:', (10, 120, 6))\n",
      "('i:', 145, ' x.shape:', (10, 120, 6))\n",
      "('i:', 146, ' x.shape:', (10, 120, 6))\n",
      "('i:', 147, ' x.shape:', (10, 120, 6))\n",
      "('i:', 148, ' x.shape:', (10, 120, 6))\n",
      "('i:', 149, ' x.shape:', (10, 120, 6))\n",
      "('i:', 150, ' x.shape:', (10, 120, 6))\n",
      "('i:', 151, ' x.shape:', (10, 120, 6))\n",
      "('i:', 152, ' x.shape:', (10, 120, 6))\n",
      "('i:', 153, ' x.shape:', (10, 120, 6))\n",
      "('i:', 154, ' x.shape:', (10, 120, 6))\n",
      "('i:', 155, ' x.shape:', (10, 120, 6))\n",
      "('i:', 156, ' x.shape:', (10, 120, 6))\n",
      "('i:', 157, ' x.shape:', (10, 120, 6))\n",
      "('i:', 158, ' x.shape:', (10, 120, 6))\n",
      "('i:', 159, ' x.shape:', (10, 120, 6))\n",
      "('i:', 160, ' x.shape:', (10, 120, 6))\n",
      "('i:', 161, ' x.shape:', (10, 120, 6))\n",
      "('i:', 162, ' x.shape:', (10, 120, 6))\n",
      "('i:', 163, ' x.shape:', (10, 120, 6))\n",
      "('i:', 164, ' x.shape:', (10, 120, 6))\n",
      "('i:', 165, ' x.shape:', (10, 120, 6))\n",
      "('i:', 166, ' x.shape:', (10, 120, 6))\n",
      "('i:', 167, ' x.shape:', (10, 120, 6))\n",
      "('i:', 168, ' x.shape:', (10, 120, 6))\n",
      "('i:', 169, ' x.shape:', (10, 120, 6))\n",
      "('i:', 170, ' x.shape:', (10, 120, 6))\n",
      "('i:', 171, ' x.shape:', (10, 120, 6))\n",
      "('i:', 172, ' x.shape:', (10, 120, 6))\n",
      "('i:', 173, ' x.shape:', (10, 120, 6))\n",
      "('i:', 174, ' x.shape:', (10, 120, 6))\n",
      "('i:', 175, ' x.shape:', (10, 120, 6))\n",
      "('i:', 176, ' x.shape:', (10, 120, 6))\n",
      "('i:', 177, ' x.shape:', (10, 120, 6))\n",
      "('i:', 178, ' x.shape:', (10, 120, 6))\n",
      "('i:', 179, ' x.shape:', (10, 120, 6))\n",
      "('i:', 180, ' x.shape:', (10, 120, 6))\n",
      "('i:', 181, ' x.shape:', (10, 120, 6))\n",
      "('i:', 182, ' x.shape:', (10, 120, 6))\n",
      "('i:', 183, ' x.shape:', (10, 120, 6))\n",
      "('i:', 184, ' x.shape:', (10, 120, 6))\n",
      "('i:', 185, ' x.shape:', (10, 120, 6))\n",
      "('i:', 186, ' x.shape:', (10, 120, 6))\n",
      "('i:', 187, ' x.shape:', (10, 120, 6))\n",
      "('i:', 188, ' x.shape:', (10, 120, 6))\n",
      "('i:', 189, ' x.shape:', (10, 120, 6))\n",
      "('i:', 190, ' x.shape:', (10, 120, 6))\n",
      "('i:', 191, ' x.shape:', (10, 120, 6))\n",
      "('i:', 192, ' x.shape:', (10, 120, 6))\n",
      "('i:', 193, ' x.shape:', (10, 120, 6))\n",
      "('i:', 194, ' x.shape:', (10, 120, 6))\n",
      "('i:', 195, ' x.shape:', (10, 240, 6))\n",
      "('i:', 196, ' x.shape:', (10, 120, 6))\n",
      "('i:', 197, ' x.shape:', (10, 120, 6))\n",
      "('i:', 198, ' x.shape:', (10, 120, 6))\n",
      "('i:', 199, ' x.shape:', (10, 120, 6))\n",
      "('i:', 200, ' x.shape:', (10, 120, 6))\n",
      "('i:', 201, ' x.shape:', (10, 120, 6))\n",
      "('i:', 202, ' x.shape:', (10, 120, 6))\n",
      "('i:', 203, ' x.shape:', (10, 120, 6))\n",
      "('i:', 204, ' x.shape:', (10, 120, 6))\n",
      "('i:', 205, ' x.shape:', (10, 120, 6))\n",
      "('i:', 206, ' x.shape:', (10, 120, 6))\n",
      "('i:', 207, ' x.shape:', (10, 120, 6))\n",
      "('i:', 208, ' x.shape:', (10, 120, 6))\n",
      "('i:', 209, ' x.shape:', (10, 120, 6))\n",
      "('i:', 210, ' x.shape:', (10, 120, 6))\n",
      "('i:', 211, ' x.shape:', (10, 120, 6))\n",
      "('i:', 212, ' x.shape:', (10, 120, 6))\n",
      "('i:', 213, ' x.shape:', (10, 120, 6))\n",
      "('i:', 214, ' x.shape:', (10, 120, 6))\n",
      "('i:', 215, ' x.shape:', (10, 120, 6))\n",
      "('i:', 216, ' x.shape:', (10, 120, 6))\n",
      "('i:', 217, ' x.shape:', (10, 120, 6))\n",
      "('i:', 218, ' x.shape:', (10, 120, 6))\n",
      "('i:', 219, ' x.shape:', (10, 120, 6))\n",
      "('i:', 220, ' x.shape:', (10, 120, 6))\n",
      "('i:', 221, ' x.shape:', (10, 120, 6))\n",
      "('i:', 222, ' x.shape:', (10, 120, 6))\n",
      "('i:', 223, ' x.shape:', (10, 120, 6))\n",
      "('i:', 224, ' x.shape:', (10, 120, 6))\n",
      "('i:', 225, ' x.shape:', (10, 120, 6))\n",
      "('i:', 226, ' x.shape:', (10, 120, 6))\n",
      "('i:', 227, ' x.shape:', (10, 120, 6))\n",
      "('i:', 228, ' x.shape:', (10, 120, 6))\n",
      "('i:', 229, ' x.shape:', (10, 120, 6))\n",
      "('i:', 230, ' x.shape:', (10, 120, 6))\n",
      "('i:', 231, ' x.shape:', (10, 120, 6))\n",
      "('i:', 232, ' x.shape:', (10, 120, 6))\n",
      "('i:', 233, ' x.shape:', (10, 120, 6))\n",
      "('i:', 234, ' x.shape:', (10, 120, 6))\n",
      "('i:', 235, ' x.shape:', (10, 120, 6))\n",
      "('i:', 236, ' x.shape:', (10, 120, 6))\n",
      "('i:', 237, ' x.shape:', (10, 120, 6))\n",
      "('i:', 238, ' x.shape:', (10, 120, 6))\n",
      "('i:', 239, ' x.shape:', (10, 120, 6))\n",
      "('i:', 240, ' x.shape:', (10, 120, 6))\n",
      "('i:', 241, ' x.shape:', (10, 120, 6))\n",
      "('i:', 242, ' x.shape:', (10, 120, 6))\n",
      "('i:', 243, ' x.shape:', (10, 120, 6))\n",
      "('i:', 244, ' x.shape:', (10, 120, 6))\n",
      "('i:', 245, ' x.shape:', (10, 120, 6))\n",
      "('i:', 246, ' x.shape:', (10, 120, 6))\n",
      "('i:', 247, ' x.shape:', (10, 120, 6))\n",
      "('i:', 248, ' x.shape:', (10, 120, 6))\n",
      "('i:', 249, ' x.shape:', (10, 120, 6))\n",
      "('i:', 250, ' x.shape:', (10, 120, 6))\n",
      "('i:', 251, ' x.shape:', (10, 120, 6))\n",
      "('i:', 252, ' x.shape:', (10, 120, 6))\n",
      "('i:', 253, ' x.shape:', (10, 120, 6))\n",
      "('i:', 254, ' x.shape:', (10, 120, 6))\n",
      "('i:', 255, ' x.shape:', (10, 120, 6))\n",
      "('i:', 256, ' x.shape:', (10, 120, 6))\n",
      "('i:', 257, ' x.shape:', (10, 120, 6))\n",
      "('i:', 258, ' x.shape:', (10, 120, 6))\n",
      "('i:', 259, ' x.shape:', (10, 120, 6))\n",
      "('i:', 260, ' x.shape:', (10, 120, 6))\n",
      "('i:', 261, ' x.shape:', (10, 120, 6))\n",
      "('i:', 262, ' x.shape:', (10, 120, 6))\n",
      "('i:', 263, ' x.shape:', (10, 120, 6))\n",
      "('i:', 264, ' x.shape:', (10, 120, 6))\n",
      "('i:', 265, ' x.shape:', (10, 120, 6))\n",
      "('i:', 266, ' x.shape:', (10, 120, 6))\n",
      "('i:', 267, ' x.shape:', (10, 120, 6))\n",
      "('i:', 268, ' x.shape:', (10, 120, 6))\n",
      "('i:', 269, ' x.shape:', (10, 120, 6))\n",
      "('i:', 270, ' x.shape:', (10, 120, 6))\n",
      "('i:', 271, ' x.shape:', (10, 120, 6))\n",
      "('i:', 272, ' x.shape:', (10, 120, 6))\n",
      "('i:', 273, ' x.shape:', (10, 120, 6))\n",
      "('i:', 274, ' x.shape:', (10, 240, 6))\n",
      "('i:', 275, ' x.shape:', (10, 120, 6))\n",
      "('i:', 276, ' x.shape:', (10, 120, 6))\n",
      "('i:', 277, ' x.shape:', (10, 120, 6))\n",
      "('i:', 278, ' x.shape:', (10, 120, 6))\n",
      "('i:', 279, ' x.shape:', (10, 120, 6))\n",
      "('i:', 280, ' x.shape:', (10, 120, 6))\n",
      "('i:', 281, ' x.shape:', (10, 120, 6))\n",
      "('i:', 282, ' x.shape:', (10, 120, 6))\n",
      "('i:', 283, ' x.shape:', (10, 120, 6))\n",
      "('i:', 284, ' x.shape:', (10, 120, 6))\n",
      "('i:', 285, ' x.shape:', (10, 120, 6))\n",
      "('i:', 286, ' x.shape:', (10, 120, 6))\n",
      "('i:', 287, ' x.shape:', (10, 120, 6))\n",
      "('i:', 288, ' x.shape:', (10, 120, 6))\n",
      "('i:', 289, ' x.shape:', (10, 120, 6))\n",
      "('i:', 290, ' x.shape:', (10, 120, 6))\n",
      "('i:', 291, ' x.shape:', (10, 120, 6))\n",
      "('i:', 292, ' x.shape:', (10, 120, 6))\n",
      "('i:', 293, ' x.shape:', (10, 120, 6))\n",
      "('i:', 294, ' x.shape:', (10, 120, 6))\n",
      "('i:', 295, ' x.shape:', (10, 120, 6))\n",
      "('i:', 296, ' x.shape:', (10, 120, 6))\n",
      "('i:', 297, ' x.shape:', (10, 120, 6))\n",
      "('i:', 298, ' x.shape:', (10, 120, 6))\n",
      "('i:', 299, ' x.shape:', (10, 120, 6))\n",
      "('i:', 300, ' x.shape:', (10, 120, 6))\n",
      "('i:', 301, ' x.shape:', (10, 120, 6))\n",
      "('i:', 302, ' x.shape:', (10, 120, 6))\n",
      "('i:', 303, ' x.shape:', (10, 120, 6))\n",
      "('i:', 304, ' x.shape:', (10, 120, 6))\n",
      "('i:', 305, ' x.shape:', (10, 120, 6))\n",
      "('i:', 306, ' x.shape:', (10, 120, 6))\n",
      "('i:', 307, ' x.shape:', (10, 120, 6))\n",
      "('i:', 308, ' x.shape:', (10, 120, 6))\n",
      "('i:', 309, ' x.shape:', (10, 120, 6))\n",
      "('i:', 310, ' x.shape:', (10, 120, 6))\n",
      "('i:', 311, ' x.shape:', (10, 120, 6))\n",
      "('i:', 312, ' x.shape:', (10, 120, 6))\n",
      "('i:', 313, ' x.shape:', (10, 120, 6))\n",
      "('i:', 314, ' x.shape:', (10, 120, 6))\n",
      "('i:', 315, ' x.shape:', (10, 120, 6))\n",
      "('i:', 316, ' x.shape:', (10, 120, 6))\n",
      "('i:', 317, ' x.shape:', (10, 240, 6))\n",
      "('i:', 318, ' x.shape:', (10, 120, 6))\n",
      "('i:', 319, ' x.shape:', (10, 120, 6))\n",
      "('i:', 320, ' x.shape:', (10, 120, 6))\n",
      "('i:', 321, ' x.shape:', (10, 120, 6))\n",
      "('i:', 322, ' x.shape:', (10, 120, 6))\n",
      "('i:', 323, ' x.shape:', (10, 120, 6))\n",
      "('i:', 324, ' x.shape:', (10, 120, 6))\n",
      "('i:', 325, ' x.shape:', (10, 120, 6))\n",
      "('i:', 326, ' x.shape:', (10, 120, 6))\n",
      "('i:', 327, ' x.shape:', (10, 120, 6))\n",
      "('i:', 328, ' x.shape:', (10, 120, 6))\n",
      "('i:', 329, ' x.shape:', (10, 120, 6))\n",
      "('i:', 330, ' x.shape:', (10, 120, 6))\n",
      "('i:', 331, ' x.shape:', (10, 120, 6))\n",
      "('i:', 332, ' x.shape:', (10, 120, 6))\n",
      "('i:', 333, ' x.shape:', (10, 120, 6))\n",
      "('i:', 334, ' x.shape:', (10, 120, 6))\n",
      "('i:', 335, ' x.shape:', (10, 120, 6))\n",
      "('i:', 336, ' x.shape:', (10, 120, 6))\n",
      "('i:', 337, ' x.shape:', (10, 120, 6))\n",
      "('i:', 338, ' x.shape:', (10, 120, 6))\n",
      "('i:', 339, ' x.shape:', (10, 120, 6))\n",
      "('i:', 340, ' x.shape:', (10, 120, 6))\n",
      "('i:', 341, ' x.shape:', (10, 120, 6))\n",
      "('i:', 342, ' x.shape:', (10, 120, 6))\n",
      "('i:', 343, ' x.shape:', (10, 120, 6))\n",
      "('i:', 344, ' x.shape:', (10, 120, 6))\n",
      "('i:', 345, ' x.shape:', (10, 120, 6))\n",
      "('i:', 346, ' x.shape:', (10, 120, 6))\n",
      "('i:', 347, ' x.shape:', (10, 120, 6))\n",
      "('i:', 348, ' x.shape:', (10, 120, 6))\n",
      "('i:', 349, ' x.shape:', (10, 120, 6))\n",
      "('i:', 350, ' x.shape:', (10, 120, 6))\n",
      "('i:', 351, ' x.shape:', (10, 120, 6))\n",
      "('i:', 352, ' x.shape:', (10, 120, 6))\n",
      "('i:', 353, ' x.shape:', (10, 120, 6))\n",
      "('i:', 354, ' x.shape:', (10, 120, 6))\n",
      "('i:', 355, ' x.shape:', (10, 120, 6))\n",
      "('i:', 356, ' x.shape:', (10, 120, 6))\n",
      "('i:', 357, ' x.shape:', (10, 120, 6))\n",
      "('i:', 358, ' x.shape:', (10, 120, 6))\n",
      "('i:', 359, ' x.shape:', (10, 120, 6))\n",
      "('i:', 360, ' x.shape:', (10, 120, 6))\n",
      "('i:', 361, ' x.shape:', (10, 120, 6))\n",
      "('i:', 362, ' x.shape:', (10, 120, 6))\n",
      "('i:', 363, ' x.shape:', (10, 120, 6))\n",
      "('i:', 364, ' x.shape:', (10, 120, 6))\n",
      "('i:', 365, ' x.shape:', (10, 120, 6))\n",
      "('i:', 366, ' x.shape:', (10, 120, 6))\n",
      "('i:', 367, ' x.shape:', (10, 120, 6))\n",
      "('i:', 368, ' x.shape:', (10, 120, 6))\n",
      "('i:', 369, ' x.shape:', (10, 120, 6))\n",
      "('i:', 370, ' x.shape:', (10, 120, 6))\n",
      "('i:', 371, ' x.shape:', (10, 120, 6))\n",
      "('i:', 372, ' x.shape:', (10, 120, 6))\n",
      "('i:', 373, ' x.shape:', (10, 120, 6))\n",
      "('i:', 374, ' x.shape:', (10, 120, 6))\n",
      "('i:', 375, ' x.shape:', (10, 120, 6))\n",
      "('i:', 376, ' x.shape:', (10, 120, 6))\n",
      "('i:', 377, ' x.shape:', (10, 120, 6))\n",
      "('i:', 378, ' x.shape:', (10, 120, 6))\n",
      "('i:', 379, ' x.shape:', (10, 120, 6))\n",
      "('i:', 380, ' x.shape:', (10, 120, 6))\n",
      "('i:', 381, ' x.shape:', (10, 120, 6))\n",
      "('i:', 382, ' x.shape:', (10, 120, 6))\n",
      "('i:', 383, ' x.shape:', (10, 120, 6))\n",
      "('i:', 384, ' x.shape:', (10, 120, 6))\n",
      "('i:', 385, ' x.shape:', (10, 120, 6))\n",
      "('i:', 386, ' x.shape:', (10, 120, 6))\n",
      "('i:', 387, ' x.shape:', (10, 120, 6))\n",
      "('i:', 388, ' x.shape:', (10, 120, 6))\n",
      "('i:', 389, ' x.shape:', (10, 120, 6))\n",
      "('i:', 390, ' x.shape:', (10, 120, 6))\n",
      "('i:', 391, ' x.shape:', (10, 120, 6))\n",
      "('i:', 392, ' x.shape:', (10, 120, 6))\n",
      "('i:', 393, ' x.shape:', (10, 120, 6))\n",
      "('i:', 394, ' x.shape:', (10, 120, 6))\n",
      "('i:', 395, ' x.shape:', (10, 120, 6))\n",
      "('i:', 396, ' x.shape:', (10, 120, 6))\n",
      "('i:', 397, ' x.shape:', (10, 120, 6))\n",
      "('i:', 398, ' x.shape:', (10, 120, 6))\n",
      "('i:', 399, ' x.shape:', (10, 120, 6))\n",
      "('i:', 400, ' x.shape:', (10, 120, 6))\n",
      "('i:', 401, ' x.shape:', (10, 120, 6))\n",
      "('i:', 402, ' x.shape:', (10, 120, 6))\n",
      "('i:', 403, ' x.shape:', (10, 120, 6))\n",
      "('i:', 404, ' x.shape:', (10, 120, 6))\n",
      "('i:', 405, ' x.shape:', (10, 120, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i:', 406, ' x.shape:', (10, 120, 6))\n",
      "('i:', 407, ' x.shape:', (10, 120, 6))\n",
      "('i:', 408, ' x.shape:', (10, 120, 6))\n",
      "('i:', 409, ' x.shape:', (10, 120, 6))\n",
      "('i:', 410, ' x.shape:', (10, 120, 6))\n",
      "('i:', 411, ' x.shape:', (10, 120, 6))\n",
      "('i:', 412, ' x.shape:', (10, 120, 6))\n",
      "('i:', 413, ' x.shape:', (10, 120, 6))\n",
      "('i:', 414, ' x.shape:', (10, 120, 6))\n",
      "('i:', 415, ' x.shape:', (10, 120, 6))\n",
      "('i:', 416, ' x.shape:', (10, 120, 6))\n",
      "('i:', 417, ' x.shape:', (10, 240, 6))\n",
      "('i:', 418, ' x.shape:', (10, 120, 6))\n",
      "('i:', 419, ' x.shape:', (10, 120, 6))\n",
      "('i:', 420, ' x.shape:', (10, 120, 6))\n",
      "('i:', 421, ' x.shape:', (10, 120, 6))\n",
      "('i:', 422, ' x.shape:', (10, 120, 6))\n",
      "('i:', 423, ' x.shape:', (10, 120, 6))\n",
      "('i:', 424, ' x.shape:', (10, 120, 6))\n",
      "('i:', 425, ' x.shape:', (10, 120, 6))\n",
      "('i:', 426, ' x.shape:', (10, 120, 6))\n",
      "('i:', 427, ' x.shape:', (10, 120, 6))\n",
      "('i:', 428, ' x.shape:', (10, 120, 6))\n",
      "('i:', 429, ' x.shape:', (10, 120, 6))\n",
      "('i:', 430, ' x.shape:', (10, 120, 6))\n",
      "('i:', 431, ' x.shape:', (10, 120, 6))\n",
      "('i:', 432, ' x.shape:', (10, 120, 6))\n",
      "('i:', 433, ' x.shape:', (10, 120, 6))\n",
      "('i:', 434, ' x.shape:', (10, 120, 6))\n",
      "('i:', 435, ' x.shape:', (10, 120, 6))\n",
      "('i:', 436, ' x.shape:', (10, 120, 6))\n",
      "('i:', 437, ' x.shape:', (10, 120, 6))\n",
      "('i:', 438, ' x.shape:', (10, 120, 6))\n",
      "('i:', 439, ' x.shape:', (10, 120, 6))\n",
      "('i:', 440, ' x.shape:', (10, 120, 6))\n",
      "('i:', 441, ' x.shape:', (10, 120, 6))\n",
      "('i:', 442, ' x.shape:', (10, 120, 6))\n",
      "('i:', 443, ' x.shape:', (10, 120, 6))\n",
      "('i:', 444, ' x.shape:', (10, 120, 6))\n",
      "('i:', 445, ' x.shape:', (10, 120, 6))\n",
      "('i:', 446, ' x.shape:', (10, 120, 6))\n",
      "('i:', 447, ' x.shape:', (10, 120, 6))\n",
      "('i:', 448, ' x.shape:', (10, 120, 6))\n",
      "('i:', 449, ' x.shape:', (10, 120, 6))\n",
      "('i:', 450, ' x.shape:', (10, 120, 6))\n",
      "('i:', 451, ' x.shape:', (10, 120, 6))\n",
      "('i:', 452, ' x.shape:', (10, 120, 6))\n",
      "('i:', 453, ' x.shape:', (10, 120, 6))\n",
      "('i:', 454, ' x.shape:', (10, 120, 6))\n",
      "('i:', 455, ' x.shape:', (10, 120, 6))\n",
      "('i:', 456, ' x.shape:', (10, 120, 6))\n",
      "('i:', 457, ' x.shape:', (10, 120, 6))\n",
      "('i:', 458, ' x.shape:', (10, 120, 6))\n",
      "('i:', 459, ' x.shape:', (10, 120, 6))\n",
      "('i:', 460, ' x.shape:', (10, 120, 6))\n",
      "('i:', 461, ' x.shape:', (10, 120, 6))\n",
      "('i:', 462, ' x.shape:', (10, 120, 6))\n",
      "('i:', 463, ' x.shape:', (10, 120, 6))\n",
      "('i:', 464, ' x.shape:', (10, 120, 6))\n",
      "('i:', 465, ' x.shape:', (10, 120, 6))\n",
      "('i:', 466, ' x.shape:', (10, 120, 6))\n",
      "('i:', 467, ' x.shape:', (10, 120, 6))\n",
      "('i:', 468, ' x.shape:', (10, 120, 6))\n",
      "('i:', 469, ' x.shape:', (10, 120, 6))\n",
      "('i:', 470, ' x.shape:', (10, 120, 6))\n",
      "('i:', 471, ' x.shape:', (10, 120, 6))\n",
      "('i:', 472, ' x.shape:', (10, 120, 6))\n",
      "('i:', 473, ' x.shape:', (10, 120, 6))\n",
      "('i:', 474, ' x.shape:', (10, 120, 6))\n",
      "('i:', 475, ' x.shape:', (10, 120, 6))\n",
      "('i:', 476, ' x.shape:', (10, 120, 6))\n",
      "('i:', 477, ' x.shape:', (10, 120, 6))\n",
      "('i:', 478, ' x.shape:', (10, 120, 6))\n",
      "('i:', 479, ' x.shape:', (10, 120, 6))\n",
      "('i:', 480, ' x.shape:', (10, 120, 6))\n",
      "('i:', 481, ' x.shape:', (10, 120, 6))\n",
      "('i:', 482, ' x.shape:', (10, 120, 6))\n",
      "('i:', 483, ' x.shape:', (10, 120, 6))\n",
      "('i:', 484, ' x.shape:', (10, 120, 6))\n",
      "('i:', 485, ' x.shape:', (10, 120, 6))\n",
      "('i:', 486, ' x.shape:', (10, 120, 6))\n",
      "('i:', 487, ' x.shape:', (10, 120, 6))\n",
      "('i:', 488, ' x.shape:', (10, 120, 6))\n",
      "('i:', 489, ' x.shape:', (10, 240, 6))\n",
      "('i:', 490, ' x.shape:', (10, 120, 6))\n",
      "('i:', 491, ' x.shape:', (10, 120, 6))\n",
      "('i:', 492, ' x.shape:', (10, 120, 6))\n",
      "('i:', 493, ' x.shape:', (10, 120, 6))\n",
      "('i:', 494, ' x.shape:', (10, 120, 6))\n",
      "('i:', 495, ' x.shape:', (10, 120, 6))\n",
      "('i:', 496, ' x.shape:', (10, 120, 6))\n",
      "('i:', 497, ' x.shape:', (10, 120, 6))\n",
      "('i:', 498, ' x.shape:', (10, 120, 6))\n",
      "('i:', 499, ' x.shape:', (10, 120, 6))\n",
      "('i:', 500, ' x.shape:', (10, 120, 6))\n",
      "('i:', 501, ' x.shape:', (10, 120, 6))\n",
      "('i:', 502, ' x.shape:', (10, 120, 6))\n",
      "('i:', 503, ' x.shape:', (10, 120, 6))\n",
      "('i:', 504, ' x.shape:', (10, 120, 6))\n",
      "('i:', 505, ' x.shape:', (10, 120, 6))\n",
      "('i:', 506, ' x.shape:', (10, 120, 6))\n",
      "('i:', 507, ' x.shape:', (10, 120, 6))\n",
      "('i:', 508, ' x.shape:', (10, 120, 6))\n",
      "('i:', 509, ' x.shape:', (10, 120, 6))\n",
      "('i:', 510, ' x.shape:', (10, 240, 6))\n",
      "('i:', 511, ' x.shape:', (10, 120, 6))\n",
      "('i:', 512, ' x.shape:', (10, 120, 6))\n",
      "('i:', 513, ' x.shape:', (10, 120, 6))\n",
      "('i:', 514, ' x.shape:', (10, 120, 6))\n",
      "('i:', 515, ' x.shape:', (10, 120, 6))\n",
      "('i:', 516, ' x.shape:', (10, 120, 6))\n",
      "('i:', 517, ' x.shape:', (10, 120, 6))\n",
      "('i:', 518, ' x.shape:', (10, 120, 6))\n",
      "('i:', 519, ' x.shape:', (10, 120, 6))\n",
      "('i:', 520, ' x.shape:', (10, 120, 6))\n",
      "('i:', 521, ' x.shape:', (10, 120, 6))\n",
      "('i:', 522, ' x.shape:', (10, 120, 6))\n",
      "('i:', 523, ' x.shape:', (10, 120, 6))\n",
      "('i:', 524, ' x.shape:', (10, 120, 6))\n",
      "('i:', 525, ' x.shape:', (10, 120, 6))\n",
      "('i:', 526, ' x.shape:', (10, 120, 6))\n",
      "('i:', 527, ' x.shape:', (10, 120, 6))\n",
      "('i:', 528, ' x.shape:', (10, 120, 6))\n",
      "('i:', 529, ' x.shape:', (10, 120, 6))\n",
      "('i:', 530, ' x.shape:', (10, 120, 6))\n",
      "('i:', 531, ' x.shape:', (10, 120, 6))\n",
      "('i:', 532, ' x.shape:', (10, 120, 6))\n",
      "('i:', 533, ' x.shape:', (10, 120, 6))\n",
      "('i:', 534, ' x.shape:', (10, 120, 6))\n",
      "('i:', 535, ' x.shape:', (10, 120, 6))\n",
      "('i:', 536, ' x.shape:', (10, 120, 6))\n",
      "('i:', 537, ' x.shape:', (10, 120, 6))\n",
      "('i:', 538, ' x.shape:', (10, 240, 6))\n",
      "('i:', 539, ' x.shape:', (10, 240, 6))\n",
      "('i:', 540, ' x.shape:', (10, 120, 6))\n",
      "('i:', 541, ' x.shape:', (10, 120, 6))\n",
      "('i:', 542, ' x.shape:', (10, 120, 6))\n",
      "('i:', 543, ' x.shape:', (10, 120, 6))\n",
      "('i:', 544, ' x.shape:', (10, 120, 6))\n",
      "('i:', 545, ' x.shape:', (10, 120, 6))\n",
      "('i:', 546, ' x.shape:', (10, 120, 6))\n",
      "('i:', 547, ' x.shape:', (10, 120, 6))\n",
      "('i:', 548, ' x.shape:', (10, 120, 6))\n",
      "('i:', 549, ' x.shape:', (10, 120, 6))\n",
      "('i:', 550, ' x.shape:', (10, 120, 6))\n",
      "('i:', 551, ' x.shape:', (10, 120, 6))\n",
      "('i:', 552, ' x.shape:', (10, 120, 6))\n",
      "('i:', 553, ' x.shape:', (10, 120, 6))\n",
      "('i:', 554, ' x.shape:', (10, 120, 6))\n",
      "('i:', 555, ' x.shape:', (10, 120, 6))\n",
      "('i:', 556, ' x.shape:', (10, 120, 6))\n",
      "('i:', 557, ' x.shape:', (10, 120, 6))\n",
      "('i:', 558, ' x.shape:', (10, 120, 6))\n",
      "('i:', 559, ' x.shape:', (10, 120, 6))\n",
      "('i:', 560, ' x.shape:', (10, 120, 6))\n",
      "('i:', 561, ' x.shape:', (10, 120, 6))\n",
      "('i:', 562, ' x.shape:', (10, 120, 6))\n",
      "('i:', 563, ' x.shape:', (10, 120, 6))\n",
      "('i:', 564, ' x.shape:', (10, 120, 6))\n",
      "('i:', 565, ' x.shape:', (10, 120, 6))\n",
      "('i:', 566, ' x.shape:', (10, 120, 6))\n",
      "('i:', 567, ' x.shape:', (10, 120, 6))\n",
      "('i:', 568, ' x.shape:', (10, 120, 6))\n",
      "('i:', 569, ' x.shape:', (10, 120, 6))\n",
      "('i:', 570, ' x.shape:', (10, 120, 6))\n",
      "('i:', 571, ' x.shape:', (10, 120, 6))\n",
      "('i:', 572, ' x.shape:', (10, 120, 6))\n",
      "('i:', 573, ' x.shape:', (10, 120, 6))\n",
      "('i:', 574, ' x.shape:', (10, 120, 6))\n",
      "('i:', 575, ' x.shape:', (10, 120, 6))\n",
      "('i:', 576, ' x.shape:', (10, 120, 6))\n",
      "('i:', 577, ' x.shape:', (10, 120, 6))\n",
      "('i:', 578, ' x.shape:', (10, 240, 6))\n",
      "('i:', 579, ' x.shape:', (10, 120, 6))\n",
      "('i:', 580, ' x.shape:', (10, 120, 6))\n",
      "('i:', 581, ' x.shape:', (10, 120, 6))\n",
      "('i:', 582, ' x.shape:', (10, 120, 6))\n",
      "('i:', 583, ' x.shape:', (10, 120, 6))\n",
      "('i:', 584, ' x.shape:', (10, 120, 6))\n",
      "('i:', 585, ' x.shape:', (10, 120, 6))\n",
      "('i:', 586, ' x.shape:', (10, 120, 6))\n",
      "('i:', 587, ' x.shape:', (10, 120, 6))\n",
      "('i:', 588, ' x.shape:', (10, 120, 6))\n",
      "('i:', 589, ' x.shape:', (10, 120, 6))\n",
      "('i:', 590, ' x.shape:', (10, 120, 6))\n",
      "('i:', 591, ' x.shape:', (10, 120, 6))\n",
      "('i:', 592, ' x.shape:', (10, 120, 6))\n",
      "('i:', 593, ' x.shape:', (10, 120, 6))\n",
      "('i:', 594, ' x.shape:', (10, 120, 6))\n",
      "('i:', 595, ' x.shape:', (10, 120, 6))\n",
      "('i:', 596, ' x.shape:', (10, 120, 6))\n",
      "('i:', 597, ' x.shape:', (10, 120, 6))\n",
      "('i:', 598, ' x.shape:', (10, 120, 6))\n",
      "('i:', 599, ' x.shape:', (10, 120, 6))\n",
      "('i:', 600, ' x.shape:', (10, 120, 6))\n",
      "('i:', 601, ' x.shape:', (10, 120, 6))\n",
      "('i:', 602, ' x.shape:', (10, 120, 6))\n",
      "('i:', 603, ' x.shape:', (10, 120, 6))\n",
      "('i:', 604, ' x.shape:', (10, 120, 6))\n",
      "('i:', 605, ' x.shape:', (10, 120, 6))\n",
      "('i:', 606, ' x.shape:', (10, 120, 6))\n",
      "('i:', 607, ' x.shape:', (10, 120, 6))\n",
      "('i:', 608, ' x.shape:', (10, 120, 6))\n",
      "('i:', 609, ' x.shape:', (10, 120, 6))\n",
      "('i:', 610, ' x.shape:', (10, 120, 6))\n",
      "('i:', 611, ' x.shape:', (10, 120, 6))\n",
      "('i:', 612, ' x.shape:', (10, 120, 6))\n",
      "('i:', 613, ' x.shape:', (10, 120, 6))\n",
      "('i:', 614, ' x.shape:', (10, 240, 6))\n",
      "('i:', 615, ' x.shape:', (10, 120, 6))\n",
      "('i:', 616, ' x.shape:', (10, 120, 6))\n",
      "('i:', 617, ' x.shape:', (10, 120, 6))\n",
      "('i:', 618, ' x.shape:', (10, 120, 6))\n",
      "('i:', 619, ' x.shape:', (10, 120, 6))\n",
      "('i:', 620, ' x.shape:', (10, 120, 6))\n",
      "('i:', 621, ' x.shape:', (10, 120, 6))\n",
      "('i:', 622, ' x.shape:', (10, 120, 6))\n",
      "('i:', 623, ' x.shape:', (10, 120, 6))\n",
      "('i:', 624, ' x.shape:', (10, 120, 6))\n",
      "('i:', 625, ' x.shape:', (10, 120, 6))\n",
      "('i:', 626, ' x.shape:', (10, 120, 6))\n",
      "('i:', 627, ' x.shape:', (10, 120, 6))\n",
      "('i:', 628, ' x.shape:', (10, 120, 6))\n",
      "('i:', 629, ' x.shape:', (10, 120, 6))\n",
      "('i:', 630, ' x.shape:', (10, 120, 6))\n",
      "('i:', 631, ' x.shape:', (10, 120, 6))\n",
      "('i:', 632, ' x.shape:', (10, 120, 6))\n",
      "('i:', 633, ' x.shape:', (10, 120, 6))\n",
      "('i:', 634, ' x.shape:', (10, 120, 6))\n",
      "('i:', 635, ' x.shape:', (10, 120, 6))\n",
      "('i:', 636, ' x.shape:', (10, 120, 6))\n",
      "('i:', 637, ' x.shape:', (10, 120, 6))\n",
      "('i:', 638, ' x.shape:', (10, 120, 6))\n",
      "('i:', 639, ' x.shape:', (10, 120, 6))\n",
      "('i:', 640, ' x.shape:', (10, 120, 6))\n",
      "('i:', 641, ' x.shape:', (10, 120, 6))\n",
      "('i:', 642, ' x.shape:', (10, 120, 6))\n",
      "('i:', 643, ' x.shape:', (10, 120, 6))\n",
      "('i:', 644, ' x.shape:', (10, 120, 6))\n",
      "('i:', 645, ' x.shape:', (10, 120, 6))\n",
      "('i:', 646, ' x.shape:', (10, 120, 6))\n",
      "('i:', 647, ' x.shape:', (10, 120, 6))\n",
      "('i:', 648, ' x.shape:', (10, 120, 6))\n",
      "('i:', 649, ' x.shape:', (10, 120, 6))\n",
      "('i:', 650, ' x.shape:', (10, 120, 6))\n",
      "('i:', 651, ' x.shape:', (10, 120, 6))\n",
      "('i:', 652, ' x.shape:', (10, 120, 6))\n",
      "('i:', 653, ' x.shape:', (10, 120, 6))\n",
      "('i:', 654, ' x.shape:', (10, 120, 6))\n",
      "('i:', 655, ' x.shape:', (10, 120, 6))\n",
      "('i:', 656, ' x.shape:', (10, 240, 6))\n",
      "('i:', 657, ' x.shape:', (10, 120, 6))\n",
      "('i:', 658, ' x.shape:', (10, 120, 6))\n",
      "('i:', 659, ' x.shape:', (10, 120, 6))\n",
      "('i:', 660, ' x.shape:', (10, 120, 6))\n",
      "('i:', 661, ' x.shape:', (10, 120, 6))\n",
      "('i:', 662, ' x.shape:', (10, 240, 6))\n",
      "('i:', 663, ' x.shape:', (10, 120, 6))\n",
      "('i:', 664, ' x.shape:', (10, 120, 6))\n",
      "('i:', 665, ' x.shape:', (10, 120, 6))\n",
      "('i:', 666, ' x.shape:', (10, 120, 6))\n",
      "('i:', 667, ' x.shape:', (10, 120, 6))\n",
      "('i:', 668, ' x.shape:', (10, 240, 6))\n",
      "('i:', 669, ' x.shape:', (10, 120, 6))\n",
      "('i:', 670, ' x.shape:', (10, 120, 6))\n",
      "('i:', 671, ' x.shape:', (10, 120, 6))\n",
      "('i:', 672, ' x.shape:', (10, 120, 6))\n",
      "('i:', 673, ' x.shape:', (10, 120, 6))\n",
      "('i:', 674, ' x.shape:', (10, 120, 6))\n",
      "('i:', 675, ' x.shape:', (10, 120, 6))\n",
      "('i:', 676, ' x.shape:', (10, 120, 6))\n",
      "('i:', 677, ' x.shape:', (10, 120, 6))\n",
      "('i:', 678, ' x.shape:', (10, 120, 6))\n",
      "('i:', 679, ' x.shape:', (10, 120, 6))\n",
      "('i:', 680, ' x.shape:', (10, 120, 6))\n",
      "('i:', 681, ' x.shape:', (10, 120, 6))\n",
      "('i:', 682, ' x.shape:', (10, 120, 6))\n",
      "('i:', 683, ' x.shape:', (10, 120, 6))\n",
      "('i:', 684, ' x.shape:', (10, 120, 6))\n",
      "('i:', 685, ' x.shape:', (10, 120, 6))\n",
      "('i:', 686, ' x.shape:', (10, 120, 6))\n",
      "('i:', 687, ' x.shape:', (10, 120, 6))\n",
      "('i:', 688, ' x.shape:', (10, 120, 6))\n",
      "('i:', 689, ' x.shape:', (10, 120, 6))\n",
      "('i:', 690, ' x.shape:', (10, 120, 6))\n",
      "('i:', 691, ' x.shape:', (10, 120, 6))\n",
      "('i:', 692, ' x.shape:', (10, 120, 6))\n",
      "('i:', 693, ' x.shape:', (10, 120, 6))\n",
      "('i:', 694, ' x.shape:', (10, 120, 6))\n",
      "('i:', 695, ' x.shape:', (10, 120, 6))\n",
      "('i:', 696, ' x.shape:', (10, 120, 6))\n",
      "('i:', 697, ' x.shape:', (10, 120, 6))\n",
      "('i:', 698, ' x.shape:', (10, 120, 6))\n",
      "('i:', 699, ' x.shape:', (10, 120, 6))\n",
      "('i:', 700, ' x.shape:', (10, 120, 6))\n",
      "('i:', 701, ' x.shape:', (10, 120, 6))\n",
      "('i:', 702, ' x.shape:', (10, 120, 6))\n",
      "('i:', 703, ' x.shape:', (10, 120, 6))\n",
      "('i:', 704, ' x.shape:', (10, 120, 6))\n",
      "('i:', 705, ' x.shape:', (10, 120, 6))\n",
      "('i:', 706, ' x.shape:', (10, 120, 6))\n",
      "('i:', 707, ' x.shape:', (10, 120, 6))\n",
      "('i:', 708, ' x.shape:', (10, 120, 6))\n",
      "('i:', 709, ' x.shape:', (10, 120, 6))\n",
      "('i:', 710, ' x.shape:', (10, 120, 6))\n",
      "('i:', 711, ' x.shape:', (10, 120, 6))\n",
      "('i:', 712, ' x.shape:', (10, 120, 6))\n",
      "('i:', 713, ' x.shape:', (10, 120, 6))\n",
      "('i:', 714, ' x.shape:', (10, 120, 6))\n",
      "('i:', 715, ' x.shape:', (10, 120, 6))\n",
      "('i:', 716, ' x.shape:', (10, 120, 6))\n",
      "('i:', 717, ' x.shape:', (10, 120, 6))\n",
      "('i:', 718, ' x.shape:', (10, 120, 6))\n",
      "('i:', 719, ' x.shape:', (10, 120, 6))\n",
      "('i:', 720, ' x.shape:', (10, 120, 6))\n",
      "('i:', 721, ' x.shape:', (10, 120, 6))\n",
      "('i:', 722, ' x.shape:', (10, 120, 6))\n",
      "('i:', 723, ' x.shape:', (10, 120, 6))\n",
      "('i:', 724, ' x.shape:', (10, 120, 6))\n",
      "('i:', 725, ' x.shape:', (10, 120, 6))\n",
      "('i:', 726, ' x.shape:', (10, 120, 6))\n",
      "('i:', 727, ' x.shape:', (10, 120, 6))\n",
      "('i:', 728, ' x.shape:', (10, 120, 6))\n",
      "('i:', 729, ' x.shape:', (10, 120, 6))\n",
      "('i:', 730, ' x.shape:', (10, 120, 6))\n",
      "('i:', 731, ' x.shape:', (10, 120, 6))\n",
      "('i:', 732, ' x.shape:', (10, 120, 6))\n",
      "('i:', 733, ' x.shape:', (10, 120, 6))\n",
      "('i:', 734, ' x.shape:', (10, 120, 6))\n",
      "('i:', 735, ' x.shape:', (10, 120, 6))\n",
      "('i:', 736, ' x.shape:', (10, 120, 6))\n",
      "('i:', 737, ' x.shape:', (10, 120, 6))\n",
      "('i:', 738, ' x.shape:', (10, 120, 6))\n",
      "('i:', 739, ' x.shape:', (10, 120, 6))\n",
      "('i:', 740, ' x.shape:', (10, 120, 6))\n",
      "('i:', 741, ' x.shape:', (10, 120, 6))\n",
      "('i:', 742, ' x.shape:', (10, 120, 6))\n",
      "('i:', 743, ' x.shape:', (10, 120, 6))\n",
      "('i:', 744, ' x.shape:', (10, 120, 6))\n",
      "('i:', 745, ' x.shape:', (10, 120, 6))\n",
      "('i:', 746, ' x.shape:', (10, 120, 6))\n",
      "('i:', 747, ' x.shape:', (10, 120, 6))\n",
      "('i:', 748, ' x.shape:', (10, 120, 6))\n",
      "('i:', 749, ' x.shape:', (10, 120, 6))\n",
      "('i:', 750, ' x.shape:', (10, 120, 6))\n",
      "('i:', 751, ' x.shape:', (10, 120, 6))\n",
      "('i:', 752, ' x.shape:', (10, 240, 6))\n",
      "('i:', 753, ' x.shape:', (10, 120, 6))\n",
      "('i:', 754, ' x.shape:', (10, 120, 6))\n",
      "('i:', 755, ' x.shape:', (10, 120, 6))\n",
      "('i:', 756, ' x.shape:', (10, 120, 6))\n",
      "('i:', 757, ' x.shape:', (10, 120, 6))\n",
      "('i:', 758, ' x.shape:', (10, 120, 6))\n",
      "('i:', 759, ' x.shape:', (10, 120, 6))\n",
      "('i:', 760, ' x.shape:', (10, 120, 6))\n",
      "('i:', 761, ' x.shape:', (10, 120, 6))\n",
      "('i:', 762, ' x.shape:', (10, 120, 6))\n",
      "('i:', 763, ' x.shape:', (10, 120, 6))\n",
      "('i:', 764, ' x.shape:', (10, 120, 6))\n",
      "('i:', 765, ' x.shape:', (10, 120, 6))\n",
      "('i:', 766, ' x.shape:', (10, 120, 6))\n",
      "('i:', 767, ' x.shape:', (10, 120, 6))\n",
      "('i:', 768, ' x.shape:', (10, 120, 6))\n",
      "('i:', 769, ' x.shape:', (10, 120, 6))\n",
      "('i:', 770, ' x.shape:', (10, 120, 6))\n",
      "('i:', 771, ' x.shape:', (10, 120, 6))\n",
      "('i:', 772, ' x.shape:', (10, 120, 6))\n",
      "('i:', 773, ' x.shape:', (10, 120, 6))\n",
      "('i:', 774, ' x.shape:', (10, 120, 6))\n",
      "('i:', 775, ' x.shape:', (10, 120, 6))\n",
      "('i:', 776, ' x.shape:', (10, 120, 6))\n",
      "('i:', 777, ' x.shape:', (10, 120, 6))\n",
      "('i:', 778, ' x.shape:', (10, 240, 6))\n",
      "('i:', 779, ' x.shape:', (10, 120, 6))\n",
      "('i:', 780, ' x.shape:', (10, 120, 6))\n",
      "('i:', 781, ' x.shape:', (10, 120, 6))\n",
      "('i:', 782, ' x.shape:', (10, 120, 6))\n",
      "('i:', 783, ' x.shape:', (10, 120, 6))\n",
      "('i:', 784, ' x.shape:', (10, 120, 6))\n",
      "('i:', 785, ' x.shape:', (10, 120, 6))\n",
      "('i:', 786, ' x.shape:', (10, 120, 6))\n",
      "('i:', 787, ' x.shape:', (10, 120, 6))\n",
      "('i:', 788, ' x.shape:', (10, 120, 6))\n",
      "('i:', 789, ' x.shape:', (10, 120, 6))\n",
      "('i:', 790, ' x.shape:', (10, 120, 6))\n",
      "('i:', 791, ' x.shape:', (10, 120, 6))\n",
      "('i:', 792, ' x.shape:', (10, 120, 6))\n",
      "('i:', 793, ' x.shape:', (10, 120, 6))\n",
      "('i:', 794, ' x.shape:', (10, 120, 6))\n",
      "('i:', 795, ' x.shape:', (10, 120, 6))\n",
      "('i:', 796, ' x.shape:', (10, 120, 6))\n",
      "('i:', 797, ' x.shape:', (10, 120, 6))\n",
      "('i:', 798, ' x.shape:', (10, 120, 6))\n",
      "('i:', 799, ' x.shape:', (10, 120, 6))\n",
      "('i:', 800, ' x.shape:', (10, 120, 6))\n",
      "('i:', 801, ' x.shape:', (10, 120, 6))\n",
      "('i:', 802, ' x.shape:', (10, 120, 6))\n",
      "('i:', 803, ' x.shape:', (10, 120, 6))\n",
      "('i:', 804, ' x.shape:', (10, 120, 6))\n",
      "('i:', 805, ' x.shape:', (10, 120, 6))\n",
      "('i:', 806, ' x.shape:', (10, 120, 6))\n",
      "('i:', 807, ' x.shape:', (10, 120, 6))\n",
      "('i:', 808, ' x.shape:', (10, 120, 6))\n",
      "('i:', 809, ' x.shape:', (10, 120, 6))\n",
      "('i:', 810, ' x.shape:', (10, 120, 6))\n",
      "('i:', 811, ' x.shape:', (10, 120, 6))\n",
      "('i:', 812, ' x.shape:', (10, 120, 6))\n",
      "('i:', 813, ' x.shape:', (10, 120, 6))\n",
      "('i:', 814, ' x.shape:', (10, 120, 6))\n",
      "('i:', 815, ' x.shape:', (10, 120, 6))\n",
      "('i:', 816, ' x.shape:', (10, 120, 6))\n",
      "('i:', 817, ' x.shape:', (10, 120, 6))\n",
      "('i:', 818, ' x.shape:', (10, 120, 6))\n",
      "('i:', 819, ' x.shape:', (10, 120, 6))\n",
      "('i:', 820, ' x.shape:', (10, 120, 6))\n",
      "('i:', 821, ' x.shape:', (10, 120, 6))\n",
      "('i:', 822, ' x.shape:', (10, 120, 6))\n",
      "('i:', 823, ' x.shape:', (10, 120, 6))\n",
      "('i:', 824, ' x.shape:', (10, 120, 6))\n",
      "('i:', 825, ' x.shape:', (10, 120, 6))\n",
      "('i:', 826, ' x.shape:', (10, 120, 6))\n",
      "('i:', 827, ' x.shape:', (10, 120, 6))\n",
      "('i:', 828, ' x.shape:', (10, 120, 6))\n",
      "('i:', 829, ' x.shape:', (10, 120, 6))\n",
      "('i:', 830, ' x.shape:', (10, 120, 6))\n",
      "('i:', 831, ' x.shape:', (10, 120, 6))\n",
      "('i:', 832, ' x.shape:', (10, 120, 6))\n",
      "('i:', 833, ' x.shape:', (10, 120, 6))\n",
      "('i:', 834, ' x.shape:', (10, 120, 6))\n",
      "('i:', 835, ' x.shape:', (10, 120, 6))\n",
      "('i:', 836, ' x.shape:', (10, 120, 6))\n",
      "('i:', 837, ' x.shape:', (10, 120, 6))\n",
      "('i:', 838, ' x.shape:', (10, 120, 6))\n",
      "('i:', 839, ' x.shape:', (10, 120, 6))\n",
      "('i:', 840, ' x.shape:', (10, 120, 6))\n",
      "('i:', 841, ' x.shape:', (10, 120, 6))\n",
      "('i:', 842, ' x.shape:', (10, 120, 6))\n",
      "('i:', 843, ' x.shape:', (10, 120, 6))\n",
      "('i:', 844, ' x.shape:', (10, 120, 6))\n",
      "('i:', 845, ' x.shape:', (10, 120, 6))\n",
      "('i:', 846, ' x.shape:', (10, 120, 6))\n",
      "('i:', 847, ' x.shape:', (10, 120, 6))\n",
      "('i:', 848, ' x.shape:', (10, 120, 6))\n",
      "('i:', 849, ' x.shape:', (10, 120, 6))\n",
      "('i:', 850, ' x.shape:', (10, 120, 6))\n",
      "('i:', 851, ' x.shape:', (10, 120, 6))\n",
      "('i:', 852, ' x.shape:', (10, 120, 6))\n",
      "('i:', 853, ' x.shape:', (10, 120, 6))\n",
      "('i:', 854, ' x.shape:', (10, 120, 6))\n",
      "('i:', 855, ' x.shape:', (10, 120, 6))\n",
      "('i:', 856, ' x.shape:', (10, 120, 6))\n",
      "('i:', 857, ' x.shape:', (10, 120, 6))\n",
      "('i:', 858, ' x.shape:', (10, 120, 6))\n",
      "('i:', 859, ' x.shape:', (10, 120, 6))\n",
      "('i:', 860, ' x.shape:', (10, 120, 6))\n",
      "('i:', 861, ' x.shape:', (10, 120, 6))\n",
      "('i:', 862, ' x.shape:', (10, 120, 6))\n",
      "('i:', 863, ' x.shape:', (10, 120, 6))\n",
      "('i:', 864, ' x.shape:', (10, 120, 6))\n",
      "('i:', 865, ' x.shape:', (10, 120, 6))\n",
      "('i:', 866, ' x.shape:', (10, 120, 6))\n",
      "('i:', 867, ' x.shape:', (10, 120, 6))\n",
      "('i:', 868, ' x.shape:', (10, 120, 6))\n",
      "('i:', 869, ' x.shape:', (10, 120, 6))\n",
      "('i:', 870, ' x.shape:', (10, 120, 6))\n",
      "('i:', 871, ' x.shape:', (10, 120, 6))\n",
      "('i:', 872, ' x.shape:', (10, 120, 6))\n",
      "('i:', 873, ' x.shape:', (10, 120, 6))\n",
      "('i:', 874, ' x.shape:', (10, 120, 6))\n",
      "('i:', 875, ' x.shape:', (10, 120, 6))\n",
      "('i:', 876, ' x.shape:', (10, 120, 6))\n",
      "('i:', 877, ' x.shape:', (10, 120, 6))\n",
      "('i:', 878, ' x.shape:', (10, 120, 6))\n",
      "('i:', 879, ' x.shape:', (10, 120, 6))\n",
      "('i:', 880, ' x.shape:', (10, 120, 6))\n",
      "('i:', 881, ' x.shape:', (10, 120, 6))\n",
      "('i:', 882, ' x.shape:', (10, 120, 6))\n",
      "('i:', 883, ' x.shape:', (10, 120, 6))\n",
      "('i:', 884, ' x.shape:', (10, 120, 6))\n",
      "('i:', 885, ' x.shape:', (10, 120, 6))\n",
      "('i:', 886, ' x.shape:', (10, 120, 6))\n",
      "('i:', 887, ' x.shape:', (10, 120, 6))\n",
      "('i:', 888, ' x.shape:', (10, 120, 6))\n",
      "('i:', 889, ' x.shape:', (10, 120, 6))\n",
      "('i:', 890, ' x.shape:', (10, 120, 6))\n",
      "('i:', 891, ' x.shape:', (10, 120, 6))\n",
      "('i:', 892, ' x.shape:', (10, 120, 6))\n",
      "('i:', 893, ' x.shape:', (10, 120, 6))\n",
      "('i:', 894, ' x.shape:', (10, 240, 6))\n",
      "('i:', 895, ' x.shape:', (10, 120, 6))\n",
      "('i:', 896, ' x.shape:', (10, 120, 6))\n",
      "('i:', 897, ' x.shape:', (10, 120, 6))\n",
      "('i:', 898, ' x.shape:', (10, 120, 6))\n",
      "('i:', 899, ' x.shape:', (10, 120, 6))\n",
      "('i:', 900, ' x.shape:', (10, 120, 6))\n",
      "('i:', 901, ' x.shape:', (10, 120, 6))\n",
      "('i:', 902, ' x.shape:', (10, 120, 6))\n",
      "('i:', 903, ' x.shape:', (10, 120, 6))\n",
      "('i:', 904, ' x.shape:', (10, 120, 6))\n",
      "('i:', 905, ' x.shape:', (10, 120, 6))\n",
      "('i:', 906, ' x.shape:', (10, 120, 6))\n",
      "('i:', 907, ' x.shape:', (10, 120, 6))\n",
      "('i:', 908, ' x.shape:', (10, 120, 6))\n",
      "('i:', 909, ' x.shape:', (10, 120, 6))\n",
      "('i:', 910, ' x.shape:', (10, 120, 6))\n",
      "('i:', 911, ' x.shape:', (10, 120, 6))\n",
      "('i:', 912, ' x.shape:', (10, 120, 6))\n",
      "('i:', 913, ' x.shape:', (10, 120, 6))\n",
      "('i:', 914, ' x.shape:', (10, 120, 6))\n",
      "('i:', 915, ' x.shape:', (10, 120, 6))\n",
      "('i:', 916, ' x.shape:', (10, 120, 6))\n",
      "('i:', 917, ' x.shape:', (10, 120, 6))\n",
      "('i:', 918, ' x.shape:', (10, 120, 6))\n",
      "('i:', 919, ' x.shape:', (10, 120, 6))\n",
      "('i:', 920, ' x.shape:', (10, 120, 6))\n",
      "('i:', 921, ' x.shape:', (10, 120, 6))\n",
      "('i:', 922, ' x.shape:', (10, 120, 6))\n",
      "('i:', 923, ' x.shape:', (10, 120, 6))\n",
      "('i:', 924, ' x.shape:', (10, 120, 6))\n",
      "('i:', 925, ' x.shape:', (10, 240, 6))\n",
      "('i:', 926, ' x.shape:', (10, 120, 6))\n",
      "('i:', 927, ' x.shape:', (10, 120, 6))\n",
      "('i:', 928, ' x.shape:', (10, 120, 6))\n",
      "('i:', 929, ' x.shape:', (10, 120, 6))\n",
      "('i:', 930, ' x.shape:', (10, 120, 6))\n",
      "('i:', 931, ' x.shape:', (10, 120, 6))\n",
      "('i:', 932, ' x.shape:', (10, 120, 6))\n",
      "('i:', 933, ' x.shape:', (10, 120, 6))\n",
      "('i:', 934, ' x.shape:', (10, 120, 6))\n",
      "('i:', 935, ' x.shape:', (10, 120, 6))\n",
      "('i:', 936, ' x.shape:', (10, 120, 6))\n",
      "('i:', 937, ' x.shape:', (10, 120, 6))\n",
      "('i:', 938, ' x.shape:', (10, 120, 6))\n",
      "('i:', 939, ' x.shape:', (10, 120, 6))\n",
      "('i:', 940, ' x.shape:', (10, 120, 6))\n",
      "('i:', 941, ' x.shape:', (10, 120, 6))\n",
      "('i:', 942, ' x.shape:', (10, 120, 6))\n",
      "('i:', 943, ' x.shape:', (10, 120, 6))\n",
      "('i:', 944, ' x.shape:', (10, 120, 6))\n",
      "('i:', 945, ' x.shape:', (10, 120, 6))\n",
      "('i:', 946, ' x.shape:', (10, 120, 6))\n",
      "('i:', 947, ' x.shape:', (10, 120, 6))\n",
      "('i:', 948, ' x.shape:', (10, 120, 6))\n",
      "('i:', 949, ' x.shape:', (10, 120, 6))\n",
      "('i:', 950, ' x.shape:', (10, 120, 6))\n",
      "('i:', 951, ' x.shape:', (10, 120, 6))\n",
      "('i:', 952, ' x.shape:', (10, 120, 6))\n",
      "('i:', 953, ' x.shape:', (10, 120, 6))\n",
      "('i:', 954, ' x.shape:', (10, 120, 6))\n",
      "('i:', 955, ' x.shape:', (10, 120, 6))\n",
      "('i:', 956, ' x.shape:', (10, 240, 6))\n",
      "('i:', 957, ' x.shape:', (10, 120, 6))\n",
      "('i:', 958, ' x.shape:', (10, 120, 6))\n",
      "('i:', 959, ' x.shape:', (10, 120, 6))\n",
      "('i:', 960, ' x.shape:', (10, 120, 6))\n",
      "('i:', 961, ' x.shape:', (10, 120, 6))\n",
      "('i:', 962, ' x.shape:', (10, 120, 6))\n",
      "('i:', 963, ' x.shape:', (10, 120, 6))\n",
      "('i:', 964, ' x.shape:', (10, 120, 6))\n",
      "('i:', 965, ' x.shape:', (10, 120, 6))\n",
      "('i:', 966, ' x.shape:', (10, 120, 6))\n",
      "('i:', 967, ' x.shape:', (10, 120, 6))\n",
      "('i:', 968, ' x.shape:', (10, 120, 6))\n",
      "('i:', 969, ' x.shape:', (10, 120, 6))\n",
      "('i:', 970, ' x.shape:', (10, 120, 6))\n",
      "('i:', 971, ' x.shape:', (10, 120, 6))\n",
      "('i:', 972, ' x.shape:', (10, 120, 6))\n",
      "('i:', 973, ' x.shape:', (10, 240, 6))\n",
      "('i:', 974, ' x.shape:', (10, 120, 6))\n",
      "('i:', 975, ' x.shape:', (10, 120, 6))\n",
      "('i:', 976, ' x.shape:', (10, 120, 6))\n",
      "('i:', 977, ' x.shape:', (10, 120, 6))\n",
      "('i:', 978, ' x.shape:', (10, 120, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i:', 979, ' x.shape:', (10, 120, 6))\n",
      "('i:', 980, ' x.shape:', (10, 120, 6))\n",
      "('i:', 981, ' x.shape:', (10, 120, 6))\n",
      "('i:', 982, ' x.shape:', (10, 120, 6))\n",
      "('i:', 983, ' x.shape:', (10, 120, 6))\n",
      "('i:', 984, ' x.shape:', (10, 120, 6))\n",
      "('i:', 985, ' x.shape:', (10, 120, 6))\n",
      "('i:', 986, ' x.shape:', (10, 120, 6))\n",
      "('i:', 987, ' x.shape:', (10, 120, 6))\n",
      "('i:', 988, ' x.shape:', (10, 120, 6))\n",
      "('i:', 989, ' x.shape:', (10, 120, 6))\n",
      "('i:', 990, ' x.shape:', (10, 120, 6))\n",
      "('i:', 991, ' x.shape:', (10, 120, 6))\n",
      "('i:', 992, ' x.shape:', (10, 120, 6))\n",
      "('i:', 993, ' x.shape:', (10, 120, 6))\n",
      "('i:', 994, ' x.shape:', (10, 120, 6))\n",
      "('i:', 995, ' x.shape:', (10, 120, 6))\n",
      "('i:', 996, ' x.shape:', (10, 120, 6))\n",
      "('i:', 997, ' x.shape:', (10, 120, 6))\n",
      "('i:', 998, ' x.shape:', (10, 120, 6))\n",
      "('i:', 999, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1000, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1001, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1002, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1003, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1004, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1005, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1006, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1007, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1008, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1009, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1010, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1011, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1012, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1013, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1014, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1015, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1016, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1017, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1018, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1019, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1020, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1021, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1022, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1023, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1024, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1025, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1026, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1027, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1028, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1029, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1030, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1031, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1032, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1033, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1034, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1035, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1036, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1037, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1038, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1039, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1040, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1041, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1042, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1043, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1044, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1045, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1046, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1047, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1048, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1049, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1050, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1051, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1052, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1053, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1054, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1055, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1056, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1057, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1058, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1059, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1060, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1061, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1062, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1063, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1064, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1065, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1066, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1067, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1068, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1069, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1070, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1071, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1072, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1073, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1074, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1075, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1076, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1077, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1078, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1079, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1080, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1081, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1082, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1083, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1084, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1085, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1086, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1087, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1088, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1089, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1090, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1091, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1092, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1093, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1094, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1095, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1096, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1097, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1098, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1099, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1100, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1101, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1102, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1103, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1104, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1105, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1106, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1107, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1108, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1109, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1110, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1111, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1112, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1113, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1114, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1115, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1116, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1117, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1118, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1119, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1120, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1121, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1122, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1123, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1124, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1125, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1126, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1127, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1128, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1129, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1130, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1131, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1132, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1133, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1134, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1135, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1136, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1137, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1138, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1139, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1140, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1141, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1142, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1143, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1144, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1145, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1146, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1147, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1148, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1149, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1150, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1151, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1152, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1153, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1154, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1155, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1156, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1157, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1158, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1159, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1160, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1161, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1162, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1163, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1164, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1165, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1166, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1167, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1168, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1169, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1170, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1171, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1172, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1173, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1174, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1175, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1176, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1177, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1178, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1179, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1180, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1181, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1182, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1183, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1184, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1185, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1186, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1187, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1188, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1189, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1190, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1191, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1192, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1193, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1194, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1195, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1196, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1197, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1198, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1199, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1200, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1201, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1202, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1203, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1204, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1205, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1206, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1207, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1208, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1209, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1210, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1211, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1212, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1213, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1214, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1215, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1216, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1217, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1218, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1219, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1220, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1221, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1222, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1223, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1224, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1225, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1226, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1227, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1228, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1229, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1230, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1231, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1232, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1233, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1234, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1235, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1236, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1237, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1238, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1239, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1240, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1241, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1242, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1243, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1244, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1245, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1246, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1247, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1248, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1249, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1250, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1251, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1252, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1253, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1254, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1255, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1256, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1257, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1258, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1259, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1260, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1261, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1262, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1263, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1264, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1265, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1266, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1267, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1268, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1269, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1270, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1271, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1272, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1273, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1274, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1275, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1276, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1277, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1278, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1279, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1280, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1281, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1282, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1283, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1284, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1285, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1286, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1287, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1288, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1289, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1290, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1291, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1292, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1293, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1294, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1295, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1296, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1297, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1298, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1299, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1300, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1301, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1302, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1303, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1304, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1305, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1306, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1307, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1308, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1309, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1310, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1311, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1312, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1313, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1314, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1315, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1316, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1317, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1318, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1319, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1320, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1321, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1322, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1323, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1324, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1325, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1326, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1327, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1328, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1329, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1330, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1331, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1332, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1333, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1334, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1335, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1336, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1337, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1338, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1339, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1340, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1341, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1342, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1343, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1344, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1345, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1346, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1347, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1348, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1349, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1350, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1351, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1352, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1353, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1354, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1355, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1356, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1357, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1358, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1359, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1360, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1361, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1362, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1363, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1364, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1365, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1366, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1367, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1368, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1369, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1370, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1371, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1372, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1373, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1374, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1375, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1376, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1377, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1378, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1379, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1380, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1381, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1382, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1383, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1384, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1385, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1386, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1387, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1388, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1389, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1390, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1391, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1392, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1393, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1394, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1395, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1396, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1397, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1398, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1399, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1400, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1401, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1402, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1403, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1404, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1405, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1406, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1407, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1408, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1409, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1410, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1411, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1412, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1413, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1414, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1415, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1416, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1417, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1418, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1419, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1420, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1421, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1422, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1423, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1424, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1425, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1426, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1427, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1428, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1429, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1430, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1431, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1432, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1433, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1434, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1435, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1436, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1437, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1438, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1439, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1440, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1441, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1442, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1443, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1444, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1445, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1446, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1447, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1448, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1449, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1450, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1451, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1452, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1453, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1454, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1455, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1456, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1457, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1458, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1459, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1460, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1461, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1462, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1463, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1464, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1465, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1466, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1467, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1468, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1469, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1470, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1471, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1472, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1473, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1474, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1475, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1476, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1477, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1478, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1479, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1480, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1481, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1482, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1483, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1484, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1485, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1486, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1487, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1488, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1489, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1490, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1491, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1492, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1493, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1494, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1495, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1496, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1497, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1498, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1499, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1500, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1501, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1502, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1503, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1504, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1505, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1506, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1507, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1508, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1509, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1510, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1511, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1512, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1513, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1514, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1515, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1516, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1517, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1518, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1519, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1520, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1521, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1522, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1523, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1524, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1525, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1526, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1527, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1528, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1529, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1530, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1531, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1532, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1533, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1534, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1535, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1536, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1537, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1538, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1539, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1540, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1541, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1542, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1543, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1544, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1545, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1546, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1547, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1548, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1549, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1550, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1551, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1552, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1553, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1554, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1555, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1556, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1557, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1558, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1559, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1560, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1561, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1562, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1563, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1564, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1565, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1566, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1567, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1568, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1569, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1570, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1571, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1572, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1573, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1574, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1575, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1576, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1577, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1578, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1579, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1580, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1581, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1582, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1583, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1584, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1585, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1586, ' x.shape:', (10, 120, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i:', 1587, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1588, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1589, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1590, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1591, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1592, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1593, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1594, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1595, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1596, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1597, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1598, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1599, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1600, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1601, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1602, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1603, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1604, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1605, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1606, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1607, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1608, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1609, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1610, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1611, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1612, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1613, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1614, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1615, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1616, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1617, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1618, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1619, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1620, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1621, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1622, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1623, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1624, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1625, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1626, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1627, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1628, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1629, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1630, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1631, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1632, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1633, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1634, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1635, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1636, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1637, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1638, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1639, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1640, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1641, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1642, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1643, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1644, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1645, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1646, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1647, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1648, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1649, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1650, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1651, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1652, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1653, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1654, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1655, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1656, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1657, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1658, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1659, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1660, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1661, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1662, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1663, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1664, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1665, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1666, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1667, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1668, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1669, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1670, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1671, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1672, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1673, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1674, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1675, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1676, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1677, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1678, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1679, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1680, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1681, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1682, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1683, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1684, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1685, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1686, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1687, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1688, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1689, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1690, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1691, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1692, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1693, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1694, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1695, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1696, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1697, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1698, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1699, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1700, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1701, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1702, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1703, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1704, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1705, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1706, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1707, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1708, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1709, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1710, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1711, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1712, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1713, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1714, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1715, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1716, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1717, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1718, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1719, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1720, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1721, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1722, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1723, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1724, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1725, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1726, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1727, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1728, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1729, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1730, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1731, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1732, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1733, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1734, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1735, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1736, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1737, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1738, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1739, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1740, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1741, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1742, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1743, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1744, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1745, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1746, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1747, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1748, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1749, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1750, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1751, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1752, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1753, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1754, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1755, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1756, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1757, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1758, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1759, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1760, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1761, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1762, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1763, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1764, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1765, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1766, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1767, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1768, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1769, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1770, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1771, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1772, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1773, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1774, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1775, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1776, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1777, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1778, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1779, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1780, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1781, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1782, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1783, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1784, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1785, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1786, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1787, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1788, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1789, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1790, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1791, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1792, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1793, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1794, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1795, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1796, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1797, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1798, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1799, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1800, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1801, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1802, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1803, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1804, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1805, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1806, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1807, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1808, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1809, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1810, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1811, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1812, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1813, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1814, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1815, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1816, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1817, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1818, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1819, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1820, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1821, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1822, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1823, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1824, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1825, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1826, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1827, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1828, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1829, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1830, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1831, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1832, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1833, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1834, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1835, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1836, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1837, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1838, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1839, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1840, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1841, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1842, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1843, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1844, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1845, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1846, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1847, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1848, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1849, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1850, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1851, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1852, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1853, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1854, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1855, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1856, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1857, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1858, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1859, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1860, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1861, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1862, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1863, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1864, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1865, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1866, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1867, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1868, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1869, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1870, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1871, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1872, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1873, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1874, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1875, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1876, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1877, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1878, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1879, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1880, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1881, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1882, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1883, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1884, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1885, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1886, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1887, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1888, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1889, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1890, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1891, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1892, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1893, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1894, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1895, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1896, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1897, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1898, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1899, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1900, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1901, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1902, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1903, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1904, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1905, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1906, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1907, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1908, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1909, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1910, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1911, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1912, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1913, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1914, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1915, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1916, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1917, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1918, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1919, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1920, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1921, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1922, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1923, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1924, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1925, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1926, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1927, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1928, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1929, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1930, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1931, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1932, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1933, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1934, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1935, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1936, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1937, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1938, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1939, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1940, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1941, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1942, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1943, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1944, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1945, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1946, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1947, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1948, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1949, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1950, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1951, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1952, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1953, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1954, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1955, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1956, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1957, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1958, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1959, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1960, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1961, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1962, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1963, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1964, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1965, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1966, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1967, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1968, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1969, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1970, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1971, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1972, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1973, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1974, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1975, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1976, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1977, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1978, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1979, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1980, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1981, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1982, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1983, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1984, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1985, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1986, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1987, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1988, ' x.shape:', (10, 240, 6))\n",
      "('i:', 1989, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1990, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1991, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1992, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1993, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1994, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1995, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1996, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1997, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1998, ' x.shape:', (10, 120, 6))\n",
      "('i:', 1999, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2000, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2001, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2002, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2003, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2004, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2005, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2006, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2007, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2008, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2009, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2010, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2011, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2012, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2013, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2014, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2015, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2016, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2017, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2018, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2019, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2020, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2021, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2022, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2023, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2024, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2025, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2026, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2027, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2028, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2029, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2030, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2031, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2032, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2033, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2034, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2035, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2036, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2037, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2038, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2039, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2040, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2041, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2042, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2043, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2044, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2045, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2046, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2047, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2048, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2049, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2050, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2051, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2052, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2053, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2054, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2055, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2056, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2057, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2058, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2059, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2060, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2061, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2062, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2063, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2064, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2065, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2066, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2067, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2068, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2069, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2070, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2071, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2072, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2073, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2074, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2075, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2076, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2077, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2078, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2079, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2080, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2081, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2082, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2083, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2084, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2085, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2086, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2087, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2088, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2089, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2090, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2091, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2092, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2093, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2094, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2095, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2096, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2097, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2098, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2099, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2100, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2101, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2102, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2103, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2104, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2105, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2106, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2107, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2108, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2109, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2110, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2111, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2112, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2113, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2114, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2115, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2116, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2117, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2118, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2119, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2120, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2121, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2122, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2123, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2124, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2125, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2126, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2127, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2128, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2129, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2130, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2131, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2132, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2133, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2134, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2135, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2136, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2137, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2138, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2139, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2140, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2141, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2142, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2143, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2144, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2145, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2146, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2147, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2148, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2149, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2150, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2151, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2152, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2153, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2154, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2155, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2156, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2157, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2158, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2159, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2160, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2161, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2162, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2163, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2164, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2165, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2166, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2167, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2168, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2169, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2170, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2171, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2172, ' x.shape:', (10, 120, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i:', 2173, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2174, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2175, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2176, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2177, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2178, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2179, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2180, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2181, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2182, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2183, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2184, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2185, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2186, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2187, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2188, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2189, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2190, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2191, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2192, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2193, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2194, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2195, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2196, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2197, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2198, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2199, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2200, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2201, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2202, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2203, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2204, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2205, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2206, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2207, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2208, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2209, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2210, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2211, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2212, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2213, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2214, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2215, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2216, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2217, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2218, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2219, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2220, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2221, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2222, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2223, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2224, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2225, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2226, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2227, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2228, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2229, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2230, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2231, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2232, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2233, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2234, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2235, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2236, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2237, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2238, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2239, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2240, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2241, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2242, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2243, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2244, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2245, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2246, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2247, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2248, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2249, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2250, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2251, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2252, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2253, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2254, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2255, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2256, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2257, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2258, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2259, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2260, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2261, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2262, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2263, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2264, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2265, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2266, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2267, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2268, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2269, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2270, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2271, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2272, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2273, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2274, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2275, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2276, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2277, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2278, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2279, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2280, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2281, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2282, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2283, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2284, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2285, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2286, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2287, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2288, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2289, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2290, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2291, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2292, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2293, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2294, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2295, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2296, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2297, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2298, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2299, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2300, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2301, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2302, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2303, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2304, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2305, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2306, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2307, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2308, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2309, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2310, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2311, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2312, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2313, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2314, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2315, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2316, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2317, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2318, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2319, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2320, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2321, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2322, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2323, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2324, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2325, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2326, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2327, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2328, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2329, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2330, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2331, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2332, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2333, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2334, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2335, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2336, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2337, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2338, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2339, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2340, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2341, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2342, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2343, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2344, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2345, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2346, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2347, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2348, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2349, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2350, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2351, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2352, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2353, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2354, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2355, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2356, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2357, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2358, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2359, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2360, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2361, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2362, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2363, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2364, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2365, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2366, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2367, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2368, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2369, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2370, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2371, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2372, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2373, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2374, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2375, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2376, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2377, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2378, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2379, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2380, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2381, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2382, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2383, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2384, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2385, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2386, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2387, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2388, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2389, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2390, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2391, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2392, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2393, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2394, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2395, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2396, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2397, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2398, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2399, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2400, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2401, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2402, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2403, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2404, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2405, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2406, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2407, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2408, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2409, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2410, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2411, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2412, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2413, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2414, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2415, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2416, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2417, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2418, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2419, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2420, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2421, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2422, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2423, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2424, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2425, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2426, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2427, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2428, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2429, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2430, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2431, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2432, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2433, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2434, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2435, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2436, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2437, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2438, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2439, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2440, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2441, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2442, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2443, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2444, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2445, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2446, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2447, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2448, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2449, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2450, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2451, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2452, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2453, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2454, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2455, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2456, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2457, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2458, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2459, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2460, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2461, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2462, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2463, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2464, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2465, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2466, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2467, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2468, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2469, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2470, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2471, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2472, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2473, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2474, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2475, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2476, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2477, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2478, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2479, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2480, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2481, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2482, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2483, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2484, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2485, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2486, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2487, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2488, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2489, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2490, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2491, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2492, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2493, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2494, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2495, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2496, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2497, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2498, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2499, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2500, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2501, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2502, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2503, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2504, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2505, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2506, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2507, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2508, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2509, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2510, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2511, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2512, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2513, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2514, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2515, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2516, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2517, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2518, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2519, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2520, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2521, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2522, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2523, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2524, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2525, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2526, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2527, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2528, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2529, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2530, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2531, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2532, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2533, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2534, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2535, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2536, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2537, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2538, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2539, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2540, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2541, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2542, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2543, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2544, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2545, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2546, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2547, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2548, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2549, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2550, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2551, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2552, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2553, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2554, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2555, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2556, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2557, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2558, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2559, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2560, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2561, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2562, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2563, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2564, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2565, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2566, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2567, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2568, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2569, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2570, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2571, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2572, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2573, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2574, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2575, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2576, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2577, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2578, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2579, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2580, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2581, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2582, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2583, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2584, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2585, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2586, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2587, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2588, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2589, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2590, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2591, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2592, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2593, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2594, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2595, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2596, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2597, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2598, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2599, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2600, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2601, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2602, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2603, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2604, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2605, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2606, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2607, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2608, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2609, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2610, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2611, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2612, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2613, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2614, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2615, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2616, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2617, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2618, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2619, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2620, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2621, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2622, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2623, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2624, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2625, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2626, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2627, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2628, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2629, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2630, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2631, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2632, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2633, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2634, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2635, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2636, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2637, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2638, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2639, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2640, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2641, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2642, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2643, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2644, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2645, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2646, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2647, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2648, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2649, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2650, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2651, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2652, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2653, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2654, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2655, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2656, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2657, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2658, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2659, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2660, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2661, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2662, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2663, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2664, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2665, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2666, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2667, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2668, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2669, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2670, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2671, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2672, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2673, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2674, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2675, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2676, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2677, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2678, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2679, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2680, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2681, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2682, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2683, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2684, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2685, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2686, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2687, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2688, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2689, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2690, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2691, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2692, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2693, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2694, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2695, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2696, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2697, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2698, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2699, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2700, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2701, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2702, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2703, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2704, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2705, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2706, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2707, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2708, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2709, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2710, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2711, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2712, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2713, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2714, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2715, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2716, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2717, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2718, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2719, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2720, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2721, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2722, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2723, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2724, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2725, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2726, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2727, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2728, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2729, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2730, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2731, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2732, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2733, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2734, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2735, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2736, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2737, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2738, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2739, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2740, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2741, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2742, ' x.shape:', (10, 120, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i:', 2743, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2744, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2745, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2746, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2747, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2748, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2749, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2750, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2751, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2752, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2753, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2754, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2755, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2756, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2757, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2758, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2759, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2760, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2761, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2762, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2763, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2764, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2765, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2766, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2767, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2768, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2769, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2770, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2771, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2772, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2773, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2774, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2775, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2776, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2777, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2778, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2779, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2780, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2781, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2782, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2783, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2784, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2785, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2786, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2787, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2788, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2789, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2790, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2791, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2792, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2793, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2794, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2795, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2796, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2797, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2798, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2799, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2800, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2801, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2802, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2803, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2804, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2805, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2806, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2807, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2808, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2809, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2810, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2811, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2812, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2813, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2814, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2815, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2816, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2817, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2818, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2819, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2820, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2821, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2822, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2823, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2824, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2825, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2826, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2827, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2828, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2829, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2830, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2831, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2832, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2833, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2834, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2835, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2836, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2837, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2838, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2839, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2840, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2841, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2842, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2843, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2844, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2845, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2846, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2847, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2848, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2849, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2850, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2851, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2852, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2853, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2854, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2855, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2856, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2857, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2858, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2859, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2860, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2861, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2862, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2863, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2864, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2865, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2866, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2867, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2868, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2869, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2870, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2871, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2872, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2873, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2874, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2875, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2876, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2877, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2878, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2879, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2880, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2881, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2882, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2883, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2884, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2885, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2886, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2887, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2888, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2889, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2890, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2891, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2892, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2893, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2894, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2895, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2896, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2897, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2898, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2899, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2900, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2901, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2902, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2903, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2904, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2905, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2906, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2907, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2908, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2909, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2910, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2911, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2912, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2913, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2914, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2915, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2916, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2917, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2918, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2919, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2920, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2921, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2922, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2923, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2924, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2925, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2926, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2927, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2928, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2929, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2930, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2931, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2932, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2933, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2934, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2935, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2936, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2937, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2938, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2939, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2940, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2941, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2942, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2943, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2944, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2945, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2946, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2947, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2948, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2949, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2950, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2951, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2952, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2953, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2954, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2955, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2956, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2957, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2958, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2959, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2960, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2961, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2962, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2963, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2964, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2965, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2966, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2967, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2968, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2969, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2970, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2971, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2972, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2973, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2974, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2975, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2976, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2977, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2978, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2979, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2980, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2981, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2982, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2983, ' x.shape:', (10, 240, 6))\n",
      "('i:', 2984, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2985, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2986, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2987, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2988, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2989, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2990, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2991, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2992, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2993, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2994, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2995, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2996, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2997, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2998, ' x.shape:', (10, 120, 6))\n",
      "('i:', 2999, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3000, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3001, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3002, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3003, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3004, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3005, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3006, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3007, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3008, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3009, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3010, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3011, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3012, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3013, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3014, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3015, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3016, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3017, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3018, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3019, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3020, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3021, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3022, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3023, ' x.shape:', (10, 240, 6))\n",
      "('i:', 3024, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3025, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3026, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3027, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3028, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3029, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3030, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3031, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3032, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3033, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3034, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3035, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3036, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3037, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3038, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3039, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3040, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3041, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3042, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3043, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3044, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3045, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3046, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3047, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3048, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3049, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3050, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3051, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3052, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3053, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3054, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3055, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3056, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3057, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3058, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3059, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3060, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3061, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3062, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3063, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3064, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3065, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3066, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3067, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3068, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3069, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3070, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3071, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3072, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3073, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3074, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3075, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3076, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3077, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3078, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3079, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3080, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3081, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3082, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3083, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3084, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3085, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3086, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3087, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3088, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3089, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3090, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3091, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3092, ' x.shape:', (10, 240, 6))\n",
      "('i:', 3093, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3094, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3095, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3096, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3097, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3098, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3099, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3100, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3101, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3102, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3103, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3104, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3105, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3106, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3107, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3108, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3109, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3110, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3111, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3112, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3113, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3114, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3115, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3116, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3117, ' x.shape:', (10, 240, 6))\n",
      "('i:', 3118, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3119, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3120, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3121, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3122, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3123, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3124, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3125, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3126, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3127, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3128, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3129, ' x.shape:', (10, 240, 6))\n",
      "('i:', 3130, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3131, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3132, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3133, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3134, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3135, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3136, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3137, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3138, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3139, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3140, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3141, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3142, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3143, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3144, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3145, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3146, ' x.shape:', (10, 240, 6))\n",
      "('i:', 3147, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3148, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3149, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3150, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3151, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3152, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3153, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3154, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3155, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3156, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3157, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3158, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3159, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3160, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3161, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3162, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3163, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3164, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3165, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3166, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3167, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3168, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3169, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3170, ' x.shape:', (10, 240, 6))\n",
      "('i:', 3171, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3172, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3173, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3174, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3175, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3176, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3177, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3178, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3179, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3180, ' x.shape:', (10, 240, 6))\n",
      "('i:', 3181, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3182, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3183, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3184, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3185, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3186, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3187, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3188, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3189, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3190, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3191, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3192, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3193, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3194, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3195, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3196, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3197, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3198, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3199, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3200, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3201, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3202, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3203, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3204, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3205, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3206, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3207, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3208, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3209, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3210, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3211, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3212, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3213, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3214, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3215, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3216, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3217, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3218, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3219, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3220, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3221, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3222, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3223, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3224, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3225, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3226, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3227, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3228, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3229, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3230, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3231, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3232, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3233, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3234, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3235, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3236, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3237, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3238, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3239, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3240, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3241, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3242, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3243, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3244, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3245, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3246, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3247, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3248, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3249, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3250, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3251, ' x.shape:', (10, 240, 6))\n",
      "('i:', 3252, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3253, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3254, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3255, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3256, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3257, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3258, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3259, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3260, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3261, ' x.shape:', (10, 120, 6))\n",
      "('i:', 3262, ' x.shape:', (10, 120, 6))\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(args.batch_size, args.data_scale, args.bptt_length) # batch_size=10, bptt_length=120\n",
    "data_loader.reset_batch_pointer()\n",
    "for i in range(int(data_loader.num_sequences / args.batch_size)):\n",
    "    x, y, w, c, lens = data_loader.next_batch()\n",
    "    print('i:',i,' x.shape:', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32632\n"
     ]
    }
   ],
   "source": [
    "print(data_loader.num_sequences )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00105173  0.00144482 -0.00125339]\n"
     ]
    }
   ],
   "source": [
    "noise_level = 0.001\n",
    "print(np.random.normal(0, noise_level, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_cluster(pi):\n",
    "      rand = random.random()\n",
    "      print(rand)\n",
    "      accumulate = 0.\n",
    "      for i in range(len(pi)):\n",
    "        accumulate += pi[i]\n",
    "        print('iterations:',i)\n",
    "        if accumulate >= rand:\n",
    "          print('accumulate:',accumulate)\n",
    "          return i\n",
    "      raise ValueError(\"Cannot sample a cluster!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.176895848302\n",
      "('iterations:', 0)\n",
      "('accumulate:', 0.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cluster([0.3, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = ['a', [1, 2]]\n",
    "print(a[0])\n",
    "print(a[1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m1:[batch*time, num_mixture]\n",
    "mu = (m1, m2, m3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
